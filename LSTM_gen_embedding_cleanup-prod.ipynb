{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:13:49.834155Z",
     "start_time": "2019-04-15T07:13:48.169695Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/udacity/deep-learning/blob/master/tensorboard/anna.txt\n",
    "# enwik8: http://prize.hutter1.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:17.508675Z",
     "start_time": "2019-04-15T07:26:17.505385Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pdb\n",
    "import time\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_gpu = True\n",
    "def gpu(m):\n",
    "    if to_gpu:\n",
    "        return m.cuda()\n",
    "    return m\n",
    "\n",
    "def ints_to_tensor(ints):\n",
    "    return gpu(torch.tensor(ints).long().transpose(1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model using the pytorch nn module\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, batch_size, embedding_dim):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        \n",
    "        # init the meta parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm_1 = nn.LSTMCell(input_size=embedding_dim, hidden_size=hidden_dim)\n",
    "        self.lstm_2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=hidden_dim) \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # fully connected layer to connect the output of the LSTM cell to the output\n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
    "        \n",
    "    def forward(self, x, hc, return_hc=False):\n",
    "        seq_len = x.shape[0]\n",
    "        batch_size = x.shape[1]\n",
    "        \n",
    "        # empty tensor for the output of the lstm\n",
    "        output_seq = torch.empty((seq_len, batch_size, self.vocab_size))\n",
    "        output_seq = gpu(output_seq)\n",
    "        hc1, hc2 = hc, hc\n",
    "\n",
    "        # for every step in the sequence\n",
    "        for t in range(seq_len):\n",
    "            out_t, hc1, hc2 = self.feed_one_x_t(x[t], hc1, hc2)\n",
    "            output_seq[t] = out_t\n",
    "        \n",
    "        if return_hc:\n",
    "            return output_seq, hc1, hc2\n",
    "        return output_seq\n",
    "            \n",
    "    def init_hidden(self, bs=None):\n",
    "        if bs is None:\n",
    "            bs = self.batch_size\n",
    "        # initialize the <hidden state> and the <cell state> to zeros\n",
    "        return (gpu(torch.zeros(bs, self.hidden_dim)), gpu(torch.zeros(bs, self.hidden_dim)))\n",
    "    \n",
    "    def feed_one_x_t(self, x_t, hc1, hc2):\n",
    "        # convert batch of single ints to batch of embeddings\n",
    "        xt_emb = self.emb(x_t) # returns (batch_size, embedding_dim)\n",
    "\n",
    "        # get the hidden and cell states from the first layer cell\n",
    "        hc1 = self.lstm_1(xt_emb, hc1)\n",
    "        h1, c1 = hc1 # unpack the hidden and the cell states from the first layer\n",
    "\n",
    "        # pass the hidden state from the first layer to the cell in the second layer\n",
    "        hc2 = self.lstm_2(h1, hc2)\n",
    "        h2, c2 = hc2 # unpack the hidden and cell states from the second layer cell\n",
    "\n",
    "        # form the output of the fc\n",
    "        out_t = self.fc(self.dropout(h2))\n",
    "        \n",
    "        return out_t, hc1, hc2\n",
    "    \n",
    "    def feed_one_char(self, char, hc1, hc2):\n",
    "        ints = [char2int[char]] # sequence of ints \n",
    "        ints = [ints] # a 1-batch of seqs\n",
    "        x = ints_to_tensor(ints) # shape of (seq_len, batch_size)\n",
    "        x_t = x[0] # take the first (single) part of the sequence\n",
    "        \n",
    "        return self.feed_one_x_t(x_t, hc1, hc2)\n",
    "    \n",
    "    def warm_up(self, base_str):\n",
    "        hc = self.init_hidden(bs=1)\n",
    "        ints = [char2int[c] for c in base_str]  # sequence of ints \n",
    "        ints = [ints] # a 1-batch of seqs\n",
    "        x = ints_to_tensor(ints) # shape of (seq_len, batch_size)\n",
    "        \n",
    "        out, hc1, hc2 = self.forward(x, hc, return_hc=True)\n",
    "        return out, hc1, hc2\n",
    "    \n",
    "    def sample_char(self, out_t, top_k=5):\n",
    "        # apply the softmax to the output to get the probabilities of the characters\n",
    "        out_t = F.softmax(out_t, dim=1)\n",
    "\n",
    "        # out_t now holds the vector of predictions (1, vocab_size)\n",
    "        # we want to sample 5 top characters\n",
    "        p, top_char = out_t.topk(top_k) # returns tuple (top_values, top_indices)\n",
    "\n",
    "        # get the top k characters by their probabilities\n",
    "        top_char = top_char.cpu().squeeze().numpy()\n",
    "\n",
    "        # sample a character using its probability\n",
    "        p = p.detach().cpu().squeeze().numpy()\n",
    "        char_int = np.random.choice(top_char, p = p/p.sum())\n",
    "        \n",
    "        return int2char[char_int]\n",
    "        \n",
    "    def predict(self, base_str, top_k=5, seq_len=128):\n",
    "        self.eval()\n",
    "\n",
    "        res = np.empty(seq_len+len(base_str), dtype=\"object\")\n",
    "        for i, c in enumerate(base_str):\n",
    "            res[i] = c\n",
    "        \n",
    "        out_warm, hc1, hc2 = self.warm_up(base_str)\n",
    "        out_t = out_warm[-1]\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            char = self.sample_char(out_t, top_k)\n",
    "            out_t, hc1, hc2 = self.feed_one_char(char, hc1, hc2)\n",
    "            res[i + len(base_str)] = char\n",
    "        \n",
    "        return ''.join(res)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(\"save_1\")\n",
    "net = state.pop(\"net\")\n",
    "char2int = state.pop(\"char2int\")\n",
    "int2char = state.pop(\"int2char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The structure of [[philosophy]] and the play as that a conformation at [[Samithenes]] and [[Carlon]], and [[Astochine Carty of the A'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(\"The \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
