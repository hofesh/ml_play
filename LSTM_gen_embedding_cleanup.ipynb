{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:13:49.834155Z",
     "start_time": "2019-04-15T07:13:48.169695Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/udacity/deep-learning/blob/master/tensorboard/anna.txt\n",
    "# enwik8: http://prize.hutter1.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:17.508675Z",
     "start_time": "2019-04-15T07:26:17.505385Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pdb\n",
    "import time\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:17.735041Z",
     "start_time": "2019-04-15T07:26:17.720748Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = Path(\"enwik8/enwik8\")\n",
    "text = path.open(encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:17.735041Z",
     "start_time": "2019-04-15T07:26:17.720748Z"
    }
   },
   "outputs": [],
   "source": [
    "printable = set(string.printable)\n",
    "text_clean = list(filter(lambda x: x in printable, text))\n",
    "\n",
    "# get the set of all characters\n",
    "characters = tuple(set(text_clean))\n",
    "\n",
    "# use enumeration to give the characters integer values\n",
    "int2char = dict(enumerate(characters))\n",
    "\n",
    "# create the look up dictionary from characters to the assigned integers\n",
    "char2int = {char: index for index, char in int2char.items()}\n",
    "\n",
    "# encode the text, using the character to integer dictionary\n",
    "encoded = np.array([char2int[char] for char in text_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_gpu = True\n",
    "def gpu(m):\n",
    "    if to_gpu:\n",
    "        return m.cuda()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:17.901416Z",
     "start_time": "2019-04-15T07:26:17.894616Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs_in_a_batch, n_characters):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs_in_a_batch * n_characters\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs_in_a_batch, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_characters):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_characters]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_characters]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ints_to_tensor(ints):\n",
    "    return gpu(torch.tensor(ints).long().transpose(1, 0))\n",
    "\n",
    "def xy_to_tensor(x, y):\n",
    "    x = ints_to_tensor(x)\n",
    "    y = torch.tensor(y.T).long()\n",
    "    return x, gpu(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:18.081312Z",
     "start_time": "2019-04-15T07:26:18.064093Z"
    }
   },
   "outputs": [],
   "source": [
    "# build the model using the pytorch nn module\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, batch_size, embedding_dim):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        \n",
    "        # init the meta parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm_1 = nn.LSTMCell(input_size=embedding_dim, hidden_size=hidden_dim)\n",
    "        self.lstm_2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=hidden_dim) \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # fully connected layer to connect the output of the LSTM cell to the output\n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
    "        \n",
    "    def forward(self, x, hc, return_hc=False):\n",
    "        seq_len = x.shape[0]\n",
    "        batch_size = x.shape[1]\n",
    "        \n",
    "        # empty tensor for the output of the lstm\n",
    "        output_seq = torch.empty((seq_len, batch_size, self.vocab_size))\n",
    "        output_seq = gpu(output_seq)\n",
    "        hc1, hc2 = hc, hc\n",
    "\n",
    "        # for every step in the sequence\n",
    "        for t in range(seq_len):\n",
    "            out_t, hc1, hc2 = self.feed_one_x_t(x[t], hc1, hc2)\n",
    "            output_seq[t] = out_t\n",
    "        \n",
    "        if return_hc:\n",
    "            return output_seq, hc1, hc2\n",
    "        return output_seq\n",
    "            \n",
    "    def init_hidden(self, bs=None):\n",
    "        if bs is None:\n",
    "            bs = self.batch_size\n",
    "        # initialize the <hidden state> and the <cell state> to zeros\n",
    "        return (gpu(torch.zeros(bs, self.hidden_dim)), gpu(torch.zeros(bs, self.hidden_dim)))\n",
    "    \n",
    "    def feed_one_x_t(self, x_t, hc1, hc2):\n",
    "        # convert batch of single ints to batch of embeddings\n",
    "        xt_emb = self.emb(x_t) # returns (batch_size, embedding_dim)\n",
    "\n",
    "        # get the hidden and cell states from the first layer cell\n",
    "        hc1 = self.lstm_1(xt_emb, hc1)\n",
    "        h1, c1 = hc1 # unpack the hidden and the cell states from the first layer\n",
    "\n",
    "        # pass the hidden state from the first layer to the cell in the second layer\n",
    "        hc2 = self.lstm_2(h1, hc2)\n",
    "        h2, c2 = hc2 # unpack the hidden and cell states from the second layer cell\n",
    "\n",
    "        # form the output of the fc\n",
    "        out_t = self.fc(self.dropout(h2))\n",
    "        \n",
    "        return out_t, hc1, hc2\n",
    "    \n",
    "    def feed_one_char(self, char, hc1, hc2):\n",
    "        ints = [char2int[char]] # sequence of ints \n",
    "        ints = [ints] # a 1-batch of seqs\n",
    "        x = ints_to_tensor(ints) # shape of (seq_len, batch_size)\n",
    "        x_t = x[0] # take the first (single) part of the sequence\n",
    "        \n",
    "        return self.feed_one_x_t(x_t, hc1, hc2)\n",
    "    \n",
    "    def warm_up(self, base_str):\n",
    "        hc = net.init_hidden(bs=1)\n",
    "        ints = [char2int[c] for c in base_str]  # sequence of ints \n",
    "        ints = [ints] # a 1-batch of seqs\n",
    "        x = ints_to_tensor(ints) # shape of (seq_len, batch_size)\n",
    "        \n",
    "        out, hc1, hc2 = self.forward(x, hc, return_hc=True)\n",
    "        return out, hc1, hc2\n",
    "    \n",
    "    def sample_char(self, out_t, top_k=5):\n",
    "        # apply the softmax to the output to get the probabilities of the characters\n",
    "        out_t = F.softmax(out_t, dim=1)\n",
    "\n",
    "        # out_t now holds the vector of predictions (1, vocab_size)\n",
    "        # we want to sample 5 top characters\n",
    "        p, top_char = out_t.topk(top_k) # returns tuple (top_values, top_indices)\n",
    "\n",
    "        # get the top k characters by their probabilities\n",
    "        top_char = top_char.cpu().squeeze().numpy()\n",
    "\n",
    "        # sample a character using its probability\n",
    "        p = p.detach().cpu().squeeze().numpy()\n",
    "        char_int = np.random.choice(top_char, p = p/p.sum())\n",
    "        \n",
    "        return int2char[char_int]\n",
    "        \n",
    "    def predict(self, base_str, top_k=5, seq_len=128):\n",
    "        self.eval()\n",
    "\n",
    "        res = np.empty(seq_len+len(base_str), dtype=\"object\")\n",
    "        for i, c in enumerate(base_str):\n",
    "            res[i] = c\n",
    "        \n",
    "        out_warm, hc1, hc2 = self.warm_up(base_str)\n",
    "        out_t = out_warm[-1]\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            char = self.sample_char(out_t, top_k)\n",
    "            out_t, hc1, hc2 = self.feed_one_char(char, hc1, hc2)\n",
    "            res[i + len(base_str)] = char\n",
    "        \n",
    "        return ''.join(res)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:18.226534Z",
     "start_time": "2019-04-15T07:26:18.224057Z"
    }
   },
   "outputs": [],
   "source": [
    "BS = 500 # 500\n",
    "embedding_dim = 100\n",
    "vocab_size=len(char2int)\n",
    "hidden_dim = 512 # 512\n",
    "seq_len = 128 # 128\n",
    "seq_len_BS = seq_len * BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BS: 500\n",
      "embedding_dim: 100\n",
      "vocab_size: 97\n",
      "hidden_dim: 512\n",
      "seq_len: 128\n",
      "seq_len_BS: 64000\n"
     ]
    }
   ],
   "source": [
    "print(f'BS: {BS}')\n",
    "print(f'embedding_dim: {embedding_dim}')\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "print(f'hidden_dim: {hidden_dim}')\n",
    "print(f'seq_len: {seq_len}')\n",
    "print(f'seq_len_BS: {seq_len_BS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:18.580287Z",
     "start_time": "2019-04-15T07:26:18.576595Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the validation and the training data\n",
    "val_idx = int(len(encoded) * (1 - 0.1))\n",
    "data, val_data = encoded[:val_idx], encoded[val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:18.431525Z",
     "start_time": "2019-04-15T07:26:18.395438Z"
    }
   },
   "outputs": [],
   "source": [
    "# compile the network - sequence_len, vocab_size, hidden_dim, batch_size\n",
    "net = CharLSTM(vocab_size=len(char2int), hidden_dim=hidden_dim, batch_size=BS, embedding_dim=embedding_dim)\n",
    "net = gpu(net)\n",
    "\n",
    "# define the loss and the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(get_batches(data, BS, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = xy_to_tensor(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 500]), torch.Size([128, 500]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 500, 97])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc = net.init_hidden()\n",
    "out = net(x, hc)\n",
    "out.shape # (seq_len, batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.sample_char(out[-1, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TheMC'''('fM''''9\\n(fU(f}Mf MfC f}77]fM7Cf''(}A}(MC700]]. 9]C7''ff7 MC''''9Eff.( } }ff77f7]]77Af}}f7fMf M}fo00 C77 7'0f77 0]f]M977f7\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(\"The\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:27:47.294488Z",
     "start_time": "2019-04-15T07:26:19.006577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed forward: 0.12067842483520508\n",
      "elapsed backward: 0.4368586540222168\n",
      "Epoch: 0, Batch: 0, Train Loss: 1.522233\n",
      "Epoch: 0, Batch: 0, Train Loss: 1.522233, Validation Loss: 1.487424\n",
      "The succession]], although the [[Conserve]], and arolitically at the cources and according important as\n",
      "\n",
      "Epoch: 0, Batch: 1, Train Loss: 1.506478\n",
      "Epoch: 0, Batch: 2, Train Loss: 1.507504\n",
      "Epoch: 0, Batch: 3, Train Loss: 1.488862\n",
      "Epoch: 0, Batch: 4, Train Loss: 1.483703\n",
      "Epoch: 0, Batch: 5, Train Loss: 1.507480\n",
      "Epoch: 0, Batch: 6, Train Loss: 1.497443\n",
      "Epoch: 0, Batch: 7, Train Loss: 1.504932\n",
      "Epoch: 0, Batch: 8, Train Loss: 1.489003\n",
      "Epoch: 0, Batch: 9, Train Loss: 1.484921\n",
      "Epoch: 0, Batch: 10, Train Loss: 1.507722\n",
      "Epoch: 0, Batch: 10, Train Loss: 1.507722, Validation Loss: 1.485062\n",
      "The film ship in 1984. The coup the found of case.  The could be common implement is natural teachers a\n",
      "\n",
      "Epoch: 0, Batch: 11, Train Loss: 1.489842\n",
      "Epoch: 0, Batch: 12, Train Loss: 1.482748\n",
      "Epoch: 0, Batch: 13, Train Loss: 1.499490\n",
      "Epoch: 0, Batch: 14, Train Loss: 1.477556\n",
      "Epoch: 0, Batch: 15, Train Loss: 1.476949\n",
      "Epoch: 0, Batch: 16, Train Loss: 1.478904\n",
      "Epoch: 0, Batch: 17, Train Loss: 1.478995\n",
      "Epoch: 0, Batch: 18, Train Loss: 1.467567\n",
      "Epoch: 0, Batch: 19, Train Loss: 1.484708\n",
      "Epoch: 0, Batch: 20, Train Loss: 1.481881\n",
      "Epoch: 0, Batch: 20, Train Loss: 1.481881, Validation Loss: 1.483606\n",
      "The Choical propersation]].\n",
      "\n",
      "===Sevent==\n",
      "The make and thought includes there is also accased a music su\n",
      "\n",
      "Epoch: 0, Batch: 21, Train Loss: 1.458103\n",
      "Epoch: 0, Batch: 22, Train Loss: 1.443959\n",
      "Epoch: 0, Batch: 23, Train Loss: 1.467546\n",
      "Epoch: 0, Batch: 24, Train Loss: 1.453866\n",
      "Epoch: 0, Batch: 25, Train Loss: 1.457440\n",
      "Epoch: 0, Batch: 26, Train Loss: 1.444245\n",
      "Epoch: 0, Batch: 27, Train Loss: 1.444046\n",
      "Epoch: 0, Batch: 28, Train Loss: 1.475276\n",
      "Epoch: 0, Batch: 29, Train Loss: 1.471613\n",
      "Epoch: 0, Batch: 30, Train Loss: 1.457622\n",
      "Epoch: 0, Batch: 30, Train Loss: 1.457622, Validation Loss: 1.479811\n",
      "The [[Confed]] as were as [[compiler process]] in an it will be actually also assemitially a significan\n",
      "\n",
      "Epoch: 0, Batch: 31, Train Loss: 1.470301\n",
      "Epoch: 0, Batch: 32, Train Loss: 1.483511\n",
      "Epoch: 0, Batch: 33, Train Loss: 1.485066\n",
      "Epoch: 0, Batch: 34, Train Loss: 1.467353\n",
      "Epoch: 0, Batch: 35, Train Loss: 1.458270\n",
      "Epoch: 0, Batch: 36, Train Loss: 1.461092\n",
      "Epoch: 0, Batch: 37, Train Loss: 1.471236\n",
      "Epoch: 0, Batch: 38, Train Loss: 1.450767\n",
      "Epoch: 0, Batch: 39, Train Loss: 1.468317\n",
      "Epoch: 0, Batch: 40, Train Loss: 1.448994\n",
      "Epoch: 0, Batch: 40, Train Loss: 1.448994, Validation Loss: 1.480145\n",
      "The commercial color speed. These are to see [[massing]] political program important theorate at a stru\n",
      "\n",
      "Epoch: 0, Batch: 41, Train Loss: 1.470789\n",
      "Epoch: 0, Batch: 42, Train Loss: 1.465475\n",
      "Epoch: 0, Batch: 43, Train Loss: 1.475963\n",
      "Epoch: 0, Batch: 44, Train Loss: 1.497744\n",
      "Epoch: 0, Batch: 45, Train Loss: 1.494439\n",
      "Epoch: 0, Batch: 46, Train Loss: 1.473365\n",
      "Epoch: 0, Batch: 47, Train Loss: 1.460896\n",
      "Epoch: 0, Batch: 48, Train Loss: 1.458873\n",
      "Epoch: 0, Batch: 49, Train Loss: 1.437405\n",
      "Epoch: 0, Batch: 50, Train Loss: 1.461788\n",
      "Epoch: 0, Batch: 50, Train Loss: 1.461788, Validation Loss: 1.477829\n",
      "The computer and study off the composite of characters.\n",
      "\n",
      "The second, song and acceptation to two popula\n",
      "\n",
      "Epoch: 0, Batch: 51, Train Loss: 1.460756\n",
      "Epoch: 0, Batch: 52, Train Loss: 1.466971\n",
      "Epoch: 0, Batch: 53, Train Loss: 1.458225\n",
      "Epoch: 0, Batch: 54, Train Loss: 1.466592\n",
      "Epoch: 0, Batch: 55, Train Loss: 1.462079\n",
      "Epoch: 0, Batch: 56, Train Loss: 1.479201\n",
      "Epoch: 0, Batch: 57, Train Loss: 1.463729\n",
      "Epoch: 0, Batch: 58, Train Loss: 1.473171\n",
      "Epoch: 0, Batch: 59, Train Loss: 1.460564\n",
      "Epoch: 0, Batch: 60, Train Loss: 1.473645\n",
      "Epoch: 0, Batch: 60, Train Loss: 1.473645, Validation Loss: 1.473431\n",
      "The country of [[Archuist]], who writen that he contested in the [[United States]], while sufficient te\n",
      "\n",
      "Epoch: 0, Batch: 61, Train Loss: 1.467087\n",
      "Epoch: 0, Batch: 62, Train Loss: 1.453128\n",
      "Epoch: 0, Batch: 63, Train Loss: 1.469641\n",
      "Epoch: 0, Batch: 64, Train Loss: 1.450642\n",
      "Epoch: 0, Batch: 65, Train Loss: 1.470507\n",
      "Epoch: 0, Batch: 66, Train Loss: 1.463717\n",
      "Epoch: 0, Batch: 67, Train Loss: 1.463083\n",
      "Epoch: 0, Batch: 68, Train Loss: 1.464041\n",
      "Epoch: 0, Batch: 69, Train Loss: 1.479685\n",
      "Epoch: 0, Batch: 70, Train Loss: 1.478773\n",
      "Epoch: 0, Batch: 70, Train Loss: 1.478773, Validation Loss: 1.470285\n",
      "The [[Bondard Boods|Mile]], with [[Communician Polycument]].\n",
      "\n",
      "======\n",
      "\n",
      "A single, and there was in as one\n",
      "\n",
      "Epoch: 0, Batch: 71, Train Loss: 1.487287\n",
      "Epoch: 0, Batch: 72, Train Loss: 1.489050\n",
      "Epoch: 0, Batch: 73, Train Loss: 1.476314\n",
      "Epoch: 0, Batch: 74, Train Loss: 1.464159\n",
      "Epoch: 0, Batch: 75, Train Loss: 1.462384\n",
      "Epoch: 0, Batch: 76, Train Loss: 1.433214\n",
      "Epoch: 0, Batch: 77, Train Loss: 1.455833\n",
      "Epoch: 0, Batch: 78, Train Loss: 1.447409\n",
      "Epoch: 0, Batch: 79, Train Loss: 1.445260\n",
      "Epoch: 0, Batch: 80, Train Loss: 1.444585\n",
      "Epoch: 0, Batch: 80, Train Loss: 1.444585, Validation Loss: 1.469027\n",
      "Then all had official strip period.\n",
      "\n",
      "== Event special colleges==\n",
      "\n",
      "*[[Albert Song Barry Secher]]\n",
      "*[[Each\n",
      "\n",
      "Epoch: 0, Batch: 81, Train Loss: 1.449160\n",
      "Epoch: 0, Batch: 82, Train Loss: 1.433420\n",
      "Epoch: 0, Batch: 83, Train Loss: 1.432881\n",
      "Epoch: 0, Batch: 84, Train Loss: 1.435353\n",
      "Epoch: 0, Batch: 85, Train Loss: 1.429720\n",
      "Epoch: 0, Batch: 86, Train Loss: 1.424274\n",
      "Epoch: 0, Batch: 87, Train Loss: 1.429127\n",
      "Epoch: 0, Batch: 88, Train Loss: 1.416675\n",
      "Epoch: 0, Batch: 89, Train Loss: 1.415999\n",
      "Epoch: 0, Batch: 90, Train Loss: 1.430684\n",
      "Epoch: 0, Batch: 90, Train Loss: 1.430684, Validation Loss: 1.468503\n",
      "The players are a state in this composed in alley introducers, and surrounded by those when the same at\n",
      "\n",
      "Epoch: 0, Batch: 91, Train Loss: 1.446671\n",
      "Epoch: 0, Batch: 92, Train Loss: 1.434567\n",
      "Epoch: 0, Batch: 93, Train Loss: 1.429436\n",
      "Epoch: 0, Batch: 94, Train Loss: 1.411577\n",
      "Epoch: 0, Batch: 95, Train Loss: 1.445123\n",
      "Epoch: 0, Batch: 96, Train Loss: 1.412522\n",
      "Epoch: 0, Batch: 97, Train Loss: 1.396582\n",
      "Epoch: 0, Batch: 98, Train Loss: 1.409874\n",
      "Epoch: 0, Batch: 99, Train Loss: 1.427053\n",
      "Epoch: 0, Batch: 100, Train Loss: 1.425119\n",
      "Epoch: 0, Batch: 100, Train Loss: 1.425119, Validation Loss: 1.466843\n",
      "The Allegary (deproves on any church), and this algeption in a stories of allegations. A playing a mark\n",
      "\n",
      "Epoch: 0, Batch: 101, Train Loss: 1.434429\n",
      "Epoch: 0, Batch: 102, Train Loss: 1.435804\n",
      "Epoch: 0, Batch: 103, Train Loss: 1.439797\n",
      "Epoch: 0, Batch: 104, Train Loss: 1.437605\n",
      "Epoch: 0, Batch: 105, Train Loss: 1.438513\n",
      "Epoch: 0, Batch: 106, Train Loss: 1.441432\n",
      "Epoch: 0, Batch: 107, Train Loss: 1.423456\n",
      "Epoch: 0, Batch: 108, Train Loss: 1.430581\n",
      "Epoch: 0, Batch: 109, Train Loss: 1.436296\n",
      "Epoch: 0, Batch: 110, Train Loss: 1.431830\n",
      "Epoch: 0, Batch: 110, Train Loss: 1.431830, Validation Loss: 1.464794\n",
      "The Corporatorian American Charles of Sans to the Cordinary or [[Cole Armis and English]] [[Engineer of\n",
      "\n",
      "Epoch: 0, Batch: 111, Train Loss: 1.449576\n",
      "Epoch: 0, Batch: 112, Train Loss: 1.429940\n",
      "Epoch: 0, Batch: 113, Train Loss: 1.456818\n",
      "Epoch: 0, Batch: 114, Train Loss: 1.439006\n",
      "Epoch: 0, Batch: 115, Train Loss: 1.443374\n",
      "Epoch: 0, Batch: 116, Train Loss: 1.460908\n",
      "Epoch: 0, Batch: 117, Train Loss: 1.433176\n",
      "Epoch: 0, Batch: 118, Train Loss: 1.440937\n",
      "Epoch: 0, Batch: 119, Train Loss: 1.449341\n",
      "Epoch: 0, Batch: 120, Train Loss: 1.413951\n",
      "Epoch: 0, Batch: 120, Train Loss: 1.413951, Validation Loss: 1.461294\n",
      "The competition of cases are national composed-or individuaty, and the presents in the [[Barranter Alex\n",
      "\n",
      "Epoch: 0, Batch: 121, Train Loss: 1.392916\n",
      "Epoch: 0, Batch: 122, Train Loss: 1.408429\n",
      "Epoch: 0, Batch: 123, Train Loss: 1.432267\n",
      "Epoch: 0, Batch: 124, Train Loss: 1.430686\n",
      "Epoch: 0, Batch: 125, Train Loss: 1.430531\n",
      "Epoch: 0, Batch: 126, Train Loss: 1.437221\n",
      "Epoch: 0, Batch: 127, Train Loss: 1.429887\n",
      "Epoch: 0, Batch: 128, Train Loss: 1.436076\n",
      "Epoch: 0, Batch: 129, Train Loss: 1.433660\n",
      "Epoch: 0, Batch: 130, Train Loss: 1.441548\n",
      "Epoch: 0, Batch: 130, Train Loss: 1.441548, Validation Loss: 1.457779\n",
      "The server and [[Alexandria]]. As a most progression, stopped between the capital of the top contrastin\n",
      "\n",
      "Epoch: 0, Batch: 131, Train Loss: 1.459541\n",
      "Epoch: 0, Batch: 132, Train Loss: 1.446870\n",
      "Epoch: 0, Batch: 133, Train Loss: 1.428910\n",
      "Epoch: 0, Batch: 134, Train Loss: 1.433922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 135, Train Loss: 1.430912\n",
      "Epoch: 0, Batch: 136, Train Loss: 1.440560\n",
      "Epoch: 0, Batch: 137, Train Loss: 1.467538\n",
      "Epoch: 0, Batch: 138, Train Loss: 1.457695\n",
      "Epoch: 0, Batch: 139, Train Loss: 1.462981\n",
      "Epoch: 0, Batch: 140, Train Loss: 1.429020\n",
      "Epoch: 0, Batch: 140, Train Loss: 1.429020, Validation Loss: 1.454768\n",
      "The Century Chicago and the Aughers of the United States, Analas is the [[American City of the Arabian]\n",
      "\n",
      "Epoch: 0, Batch: 141, Train Loss: 1.413854\n",
      "Epoch: 0, Batch: 142, Train Loss: 1.412826\n",
      "Epoch: 0, Batch: 143, Train Loss: 1.410467\n",
      "Epoch: 0, Batch: 144, Train Loss: 1.418755\n",
      "Epoch: 0, Batch: 145, Train Loss: 1.427979\n",
      "Epoch: 0, Batch: 146, Train Loss: 1.418987\n",
      "Epoch: 0, Batch: 147, Train Loss: 1.416317\n",
      "Epoch: 0, Batch: 148, Train Loss: 1.414020\n",
      "Epoch: 0, Batch: 149, Train Loss: 1.433620\n",
      "Epoch: 0, Batch: 150, Train Loss: 1.408010\n",
      "Epoch: 0, Batch: 150, Train Loss: 1.408010, Validation Loss: 1.453947\n",
      "The [[Canadian]] in [[1639 in Chip II|A Branders]] it is a song on a [[domestic]] oversignized the seri\n",
      "\n",
      "Epoch: 0, Batch: 151, Train Loss: 1.435318\n",
      "Epoch: 0, Batch: 152, Train Loss: 1.442118\n",
      "Epoch: 0, Batch: 153, Train Loss: 1.443315\n",
      "Epoch: 0, Batch: 154, Train Loss: 1.428497\n",
      "Epoch: 0, Batch: 155, Train Loss: 1.437840\n",
      "Epoch: 0, Batch: 156, Train Loss: 1.425715\n",
      "Epoch: 0, Batch: 157, Train Loss: 1.435019\n",
      "Epoch: 0, Batch: 158, Train Loss: 1.423925\n",
      "Epoch: 0, Batch: 159, Train Loss: 1.437163\n",
      "Epoch: 0, Batch: 160, Train Loss: 1.423786\n",
      "Epoch: 0, Batch: 160, Train Loss: 1.423786, Validation Loss: 1.452810\n",
      "The capital on a [[superstative]], and the first [[set of the communicians such of the song interested \n",
      "\n",
      "Epoch: 0, Batch: 161, Train Loss: 1.430889\n",
      "Epoch: 0, Batch: 162, Train Loss: 1.424716\n",
      "Epoch: 0, Batch: 163, Train Loss: 1.412260\n",
      "Epoch: 0, Batch: 164, Train Loss: 1.436717\n",
      "Epoch: 0, Batch: 165, Train Loss: 1.402717\n",
      "Epoch: 0, Batch: 166, Train Loss: 1.413156\n",
      "Epoch: 0, Batch: 167, Train Loss: 1.406084\n",
      "Epoch: 0, Batch: 168, Train Loss: 1.434118\n",
      "Epoch: 0, Batch: 169, Train Loss: 1.424798\n",
      "Epoch: 0, Batch: 170, Train Loss: 1.422054\n",
      "Epoch: 0, Batch: 170, Train Loss: 1.422054, Validation Loss: 1.454624\n",
      "Their the father-leader of the [[United Network|Collon]] and [[Battle of Seconstantine|Catholic Broadca\n",
      "\n",
      "Epoch: 0, Batch: 171, Train Loss: 1.418232\n",
      "Epoch: 0, Batch: 172, Train Loss: 1.444014\n",
      "Epoch: 0, Batch: 173, Train Loss: 1.428309\n",
      "Epoch: 0, Batch: 174, Train Loss: 1.443613\n",
      "Epoch: 0, Batch: 175, Train Loss: 1.430872\n",
      "Epoch: 0, Batch: 176, Train Loss: 1.451845\n",
      "Epoch: 0, Batch: 177, Train Loss: 1.455050\n",
      "Epoch: 0, Batch: 178, Train Loss: 1.426775\n",
      "Epoch: 0, Batch: 179, Train Loss: 1.437218\n",
      "Epoch: 0, Batch: 180, Train Loss: 1.431061\n",
      "Epoch: 0, Batch: 180, Train Loss: 1.431061, Validation Loss: 1.451173\n",
      "The county at the programme in [[Statistics County]] and [[Alton San Arts]] to [[Septest]] [[Cross]] as\n",
      "\n",
      "Epoch: 0, Batch: 181, Train Loss: 1.432835\n",
      "Epoch: 0, Batch: 182, Train Loss: 1.448023\n",
      "Epoch: 0, Batch: 183, Train Loss: 1.433431\n",
      "Epoch: 0, Batch: 184, Train Loss: 1.433705\n",
      "Epoch: 0, Batch: 185, Train Loss: 1.424231\n",
      "Epoch: 0, Batch: 186, Train Loss: 1.416076\n",
      "Epoch: 0, Batch: 187, Train Loss: 1.436051\n",
      "Epoch: 0, Batch: 188, Train Loss: 1.407147\n",
      "Epoch: 0, Batch: 189, Train Loss: 1.426183\n",
      "Epoch: 0, Batch: 190, Train Loss: 1.423028\n",
      "Epoch: 0, Batch: 190, Train Loss: 1.423028, Validation Loss: 1.449244\n",
      "Thes includes the [[coaster]] teams were alternate an entire.\n",
      "\n",
      "== Exchanged association==\n",
      "The southern \n",
      "\n",
      "Epoch: 0, Batch: 191, Train Loss: 1.415867\n",
      "Epoch: 0, Batch: 192, Train Loss: 1.425169\n",
      "Epoch: 0, Batch: 193, Train Loss: 1.411065\n",
      "Epoch: 0, Batch: 194, Train Loss: 1.406987\n",
      "Epoch: 0, Batch: 195, Train Loss: 1.416100\n",
      "Epoch: 0, Batch: 196, Train Loss: 1.413244\n",
      "Epoch: 0, Batch: 197, Train Loss: 1.422759\n",
      "Epoch: 0, Batch: 198, Train Loss: 1.429085\n",
      "Epoch: 0, Batch: 199, Train Loss: 1.412299\n",
      "Epoch: 0, Batch: 200, Train Loss: 1.408640\n",
      "Epoch: 0, Batch: 200, Train Loss: 1.408640, Validation Loss: 1.444708\n",
      "The conscription of [[Calvenie Acassic Bournal|Starland]], and the [[Bath of Corporation]]. If the form\n",
      "\n",
      "Epoch: 0, Batch: 201, Train Loss: 1.423021\n",
      "Epoch: 0, Batch: 202, Train Loss: 1.405702\n",
      "Epoch: 0, Batch: 203, Train Loss: 1.401939\n",
      "Epoch: 0, Batch: 204, Train Loss: 1.411869\n",
      "Epoch: 0, Batch: 205, Train Loss: 1.404018\n",
      "Epoch: 0, Batch: 206, Train Loss: 1.416516\n",
      "Epoch: 0, Batch: 207, Train Loss: 1.395517\n",
      "Epoch: 0, Batch: 208, Train Loss: 1.408819\n",
      "Epoch: 0, Batch: 209, Train Loss: 1.414152\n",
      "Epoch: 0, Batch: 210, Train Loss: 1.410534\n",
      "Epoch: 0, Batch: 210, Train Loss: 1.410534, Validation Loss: 1.442172\n",
      "Then the think the full one and combination and story, which has been also as a crashizing transportati\n",
      "\n",
      "Epoch: 0, Batch: 211, Train Loss: 1.413432\n",
      "Epoch: 0, Batch: 212, Train Loss: 1.400012\n",
      "Epoch: 0, Batch: 213, Train Loss: 1.399106\n",
      "Epoch: 0, Batch: 214, Train Loss: 1.406427\n",
      "Epoch: 0, Batch: 215, Train Loss: 1.407688\n",
      "Epoch: 0, Batch: 216, Train Loss: 1.407730\n",
      "Epoch: 0, Batch: 217, Train Loss: 1.400587\n",
      "Epoch: 0, Batch: 218, Train Loss: 1.393301\n",
      "Epoch: 0, Batch: 219, Train Loss: 1.414693\n",
      "Epoch: 0, Batch: 220, Train Loss: 1.416346\n",
      "Epoch: 0, Batch: 220, Train Loss: 1.416346, Validation Loss: 1.440301\n",
      "The Committee of English and Chronecter (bannon)|Muslam]]'' (1975), thirty, since 1973 (29). The [[Cong\n",
      "\n",
      "Epoch: 0, Batch: 221, Train Loss: 1.416092\n",
      "Epoch: 0, Batch: 222, Train Loss: 1.411575\n",
      "Epoch: 0, Batch: 223, Train Loss: 1.434477\n",
      "Epoch: 0, Batch: 224, Train Loss: 1.419278\n",
      "Epoch: 0, Batch: 225, Train Loss: 1.410566\n",
      "Epoch: 0, Batch: 226, Train Loss: 1.417868\n",
      "Epoch: 0, Batch: 227, Train Loss: 1.414344\n",
      "Epoch: 0, Batch: 228, Train Loss: 1.400438\n",
      "Epoch: 0, Batch: 229, Train Loss: 1.405145\n",
      "Epoch: 0, Batch: 230, Train Loss: 1.403178\n",
      "Epoch: 0, Batch: 230, Train Loss: 1.403178, Validation Loss: 1.439711\n",
      "The [[Poendyne]] form is a sound conflect of [[stanking painting]] or the people's [[postruphor]] (it. \n",
      "\n",
      "Epoch: 0, Batch: 231, Train Loss: 1.406404\n",
      "Epoch: 0, Batch: 232, Train Loss: 1.397625\n",
      "Epoch: 0, Batch: 233, Train Loss: 1.404668\n",
      "Epoch: 0, Batch: 234, Train Loss: 1.396134\n",
      "Epoch: 0, Batch: 235, Train Loss: 1.405257\n",
      "Epoch: 0, Batch: 236, Train Loss: 1.405278\n",
      "Epoch: 0, Batch: 237, Train Loss: 1.383995\n",
      "Epoch: 0, Batch: 238, Train Loss: 1.405821\n",
      "Epoch: 0, Batch: 239, Train Loss: 1.426075\n",
      "Epoch: 0, Batch: 240, Train Loss: 1.397496\n",
      "Epoch: 0, Batch: 240, Train Loss: 1.397496, Validation Loss: 1.436024\n",
      "The Cream was over see inspired by this team of [[Christianity|August 2]] [[1889]], there winner inform\n",
      "\n",
      "Epoch: 0, Batch: 241, Train Loss: 1.421445\n",
      "Epoch: 0, Batch: 242, Train Loss: 1.401792\n",
      "Epoch: 0, Batch: 243, Train Loss: 1.395761\n",
      "Epoch: 0, Batch: 244, Train Loss: 1.398021\n",
      "Epoch: 0, Batch: 245, Train Loss: 1.415801\n",
      "Epoch: 0, Batch: 246, Train Loss: 1.412153\n",
      "Epoch: 0, Batch: 247, Train Loss: 1.424966\n",
      "Epoch: 0, Batch: 248, Train Loss: 1.424157\n",
      "Epoch: 0, Batch: 249, Train Loss: 1.396301\n",
      "Epoch: 0, Batch: 250, Train Loss: 1.400912\n",
      "Epoch: 0, Batch: 250, Train Loss: 1.400912, Validation Loss: 1.437226\n",
      "The [[Saint Stance]] and the server of texts.\n",
      "\n",
      "The first tied to the south was these practice. \n",
      "\n",
      "==Seve\n",
      "\n",
      "Epoch: 0, Batch: 251, Train Loss: 1.393556\n",
      "Epoch: 0, Batch: 252, Train Loss: 1.392193\n",
      "Epoch: 0, Batch: 253, Train Loss: 1.382731\n",
      "Epoch: 0, Batch: 254, Train Loss: 1.392700\n",
      "Epoch: 0, Batch: 255, Train Loss: 1.410798\n",
      "Epoch: 0, Batch: 256, Train Loss: 1.387530\n",
      "Epoch: 0, Batch: 257, Train Loss: 1.384500\n",
      "Epoch: 0, Batch: 258, Train Loss: 1.405419\n",
      "Epoch: 0, Batch: 259, Train Loss: 1.413787\n",
      "Epoch: 0, Batch: 260, Train Loss: 1.418797\n",
      "Epoch: 0, Batch: 260, Train Loss: 1.418797, Validation Loss: 1.434693\n",
      "The Crown is a service, and art offending a started but some of the straign of [[Antioco]] and the [[Al\n",
      "\n",
      "Epoch: 0, Batch: 261, Train Loss: 1.409136\n",
      "Epoch: 0, Batch: 262, Train Loss: 1.407944\n",
      "Epoch: 0, Batch: 263, Train Loss: 1.393954\n",
      "Epoch: 0, Batch: 264, Train Loss: 1.395679\n",
      "Epoch: 0, Batch: 265, Train Loss: 1.397987\n",
      "Epoch: 0, Batch: 266, Train Loss: 1.379598\n",
      "Epoch: 0, Batch: 267, Train Loss: 1.384923\n",
      "Epoch: 0, Batch: 268, Train Loss: 1.386616\n",
      "Epoch: 0, Batch: 269, Train Loss: 1.391985\n",
      "Epoch: 0, Batch: 270, Train Loss: 1.422036\n",
      "Epoch: 0, Batch: 270, Train Loss: 1.422036, Validation Loss: 1.435844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The [[Californism]] and [[Sports of Encyclopedia of Chile Alexander]].\n",
      "\n",
      "=== Chick ===\n",
      "\n",
      "* [[Antarcticali\n",
      "\n",
      "Epoch: 0, Batch: 271, Train Loss: 1.409552\n",
      "Epoch: 0, Batch: 272, Train Loss: 1.397348\n",
      "Epoch: 0, Batch: 273, Train Loss: 1.397550\n",
      "Epoch: 0, Batch: 274, Train Loss: 1.405614\n",
      "Epoch: 0, Batch: 275, Train Loss: 1.383740\n",
      "Epoch: 0, Batch: 276, Train Loss: 1.385788\n",
      "Epoch: 0, Batch: 277, Train Loss: 1.401356\n",
      "Epoch: 0, Batch: 278, Train Loss: 1.381449\n",
      "Epoch: 0, Batch: 279, Train Loss: 1.402495\n",
      "Epoch: 0, Batch: 280, Train Loss: 1.367679\n",
      "Epoch: 0, Batch: 280, Train Loss: 1.367679, Validation Loss: 1.427359\n",
      "The music and problem of about 180, the southeast of the [[Belinus Christmas]]. He was advantage into t\n",
      "\n",
      "Epoch: 0, Batch: 281, Train Loss: 1.404460\n",
      "Epoch: 0, Batch: 282, Train Loss: 1.406722\n",
      "Epoch: 0, Batch: 283, Train Loss: 1.415472\n",
      "Epoch: 0, Batch: 284, Train Loss: 1.400039\n",
      "Epoch: 0, Batch: 285, Train Loss: 1.394577\n",
      "Epoch: 0, Batch: 286, Train Loss: 1.387163\n",
      "Epoch: 0, Batch: 287, Train Loss: 1.387692\n",
      "Epoch: 0, Batch: 288, Train Loss: 1.386464\n",
      "Epoch: 0, Batch: 289, Train Loss: 1.408089\n",
      "Epoch: 0, Batch: 290, Train Loss: 1.408843\n",
      "Epoch: 0, Batch: 290, Train Loss: 1.408843, Validation Loss: 1.427185\n",
      "These studios in a distinction of this singer it is accepted today this are converting and other tries.\n",
      "\n",
      "Epoch: 0, Batch: 291, Train Loss: 1.408111\n",
      "Epoch: 0, Batch: 292, Train Loss: 1.394616\n",
      "Epoch: 0, Batch: 293, Train Loss: 1.402043\n",
      "Epoch: 0, Batch: 294, Train Loss: 1.398034\n",
      "Epoch: 0, Batch: 295, Train Loss: 1.384327\n",
      "Epoch: 0, Batch: 296, Train Loss: 1.379035\n",
      "Epoch: 0, Batch: 297, Train Loss: 1.386596\n",
      "Epoch: 0, Batch: 298, Train Loss: 1.387046\n",
      "Epoch: 0, Batch: 299, Train Loss: 1.382516\n",
      "Epoch: 0, Batch: 300, Train Loss: 1.386603\n",
      "Epoch: 0, Batch: 300, Train Loss: 1.386603, Validation Loss: 1.424338\n",
      "The [[Southeast of Scotts]].\n",
      "\n",
      "This is a song the same and standard starring both states, it. It is stud\n",
      "\n",
      "Epoch: 0, Batch: 301, Train Loss: 1.386664\n",
      "Epoch: 0, Batch: 302, Train Loss: 1.385508\n",
      "Epoch: 0, Batch: 303, Train Loss: 1.403491\n",
      "Epoch: 0, Batch: 304, Train Loss: 1.390017\n",
      "Epoch: 0, Batch: 305, Train Loss: 1.385984\n",
      "Epoch: 0, Batch: 306, Train Loss: 1.388042\n",
      "Epoch: 0, Batch: 307, Train Loss: 1.397884\n",
      "Epoch: 0, Batch: 308, Train Loss: 1.409431\n",
      "Epoch: 0, Batch: 309, Train Loss: 1.410576\n",
      "Epoch: 0, Batch: 310, Train Loss: 1.403643\n",
      "Epoch: 0, Batch: 310, Train Loss: 1.403643, Validation Loss: 1.423167\n",
      "The city of San Ancient Court of the Apprise in [[South Boy Park|Samisching]] were things that the [[Ch\n",
      "\n",
      "Epoch: 0, Batch: 311, Train Loss: 1.399166\n",
      "Epoch: 0, Batch: 312, Train Loss: 1.388282\n",
      "Epoch: 0, Batch: 313, Train Loss: 1.412707\n",
      "Epoch: 0, Batch: 314, Train Loss: 1.385967\n",
      "Epoch: 0, Batch: 315, Train Loss: 1.408501\n",
      "Epoch: 0, Batch: 316, Train Loss: 1.397778\n",
      "Epoch: 0, Batch: 317, Train Loss: 1.406370\n",
      "Epoch: 0, Batch: 318, Train Loss: 1.412118\n",
      "Epoch: 0, Batch: 319, Train Loss: 1.400640\n",
      "Epoch: 0, Batch: 320, Train Loss: 1.385964\n",
      "Epoch: 0, Batch: 320, Train Loss: 1.385964, Validation Loss: 1.422694\n",
      "The source of community is they responsible on thousand that it was tracks with a street or countil tha\n",
      "\n",
      "Epoch: 0, Batch: 321, Train Loss: 1.390586\n",
      "Epoch: 0, Batch: 322, Train Loss: 1.378466\n",
      "Epoch: 0, Batch: 323, Train Loss: 1.404764\n",
      "Epoch: 0, Batch: 324, Train Loss: 1.406780\n",
      "Epoch: 0, Batch: 325, Train Loss: 1.393917\n",
      "Epoch: 0, Batch: 326, Train Loss: 1.401267\n",
      "Epoch: 0, Batch: 327, Train Loss: 1.390044\n",
      "Epoch: 0, Batch: 328, Train Loss: 1.415769\n",
      "Epoch: 0, Batch: 329, Train Loss: 1.380861\n",
      "Epoch: 0, Batch: 330, Train Loss: 1.394741\n",
      "Epoch: 0, Batch: 330, Train Loss: 1.394741, Validation Loss: 1.420479\n",
      "The [[Serious Ellister]], and [[Alexandrica Araban]]s were all the [[Argent Spain in Common Canadian]]'\n",
      "\n",
      "Epoch: 0, Batch: 331, Train Loss: 1.396137\n",
      "Epoch: 0, Batch: 332, Train Loss: 1.391463\n",
      "Epoch: 0, Batch: 333, Train Loss: 1.378076\n",
      "Epoch: 0, Batch: 334, Train Loss: 1.387882\n",
      "Epoch: 0, Batch: 335, Train Loss: 1.372389\n",
      "Epoch: 0, Batch: 336, Train Loss: 1.361980\n",
      "Epoch: 0, Batch: 337, Train Loss: 1.365894\n",
      "Epoch: 0, Batch: 338, Train Loss: 1.364756\n",
      "Epoch: 0, Batch: 339, Train Loss: 1.381582\n",
      "Epoch: 0, Batch: 340, Train Loss: 1.385884\n",
      "Epoch: 0, Batch: 340, Train Loss: 1.385884, Validation Loss: 1.419324\n",
      "The [[Ann America]] which in the [[Sales Cub]] and [[September 2]] as [[St Branch|Constitution]] and th\n",
      "\n",
      "Epoch: 0, Batch: 341, Train Loss: 1.388534\n",
      "Epoch: 0, Batch: 342, Train Loss: 1.406119\n",
      "Epoch: 0, Batch: 343, Train Loss: 1.403820\n",
      "Epoch: 0, Batch: 344, Train Loss: 1.408780\n",
      "Epoch: 0, Batch: 345, Train Loss: 1.399104\n",
      "Epoch: 0, Batch: 346, Train Loss: 1.380522\n",
      "Epoch: 0, Batch: 347, Train Loss: 1.385384\n",
      "Epoch: 0, Batch: 348, Train Loss: 1.400342\n",
      "Epoch: 0, Batch: 349, Train Loss: 1.401301\n",
      "Epoch: 0, Batch: 350, Train Loss: 1.413259\n",
      "Epoch: 0, Batch: 350, Train Loss: 1.413259, Validation Loss: 1.419515\n",
      "The means the sociolophy is conclusive and town to represent its superior intimate for this in a follow\n",
      "\n",
      "Epoch: 0, Batch: 351, Train Loss: 1.393321\n",
      "Epoch: 0, Batch: 352, Train Loss: 1.397779\n",
      "Epoch: 0, Batch: 353, Train Loss: 1.393863\n",
      "Epoch: 0, Batch: 354, Train Loss: 1.407436\n",
      "Epoch: 0, Batch: 355, Train Loss: 1.381088\n",
      "Epoch: 0, Batch: 356, Train Loss: 1.370508\n",
      "Epoch: 0, Batch: 357, Train Loss: 1.374156\n",
      "Epoch: 0, Batch: 358, Train Loss: 1.381274\n",
      "Epoch: 0, Batch: 359, Train Loss: 1.369550\n",
      "Epoch: 0, Batch: 360, Train Loss: 1.355225\n",
      "Epoch: 0, Batch: 360, Train Loss: 1.355225, Validation Loss: 1.414751\n",
      "The saign of the [[Communist Sea Canada]] ([[1985]]).\n",
      "*[[1941]] - [[Albania]], [[John Cataland|Alea]] (\n",
      "\n",
      "Epoch: 0, Batch: 361, Train Loss: 1.386990\n",
      "Epoch: 0, Batch: 362, Train Loss: 1.382301\n",
      "Epoch: 0, Batch: 363, Train Loss: 1.371393\n",
      "Epoch: 0, Batch: 364, Train Loss: 1.389288\n",
      "Epoch: 0, Batch: 365, Train Loss: 1.382980\n",
      "Epoch: 0, Batch: 366, Train Loss: 1.378852\n",
      "Epoch: 0, Batch: 367, Train Loss: 1.367032\n",
      "Epoch: 0, Batch: 368, Train Loss: 1.377410\n",
      "Epoch: 0, Batch: 369, Train Loss: 1.395939\n",
      "Epoch: 0, Batch: 370, Train Loss: 1.389849\n",
      "Epoch: 0, Batch: 370, Train Loss: 1.389849, Validation Loss: 1.414261\n",
      "The California date of the [[Scotting tax|Sampacta College]]. This is complete in parts on the concerni\n",
      "\n",
      "Epoch: 0, Batch: 371, Train Loss: 1.413982\n",
      "Epoch: 0, Batch: 372, Train Loss: 1.407186\n",
      "Epoch: 0, Batch: 373, Train Loss: 1.381258\n",
      "Epoch: 0, Batch: 374, Train Loss: 1.378958\n",
      "Epoch: 0, Batch: 375, Train Loss: 1.393772\n",
      "Epoch: 0, Batch: 376, Train Loss: 1.392043\n",
      "Epoch: 0, Batch: 377, Train Loss: 1.379714\n",
      "Epoch: 0, Batch: 378, Train Loss: 1.386894\n",
      "Epoch: 0, Batch: 379, Train Loss: 1.378905\n",
      "Epoch: 0, Batch: 380, Train Loss: 1.357731\n",
      "Epoch: 0, Batch: 380, Train Loss: 1.357731, Validation Loss: 1.414376\n",
      "The [[American Careeter of Anguistry]], that their complare the [[American Comminent Grun]] in [[1868]]\n",
      "\n",
      "Epoch: 0, Batch: 381, Train Loss: 1.385441\n",
      "Epoch: 0, Batch: 382, Train Loss: 1.378967\n",
      "Epoch: 0, Batch: 383, Train Loss: 1.366756\n",
      "Epoch: 0, Batch: 384, Train Loss: 1.359409\n",
      "Epoch: 0, Batch: 385, Train Loss: 1.378930\n",
      "Epoch: 0, Batch: 386, Train Loss: 1.385591\n",
      "Epoch: 0, Batch: 387, Train Loss: 1.396672\n",
      "Epoch: 0, Batch: 388, Train Loss: 1.380126\n",
      "Epoch: 0, Batch: 389, Train Loss: 1.386926\n",
      "Epoch: 0, Batch: 390, Train Loss: 1.366759\n",
      "Epoch: 0, Batch: 390, Train Loss: 1.366759, Validation Loss: 1.413756\n",
      "The [[Brazi Achole]], [[Anno Buller]] [[1996]] that is a [[free statu]].  The common works which was su\n",
      "\n",
      "Epoch: 0, Batch: 391, Train Loss: 1.362343\n",
      "Epoch: 0, Batch: 392, Train Loss: 1.375402\n",
      "Epoch: 0, Batch: 393, Train Loss: 1.398918\n",
      "Epoch: 0, Batch: 394, Train Loss: 1.399137\n",
      "Epoch: 0, Batch: 395, Train Loss: 1.392869\n",
      "Epoch: 0, Batch: 396, Train Loss: 1.379701\n",
      "Epoch: 0, Batch: 397, Train Loss: 1.347059\n",
      "Epoch: 0, Batch: 398, Train Loss: 1.369195\n",
      "Epoch: 0, Batch: 399, Train Loss: 1.363762\n",
      "Epoch: 0, Batch: 400, Train Loss: 1.379960\n",
      "Epoch: 0, Batch: 400, Train Loss: 1.379960, Validation Loss: 1.409302\n",
      "The Crowded Prediction of the [[Courts and Chemistry|Portial Associations of Australia]] and it is as a\n",
      "\n",
      "Epoch: 0, Batch: 401, Train Loss: 1.386534\n",
      "Epoch: 0, Batch: 402, Train Loss: 1.382080\n",
      "Epoch: 0, Batch: 403, Train Loss: 1.389693\n",
      "Epoch: 0, Batch: 404, Train Loss: 1.390186\n",
      "Epoch: 0, Batch: 405, Train Loss: 1.372970\n",
      "Epoch: 0, Batch: 406, Train Loss: 1.375406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 407, Train Loss: 1.396941\n",
      "Epoch: 0, Batch: 408, Train Loss: 1.390780\n",
      "Epoch: 0, Batch: 409, Train Loss: 1.395821\n",
      "Epoch: 0, Batch: 410, Train Loss: 1.400208\n",
      "Epoch: 0, Batch: 410, Train Loss: 1.400208, Validation Loss: 1.408961\n",
      "Their champson as ''[[The Alley of the Corid Constantina]]'. The southeast of [[Albaniani]] [[Romanic s\n",
      "\n",
      "Epoch: 0, Batch: 411, Train Loss: 1.416994\n",
      "Epoch: 0, Batch: 412, Train Loss: 1.394904\n",
      "Epoch: 0, Batch: 413, Train Loss: 1.396074\n",
      "Epoch: 0, Batch: 414, Train Loss: 1.413489\n",
      "Epoch: 0, Batch: 415, Train Loss: 1.385878\n",
      "Epoch: 0, Batch: 416, Train Loss: 1.392540\n",
      "Epoch: 0, Batch: 417, Train Loss: 1.391145\n",
      "Epoch: 0, Batch: 418, Train Loss: 1.406274\n",
      "Epoch: 0, Batch: 419, Train Loss: 1.395228\n",
      "Epoch: 0, Batch: 420, Train Loss: 1.376374\n",
      "Epoch: 0, Batch: 420, Train Loss: 1.376374, Validation Loss: 1.408531\n",
      "The Council of the [[USB organizationatory]], and that the come of the first story of [[Malaia]] in [[1\n",
      "\n",
      "Epoch: 0, Batch: 421, Train Loss: 1.387296\n",
      "Epoch: 0, Batch: 422, Train Loss: 1.395867\n",
      "Epoch: 0, Batch: 423, Train Loss: 1.374673\n",
      "Epoch: 0, Batch: 424, Train Loss: 1.380318\n",
      "Epoch: 0, Batch: 425, Train Loss: 1.362722\n",
      "Epoch: 0, Batch: 426, Train Loss: 1.376588\n",
      "Epoch: 0, Batch: 427, Train Loss: 1.390532\n",
      "Epoch: 0, Batch: 428, Train Loss: 1.405657\n",
      "Epoch: 0, Batch: 429, Train Loss: 1.393062\n",
      "Epoch: 0, Batch: 430, Train Loss: 1.399972\n",
      "Epoch: 0, Batch: 430, Train Loss: 1.399972, Validation Loss: 1.405102\n",
      "There are nece travel that to success that it should be subgressed on to the more anti-car form of perc\n",
      "\n",
      "Epoch: 0, Batch: 431, Train Loss: 1.392620\n",
      "Epoch: 0, Batch: 432, Train Loss: 1.396545\n",
      "Epoch: 0, Batch: 433, Train Loss: 1.378970\n",
      "Epoch: 0, Batch: 434, Train Loss: 1.361341\n",
      "Epoch: 0, Batch: 435, Train Loss: 1.350177\n",
      "Epoch: 0, Batch: 436, Train Loss: 1.358285\n",
      "Epoch: 0, Batch: 437, Train Loss: 1.374216\n",
      "Epoch: 0, Batch: 438, Train Loss: 1.362671\n",
      "Epoch: 0, Batch: 439, Train Loss: 1.363807\n",
      "Epoch: 0, Batch: 440, Train Loss: 1.378689\n",
      "Epoch: 0, Batch: 440, Train Loss: 1.378689, Validation Loss: 1.408002\n",
      "The comic colonisians in a direction of any off in their state and also states attempts and side, they \n",
      "\n",
      "Epoch: 0, Batch: 441, Train Loss: 1.393486\n",
      "Epoch: 0, Batch: 442, Train Loss: 1.356715\n",
      "Epoch: 0, Batch: 443, Train Loss: 1.394421\n",
      "Epoch: 0, Batch: 444, Train Loss: 1.392954\n",
      "Epoch: 0, Batch: 445, Train Loss: 1.397046\n",
      "Epoch: 0, Batch: 446, Train Loss: 1.407609\n",
      "Epoch: 0, Batch: 447, Train Loss: 1.380442\n",
      "Epoch: 0, Batch: 448, Train Loss: 1.376776\n",
      "Epoch: 0, Batch: 449, Train Loss: 1.383335\n",
      "Epoch: 0, Batch: 450, Train Loss: 1.366578\n",
      "Epoch: 0, Batch: 450, Train Loss: 1.366578, Validation Loss: 1.404668\n",
      "The Soviet Bachel areas]], as in which [[Medical Colony]] when the following [[Arctic Basings|Baseball]\n",
      "\n",
      "Epoch: 0, Batch: 451, Train Loss: 1.380172\n",
      "Epoch: 0, Batch: 452, Train Loss: 1.371076\n",
      "Epoch: 0, Batch: 453, Train Loss: 1.377410\n",
      "Epoch: 0, Batch: 454, Train Loss: 1.357172\n",
      "Epoch: 0, Batch: 455, Train Loss: 1.358044\n",
      "Epoch: 0, Batch: 456, Train Loss: 1.382231\n",
      "Epoch: 0, Batch: 457, Train Loss: 1.375894\n",
      "Epoch: 0, Batch: 458, Train Loss: 1.384744\n",
      "Epoch: 0, Batch: 459, Train Loss: 1.384225\n",
      "Epoch: 0, Batch: 460, Train Loss: 1.378328\n",
      "Epoch: 0, Batch: 460, Train Loss: 1.378328, Validation Loss: 1.403463\n",
      "The [[Australian Emperors and South]] of the Soviet [[1990]], the [[Commandare College]], associated by\n",
      "\n",
      "Epoch: 0, Batch: 461, Train Loss: 1.371604\n",
      "Epoch: 0, Batch: 462, Train Loss: 1.381893\n",
      "Epoch: 0, Batch: 463, Train Loss: 1.394854\n",
      "Epoch: 0, Batch: 464, Train Loss: 1.370081\n",
      "Epoch: 0, Batch: 465, Train Loss: 1.373063\n",
      "Epoch: 0, Batch: 466, Train Loss: 1.371646\n",
      "Epoch: 0, Batch: 467, Train Loss: 1.386524\n",
      "Epoch: 0, Batch: 468, Train Loss: 1.400105\n",
      "Epoch: 0, Batch: 469, Train Loss: 1.379878\n",
      "Epoch: 0, Batch: 470, Train Loss: 1.398560\n",
      "Epoch: 0, Batch: 470, Train Loss: 1.398560, Validation Loss: 1.403396\n",
      "The County or the [[United States]] of Stalin and Saint Army on [[Antonium]] where the city was could n\n",
      "\n",
      "Epoch: 0, Batch: 471, Train Loss: 1.369643\n",
      "Epoch: 0, Batch: 472, Train Loss: 1.367653\n",
      "Epoch: 0, Batch: 473, Train Loss: 1.361640\n",
      "Epoch: 0, Batch: 474, Train Loss: 1.373435\n",
      "Epoch: 0, Batch: 475, Train Loss: 1.394239\n",
      "Epoch: 0, Batch: 476, Train Loss: 1.398359\n",
      "Epoch: 0, Batch: 477, Train Loss: 1.378630\n",
      "Epoch: 0, Batch: 478, Train Loss: 1.378281\n",
      "Epoch: 0, Batch: 479, Train Loss: 1.388663\n",
      "Epoch: 0, Batch: 480, Train Loss: 1.400307\n",
      "Epoch: 0, Batch: 480, Train Loss: 1.400307, Validation Loss: 1.402474\n",
      "The [[Saint Alexander|Philite Australia]] in [[1884]]. This is a possible cases of an administive. This\n",
      "\n",
      "Epoch: 0, Batch: 481, Train Loss: 1.372282\n",
      "Epoch: 0, Batch: 482, Train Loss: 1.381329\n",
      "Epoch: 0, Batch: 483, Train Loss: 1.378993\n",
      "Epoch: 0, Batch: 484, Train Loss: 1.380129\n",
      "Epoch: 0, Batch: 485, Train Loss: 1.388167\n",
      "Epoch: 0, Batch: 486, Train Loss: 1.367765\n",
      "Epoch: 0, Batch: 487, Train Loss: 1.361888\n",
      "Epoch: 0, Batch: 488, Train Loss: 1.373846\n",
      "Epoch: 0, Batch: 489, Train Loss: 1.374019\n",
      "Epoch: 0, Batch: 490, Train Loss: 1.367795\n",
      "Epoch: 0, Batch: 490, Train Loss: 1.367795, Validation Loss: 1.403643\n",
      "The previously of the parties, ite all there are severe their cultural instruments, and the particle of\n",
      "\n",
      "Epoch: 0, Batch: 491, Train Loss: 1.376871\n",
      "Epoch: 0, Batch: 492, Train Loss: 1.366738\n",
      "Epoch: 0, Batch: 493, Train Loss: 1.393417\n",
      "Epoch: 0, Batch: 494, Train Loss: 1.412219\n",
      "Epoch: 0, Batch: 495, Train Loss: 1.385795\n",
      "Epoch: 0, Batch: 496, Train Loss: 1.382135\n",
      "Epoch: 0, Batch: 497, Train Loss: 1.393200\n",
      "Epoch: 0, Batch: 498, Train Loss: 1.373066\n",
      "Epoch: 0, Batch: 499, Train Loss: 1.367556\n",
      "Epoch: 0, Batch: 500, Train Loss: 1.380106\n",
      "Epoch: 0, Batch: 500, Train Loss: 1.380106, Validation Loss: 1.399953\n",
      "The collection in the [[Secretary Calitor of Broadian]], [[Borg Barker Business]] was two [[calendar]] \n",
      "\n",
      "Epoch: 0, Batch: 501, Train Loss: 1.369444\n",
      "Epoch: 0, Batch: 502, Train Loss: 1.377864\n",
      "Epoch: 0, Batch: 503, Train Loss: 1.379167\n",
      "Epoch: 0, Batch: 504, Train Loss: 1.375005\n",
      "Epoch: 0, Batch: 505, Train Loss: 1.372344\n",
      "Epoch: 0, Batch: 506, Train Loss: 1.357662\n",
      "Epoch: 0, Batch: 507, Train Loss: 1.361426\n",
      "Epoch: 0, Batch: 508, Train Loss: 1.350720\n",
      "Epoch: 0, Batch: 509, Train Loss: 1.364101\n",
      "Epoch: 0, Batch: 510, Train Loss: 1.354546\n",
      "Epoch: 0, Batch: 510, Train Loss: 1.354546, Validation Loss: 1.399665\n",
      "The same times is a cetaid such an imprivable buse. The persons of the [[County of the Coloce]], [[Corp\n",
      "\n",
      "Epoch: 0, Batch: 511, Train Loss: 1.370212\n",
      "Epoch: 0, Batch: 512, Train Loss: 1.364188\n",
      "Epoch: 0, Batch: 513, Train Loss: 1.383541\n",
      "Epoch: 0, Batch: 514, Train Loss: 1.369784\n",
      "Epoch: 0, Batch: 515, Train Loss: 1.361335\n",
      "Epoch: 0, Batch: 516, Train Loss: 1.355187\n",
      "Epoch: 0, Batch: 517, Train Loss: 1.374627\n",
      "Epoch: 0, Batch: 518, Train Loss: 1.373234\n",
      "Epoch: 0, Batch: 519, Train Loss: 1.375624\n",
      "Epoch: 0, Batch: 520, Train Loss: 1.382798\n",
      "Epoch: 0, Batch: 520, Train Loss: 1.382798, Validation Loss: 1.397768\n",
      "These country any original systems are significant to story. Their series.\n",
      "\n",
      "These included is to be the\n",
      "\n",
      "Epoch: 0, Batch: 521, Train Loss: 1.369912\n",
      "Epoch: 0, Batch: 522, Train Loss: 1.363113\n",
      "Epoch: 0, Batch: 523, Train Loss: 1.362508\n",
      "Epoch: 0, Batch: 524, Train Loss: 1.362514\n",
      "Epoch: 0, Batch: 525, Train Loss: 1.370266\n",
      "Epoch: 0, Batch: 526, Train Loss: 1.368227\n",
      "Epoch: 0, Batch: 527, Train Loss: 1.372218\n",
      "Epoch: 0, Batch: 528, Train Loss: 1.357431\n",
      "Epoch: 0, Batch: 529, Train Loss: 1.362112\n",
      "Epoch: 0, Batch: 530, Train Loss: 1.358267\n",
      "Epoch: 0, Batch: 530, Train Loss: 1.358267, Validation Loss: 1.397435\n",
      "The same time, is also an anticidental sentence. They he case in the childhologies, and the places of t\n",
      "\n",
      "Epoch: 0, Batch: 531, Train Loss: 1.356672\n",
      "Epoch: 0, Batch: 532, Train Loss: 1.349626\n",
      "Epoch: 0, Batch: 533, Train Loss: 1.388438\n",
      "Epoch: 0, Batch: 534, Train Loss: 1.386136\n",
      "Epoch: 0, Batch: 535, Train Loss: 1.377425\n",
      "Epoch: 0, Batch: 536, Train Loss: 1.367689\n",
      "Epoch: 0, Batch: 537, Train Loss: 1.356460\n",
      "Epoch: 0, Batch: 538, Train Loss: 1.354058\n",
      "Epoch: 0, Batch: 539, Train Loss: 1.363626\n",
      "Epoch: 0, Batch: 540, Train Loss: 1.357263\n",
      "Epoch: 0, Batch: 540, Train Loss: 1.357263, Validation Loss: 1.392909\n",
      "The police of the country of [[Solorian Emerism]]. It was also but three more consistent acts.  This wa\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 541, Train Loss: 1.346828\n",
      "Epoch: 0, Batch: 542, Train Loss: 1.356484\n",
      "Epoch: 0, Batch: 543, Train Loss: 1.363039\n",
      "Epoch: 0, Batch: 544, Train Loss: 1.368152\n",
      "Epoch: 0, Batch: 545, Train Loss: 1.360947\n",
      "Epoch: 0, Batch: 546, Train Loss: 1.354164\n",
      "Epoch: 0, Batch: 547, Train Loss: 1.367691\n",
      "Epoch: 0, Batch: 548, Train Loss: 1.377237\n",
      "Epoch: 0, Batch: 549, Train Loss: 1.385081\n",
      "Epoch: 0, Batch: 550, Train Loss: 1.384031\n",
      "Epoch: 0, Batch: 550, Train Loss: 1.384031, Validation Loss: 1.392416\n",
      "The Computer America]], the [[Aragon]] in 1640, was following [[particles|Antonese]]. Although he had s\n",
      "\n",
      "Epoch: 0, Batch: 551, Train Loss: 1.362809\n",
      "Epoch: 0, Batch: 552, Train Loss: 1.361419\n",
      "Epoch: 0, Batch: 553, Train Loss: 1.360672\n",
      "Epoch: 0, Batch: 554, Train Loss: 1.354573\n",
      "Epoch: 0, Batch: 555, Train Loss: 1.371560\n",
      "Epoch: 0, Batch: 556, Train Loss: 1.371862\n",
      "Epoch: 0, Batch: 557, Train Loss: 1.369039\n",
      "Epoch: 0, Batch: 558, Train Loss: 1.364101\n",
      "Epoch: 0, Batch: 559, Train Loss: 1.392281\n",
      "Epoch: 0, Batch: 560, Train Loss: 1.384930\n",
      "Epoch: 0, Batch: 560, Train Loss: 1.384930, Validation Loss: 1.389592\n",
      "The case, an order along with such the [[product from animal]]s, [[managanizator]], [[polycenius]], and\n",
      "\n",
      "Epoch: 0, Batch: 561, Train Loss: 1.373348\n",
      "Epoch: 0, Batch: 562, Train Loss: 1.371948\n",
      "Epoch: 0, Batch: 563, Train Loss: 1.367414\n",
      "Epoch: 0, Batch: 564, Train Loss: 1.375016\n",
      "Epoch: 0, Batch: 565, Train Loss: 1.369449\n",
      "Epoch: 0, Batch: 566, Train Loss: 1.341513\n",
      "Epoch: 0, Batch: 567, Train Loss: 1.341463\n",
      "Epoch: 0, Batch: 568, Train Loss: 1.369809\n",
      "Epoch: 0, Batch: 569, Train Loss: 1.351828\n",
      "Epoch: 0, Batch: 570, Train Loss: 1.365274\n",
      "Epoch: 0, Batch: 570, Train Loss: 1.365274, Validation Loss: 1.389688\n",
      "The [[Cornwall Wars]], that he was the file forcing: and some section to the computation at the [[Colon\n",
      "\n",
      "Epoch: 0, Batch: 571, Train Loss: 1.371469\n",
      "Epoch: 0, Batch: 572, Train Loss: 1.369764\n",
      "Epoch: 0, Batch: 573, Train Loss: 1.355110\n",
      "Epoch: 0, Batch: 574, Train Loss: 1.364505\n",
      "Epoch: 0, Batch: 575, Train Loss: 1.367969\n",
      "Epoch: 0, Batch: 576, Train Loss: 1.348328\n",
      "Epoch: 0, Batch: 577, Train Loss: 1.379151\n",
      "Epoch: 0, Batch: 578, Train Loss: 1.358602\n",
      "Epoch: 0, Batch: 579, Train Loss: 1.370133\n",
      "Epoch: 0, Batch: 580, Train Loss: 1.355594\n",
      "Epoch: 0, Batch: 580, Train Loss: 1.355594, Validation Loss: 1.387752\n",
      "The converting systems takes on his subject and the city. All the processorship to the most released at\n",
      "\n",
      "Epoch: 0, Batch: 581, Train Loss: 1.370129\n",
      "Epoch: 0, Batch: 582, Train Loss: 1.375291\n",
      "Epoch: 0, Batch: 583, Train Loss: 1.350883\n",
      "Epoch: 0, Batch: 584, Train Loss: 1.347040\n",
      "Epoch: 0, Batch: 585, Train Loss: 1.378018\n",
      "Epoch: 0, Batch: 586, Train Loss: 1.335046\n",
      "Epoch: 0, Batch: 587, Train Loss: 1.351393\n",
      "Epoch: 0, Batch: 588, Train Loss: 1.345422\n",
      "Epoch: 0, Batch: 589, Train Loss: 1.359547\n",
      "Epoch: 0, Batch: 590, Train Loss: 1.370177\n",
      "Epoch: 0, Batch: 590, Train Loss: 1.370177, Validation Loss: 1.388527\n",
      "There were constantly as well-astronomer.  The composer of the [[Churcheship]] in [[1911]].\n",
      "\n",
      "An example\n",
      "\n",
      "Epoch: 0, Batch: 591, Train Loss: 1.373598\n",
      "Epoch: 0, Batch: 592, Train Loss: 1.378066\n",
      "Epoch: 0, Batch: 593, Train Loss: 1.370918\n",
      "Epoch: 0, Batch: 594, Train Loss: 1.371709\n",
      "Epoch: 0, Batch: 595, Train Loss: 1.380985\n",
      "Epoch: 0, Batch: 596, Train Loss: 1.377209\n",
      "Epoch: 0, Batch: 597, Train Loss: 1.369982\n",
      "Epoch: 0, Batch: 598, Train Loss: 1.362060\n",
      "Epoch: 0, Batch: 599, Train Loss: 1.366284\n",
      "Epoch: 0, Batch: 600, Train Loss: 1.365245\n",
      "Epoch: 0, Batch: 600, Train Loss: 1.365245, Validation Loss: 1.387066\n",
      "The [[Sovious Park]], which are the formation on the city to the [[User television]] team of a series, \n",
      "\n",
      "Epoch: 0, Batch: 601, Train Loss: 1.383805\n",
      "Epoch: 0, Batch: 602, Train Loss: 1.381760\n",
      "Epoch: 0, Batch: 603, Train Loss: 1.397086\n",
      "Epoch: 0, Batch: 604, Train Loss: 1.359353\n",
      "Epoch: 0, Batch: 605, Train Loss: 1.372409\n",
      "Epoch: 0, Batch: 606, Train Loss: 1.338747\n",
      "Epoch: 0, Batch: 607, Train Loss: 1.359153\n",
      "Epoch: 0, Batch: 608, Train Loss: 1.363249\n",
      "Epoch: 0, Batch: 609, Train Loss: 1.351457\n",
      "Epoch: 0, Batch: 610, Train Loss: 1.361543\n",
      "Epoch: 0, Batch: 610, Train Loss: 1.361543, Validation Loss: 1.387107\n",
      "The [[Senate Chemist]] in the 1970s, associated was opposed to the practice throughout the country of C\n",
      "\n",
      "Epoch: 0, Batch: 611, Train Loss: 1.373149\n",
      "Epoch: 0, Batch: 612, Train Loss: 1.355180\n",
      "Epoch: 0, Batch: 613, Train Loss: 1.352381\n",
      "Epoch: 0, Batch: 614, Train Loss: 1.370509\n",
      "Epoch: 0, Batch: 615, Train Loss: 1.361220\n",
      "Epoch: 0, Batch: 616, Train Loss: 1.388171\n",
      "Epoch: 0, Batch: 617, Train Loss: 1.387368\n",
      "Epoch: 0, Batch: 618, Train Loss: 1.370321\n",
      "Epoch: 0, Batch: 619, Train Loss: 1.384328\n",
      "Epoch: 0, Batch: 620, Train Loss: 1.359149\n",
      "Epoch: 0, Batch: 620, Train Loss: 1.359149, Validation Loss: 1.384128\n",
      "The first team with it was a political radio cast of [[protestic further|manifestory]].  \n",
      "\n",
      "A campiest p\n",
      "\n",
      "Epoch: 0, Batch: 621, Train Loss: 1.342811\n",
      "Epoch: 0, Batch: 622, Train Loss: 1.362329\n",
      "Epoch: 0, Batch: 623, Train Loss: 1.356398\n",
      "Epoch: 0, Batch: 624, Train Loss: 1.360009\n",
      "Epoch: 0, Batch: 625, Train Loss: 1.369319\n",
      "Epoch: 0, Batch: 626, Train Loss: 1.352430\n",
      "Epoch: 0, Batch: 627, Train Loss: 1.368876\n",
      "Epoch: 0, Batch: 628, Train Loss: 1.368449\n",
      "Epoch: 0, Batch: 629, Train Loss: 1.356577\n",
      "Epoch: 0, Batch: 630, Train Loss: 1.359644\n",
      "Epoch: 0, Batch: 630, Train Loss: 1.359644, Validation Loss: 1.384804\n",
      "There were complex that the charatels are now country for the support on [[Standerstone]], the [[Bruna \n",
      "\n",
      "Epoch: 0, Batch: 631, Train Loss: 1.359316\n",
      "Epoch: 0, Batch: 632, Train Loss: 1.344773\n",
      "Epoch: 0, Batch: 633, Train Loss: 1.340075\n",
      "Epoch: 0, Batch: 634, Train Loss: 1.357527\n",
      "Epoch: 0, Batch: 635, Train Loss: 1.355588\n",
      "Epoch: 0, Batch: 636, Train Loss: 1.355301\n",
      "Epoch: 0, Batch: 637, Train Loss: 1.361492\n",
      "Epoch: 0, Batch: 638, Train Loss: 1.369674\n",
      "Epoch: 0, Batch: 639, Train Loss: 1.370519\n",
      "Epoch: 0, Batch: 640, Train Loss: 1.374761\n",
      "Epoch: 0, Batch: 640, Train Loss: 1.374761, Validation Loss: 1.384501\n",
      "Thes therefore although the second team is an opposite church to stands this way to the [[Carthage]]s. \n",
      "\n",
      "Epoch: 0, Batch: 641, Train Loss: 1.366099\n",
      "Epoch: 0, Batch: 642, Train Loss: 1.346844\n"
     ]
    }
   ],
   "source": [
    "val_losses = list() # empty list for the validation losses\n",
    "net.eval()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    # reinit the hidden and cell states\n",
    "    hc = net.init_hidden()\n",
    "    \n",
    "    for i, (x, y) in enumerate(get_batches(data, BS, seq_len)):\n",
    "        x_train, y_true = xy_to_tensor(x, y)\n",
    "        optimizer.zero_grad() # zero out the gradients\n",
    "    \n",
    "        # forward pass\n",
    "        t0 = time.time()\n",
    "        y_pred = net(x_train, hc)\n",
    "        if i == 0:\n",
    "            print(f'elapsed forward: {time.time() - t0}')\n",
    "    \n",
    "        # calculate the loss\n",
    "        # we need to calculate the loss across all batches, so we have to flat the y_true tensor\n",
    "        loss = criterion(y_pred.view(BS*seq_len, -1), y_true.view(BS*seq_len)) # .contiguous()?\n",
    "        \n",
    "        # calculate the gradients\n",
    "        t0 = time.time()\n",
    "        loss.backward()\n",
    "        if i == 0:\n",
    "            print(f'elapsed backward: {time.time() - t0}')\n",
    "        \n",
    "        # update the parameters of the model\n",
    "        optimizer.step()\n",
    "    \n",
    "        print(\"Epoch: {}, Batch: {}, Train Loss: {:.6f}\".format(epoch, i, loss.item()))\n",
    "\n",
    "        # feedback every 10 batches\n",
    "        with torch.no_grad():\n",
    "            if i % 10 == 0: \n",
    "                net.eval()\n",
    "                \n",
    "                x, y = next(get_batches(val_data, BS, seq_len))\n",
    "                x_val, y_val = xy_to_tensor(x, y)\n",
    "                hc_val = net.init_hidden()\n",
    "                y_pred = net(x_val, hc_val)\n",
    "                loss_val = criterion(y_pred.view(BS*seq_len, -1), y_val.view(BS*seq_len)) # .contiguous()?\n",
    "                \n",
    "                net.train()\n",
    "                print(\"Epoch: {}, Batch: {}, Train Loss: {:.6f}, Validation Loss: {:.6f}\".format(epoch, i, loss.item(), loss_val.item()))\n",
    "\n",
    "                sample = net.predict(\"The\", seq_len=100)\n",
    "                print(sample)\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.predict(\"God i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = dict(net=net, char2int=char2int, int2char=int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(state, \"save_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = torch.load(\"save_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sold of the [[United States and]].  These provides to be resting team at the sere of the same that a charaction as as a stabou d'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.predict(\"The \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
