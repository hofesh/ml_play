{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:13:49.834155Z",
     "start_time": "2019-04-15T07:13:48.169695Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/udacity/deep-learning/blob/master/tensorboard/anna.txt\n",
    "# enwik8: http://prize.hutter1.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:17.508675Z",
     "start_time": "2019-04-15T07:26:17.505385Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pdb\n",
    "import time\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:17.735041Z",
     "start_time": "2019-04-15T07:26:17.720748Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = Path(\"enwik8\")\n",
    "text = path.open(encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:17.735041Z",
     "start_time": "2019-04-15T07:26:17.720748Z"
    }
   },
   "outputs": [],
   "source": [
    "printable = set(string.printable)\n",
    "text_clean = list(filter(lambda x: x in printable, text))\n",
    "\n",
    "# get the set of all characters\n",
    "characters = tuple(set(text_clean))\n",
    "\n",
    "# use enumeration to give the characters integer values\n",
    "int2char = dict(enumerate(characters))\n",
    "\n",
    "# create the look up dictionary from characters to the assigned integers\n",
    "char2int = {char: index for index, char in int2char.items()}\n",
    "\n",
    "# encode the text, using the character to integer dictionary\n",
    "encoded = np.array([char2int[char] for char in text_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_gpu = True\n",
    "def gpu(m):\n",
    "    if to_gpu:\n",
    "        return m.cuda()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:17.901416Z",
     "start_time": "2019-04-15T07:26:17.894616Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs_in_a_batch, n_characters):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs_in_a_batch * n_characters\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs_in_a_batch, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_characters):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_characters]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_characters]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ints_to_tensor(ints):\n",
    "    return gpu(torch.tensor(ints).long().transpose(1, 0))\n",
    "\n",
    "def xy_to_tensor(x, y):\n",
    "    x = ints_to_tensor(x)\n",
    "    y = torch.tensor(y.T).long()\n",
    "    return x, gpu(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:18.081312Z",
     "start_time": "2019-04-15T07:26:18.064093Z"
    }
   },
   "outputs": [],
   "source": [
    "# build the model using the pytorch nn module\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, batch_size, embedding_dim):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        \n",
    "        # init the meta parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm_1 = nn.LSTMCell(input_size=embedding_dim, hidden_size=hidden_dim)\n",
    "        self.lstm_2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=hidden_dim) \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # fully connected layer to connect the output of the LSTM cell to the output\n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
    "        \n",
    "    def forward(self, x, hc, return_hc=False):\n",
    "        seq_len = x.shape[0]\n",
    "        batch_size = x.shape[1]\n",
    "        \n",
    "        # empty tensor for the output of the lstm\n",
    "        output_seq = torch.empty((seq_len, batch_size, self.vocab_size))\n",
    "        output_seq = gpu(output_seq)\n",
    "        hc1, hc2 = hc, hc\n",
    "\n",
    "        # for every step in the sequence\n",
    "        for t in range(seq_len):\n",
    "            out_t, hc1, hc2 = self.feed_one_x_t(x[t], hc1, hc2)\n",
    "            output_seq[t] = out_t\n",
    "        \n",
    "        if return_hc:\n",
    "            return output_seq, hc1, hc2\n",
    "        return output_seq\n",
    "            \n",
    "    def init_hidden(self, bs=None):\n",
    "        if bs is None:\n",
    "            bs = self.batch_size\n",
    "        # initialize the <hidden state> and the <cell state> to zeros\n",
    "        return (gpu(torch.zeros(bs, self.hidden_dim)), gpu(torch.zeros(bs, self.hidden_dim)))\n",
    "    \n",
    "    def feed_one_x_t(self, x_t, hc1, hc2):\n",
    "        # convert batch of single ints to batch of embeddings\n",
    "        xt_emb = self.emb(x_t) # returns (batch_size, embedding_dim)\n",
    "\n",
    "        # get the hidden and cell states from the first layer cell\n",
    "        hc1 = self.lstm_1(xt_emb, hc1)\n",
    "        h1, c1 = hc1 # unpack the hidden and the cell states from the first layer\n",
    "\n",
    "        # pass the hidden state from the first layer to the cell in the second layer\n",
    "        hc2 = self.lstm_2(h1, hc2)\n",
    "        h2, c2 = hc2 # unpack the hidden and cell states from the second layer cell\n",
    "\n",
    "        # form the output of the fc\n",
    "        out_t = self.fc(self.dropout(h2))\n",
    "        \n",
    "        return out_t, hc1, hc2\n",
    "    \n",
    "    def feed_one_char(self, char, hc1, hc2):\n",
    "        ints = [char2int[char]] # sequence of ints \n",
    "        ints = [ints] # a 1-batch of seqs\n",
    "        x = ints_to_tensor(ints) # shape of (seq_len, batch_size)\n",
    "        x_t = x[0] # take the first (single) part of the sequence\n",
    "        \n",
    "        return self.feed_one_x_t(x_t, hc1, hc2)\n",
    "    \n",
    "    def warm_up(self, base_str):\n",
    "        hc = net.init_hidden(bs=1)\n",
    "        ints = [char2int[c] for c in base_str]  # sequence of ints \n",
    "        ints = [ints] # a 1-batch of seqs\n",
    "        x = ints_to_tensor(ints) # shape of (seq_len, batch_size)\n",
    "        \n",
    "        out, hc1, hc2 = self.forward(x, hc, return_hc=True)\n",
    "        return out, hc1, hc2\n",
    "    \n",
    "    def sample_char(self, out_t, top_k=5):\n",
    "        # apply the softmax to the output to get the probabilities of the characters\n",
    "        out_t = F.softmax(out_t, dim=1)\n",
    "\n",
    "        # out_t now holds the vector of predictions (1, vocab_size)\n",
    "        # we want to sample 5 top characters\n",
    "        p, top_char = out_t.topk(top_k) # returns tuple (top_values, top_indices)\n",
    "\n",
    "        # get the top k characters by their probabilities\n",
    "        top_char = top_char.cpu().squeeze().numpy()\n",
    "\n",
    "        # sample a character using its probability\n",
    "        p = p.detach().cpu().squeeze().numpy()\n",
    "        char_int = np.random.choice(top_char, p = p/p.sum())\n",
    "        \n",
    "        return int2char[char_int]\n",
    "        \n",
    "    def predict(self, base_str, top_k=5, seq_len=128):\n",
    "        self.eval()\n",
    "\n",
    "        res = np.empty(seq_len+len(base_str), dtype=\"object\")\n",
    "        for i, c in enumerate(base_str):\n",
    "            res[i] = c\n",
    "        \n",
    "        out_warm, hc1, hc2 = self.warm_up(base_str)\n",
    "        out_t = out_warm[-1]\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            char = self.sample_char(out_t, top_k)\n",
    "            out_t, hc1, hc2 = self.feed_one_char(char, hc1, hc2)\n",
    "            res[i + len(base_str)] = char\n",
    "        \n",
    "        return ''.join(res)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:18.226534Z",
     "start_time": "2019-04-15T07:26:18.224057Z"
    }
   },
   "outputs": [],
   "source": [
    "BS = 500 # 500\n",
    "embedding_dim = 100\n",
    "vocab_size=len(char2int)\n",
    "hidden_dim = 512 # 512\n",
    "seq_len = 128 # 128\n",
    "seq_len_BS = seq_len * BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BS: 500\n",
      "embedding_dim: 100\n",
      "vocab_size: 97\n",
      "hidden_dim: 512\n",
      "seq_len: 128\n",
      "seq_len_BS: 64000\n"
     ]
    }
   ],
   "source": [
    "print(f'BS: {BS}')\n",
    "print(f'embedding_dim: {embedding_dim}')\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "print(f'hidden_dim: {hidden_dim}')\n",
    "print(f'seq_len: {seq_len}')\n",
    "print(f'seq_len_BS: {seq_len_BS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:18.580287Z",
     "start_time": "2019-04-15T07:26:18.576595Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the validation and the training data\n",
    "val_idx = int(len(encoded) * (1 - 0.1))\n",
    "data, val_data = encoded[:val_idx], encoded[val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:26:18.431525Z",
     "start_time": "2019-04-15T07:26:18.395438Z"
    }
   },
   "outputs": [],
   "source": [
    "# compile the network - sequence_len, vocab_size, hidden_dim, batch_size\n",
    "net = CharLSTM(vocab_size=len(char2int), hidden_dim=hidden_dim, batch_size=BS, embedding_dim=embedding_dim)\n",
    "net = gpu(net)\n",
    "\n",
    "# define the loss and the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(get_batches(data, BS, seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = xy_to_tensor(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 500]), torch.Size([128, 500]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 500, 97])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc = net.init_hidden()\n",
    "out = net(x, hc)\n",
    "out.shape # (seq_len, batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.sample_char(out[-1, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TheLLJ(Fyfmok(mFof(mmmoEofmkVEEEmfpp>EE/fkpkkkA(fkkq(Lf(ff%kk=qqEEEEmqEmyEEkAmp2ymmEmypmLLLfLykLfkkqLLLLxyyfLpykf(kkf(%mfEEmpfkmEEp'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(\"The\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T07:27:47.294488Z",
     "start_time": "2019-04-15T07:26:19.006577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed forward: 0.3210113048553467\n",
      "elapsed backward: 0.32642340660095215\n",
      "Epoch: 0, Batch: 0, Train Loss: 4.583667\n",
      "Epoch: 0, Batch: 0, Train Loss: 4.583667, Validation Loss: 4.532392\n",
      "ThemEEomprmyyymymmmmmpEmEpprgyroeoo fooffofomofrmemetrtmrryyoyfmpyoffppet rret he ero te ot rrhrrytorrt\n",
      "\n",
      "Epoch: 0, Batch: 1, Train Loss: 4.532494\n",
      "Epoch: 0, Batch: 2, Train Loss: 4.449418\n",
      "Epoch: 0, Batch: 3, Train Loss: 4.190681\n",
      "Epoch: 0, Batch: 4, Train Loss: 3.696977\n",
      "Epoch: 0, Batch: 5, Train Loss: 3.738126\n",
      "Epoch: 0, Batch: 6, Train Loss: 3.662930\n",
      "Epoch: 0, Batch: 7, Train Loss: 3.658451\n",
      "Epoch: 0, Batch: 8, Train Loss: 3.618943\n",
      "Epoch: 0, Batch: 9, Train Loss: 3.578385\n",
      "Epoch: 0, Batch: 10, Train Loss: 3.552013\n",
      "Epoch: 0, Batch: 10, Train Loss: 3.552013, Validation Loss: 3.510085\n",
      "Ther e aae   at e  an aa nat  t a  at a  t  at ean a tan a   eena n   teat at tae n atnetn  nant    e  \n",
      "\n",
      "Epoch: 0, Batch: 11, Train Loss: 3.537705\n",
      "Epoch: 0, Batch: 12, Train Loss: 3.536778\n",
      "Epoch: 0, Batch: 13, Train Loss: 3.536417\n",
      "Epoch: 0, Batch: 14, Train Loss: 3.540774\n",
      "Epoch: 0, Batch: 15, Train Loss: 3.525661\n",
      "Epoch: 0, Batch: 16, Train Loss: 3.506736\n",
      "Epoch: 0, Batch: 17, Train Loss: 3.506493\n",
      "Epoch: 0, Batch: 18, Train Loss: 3.490820\n",
      "Epoch: 0, Batch: 19, Train Loss: 3.475911\n",
      "Epoch: 0, Batch: 20, Train Loss: 3.471471\n",
      "Epoch: 0, Batch: 20, Train Loss: 3.471471, Validation Loss: 3.457078\n",
      "Theet e  t een t ae   eentannen n aaaena e o an  a  e t onteetea  oaeriatn tan  ere anaae  ae e a  oe t\n",
      "\n",
      "Epoch: 0, Batch: 21, Train Loss: 3.480135\n",
      "Epoch: 0, Batch: 22, Train Loss: 3.473373\n",
      "Epoch: 0, Batch: 23, Train Loss: 3.467098\n",
      "Epoch: 0, Batch: 24, Train Loss: 3.447870\n",
      "Epoch: 0, Batch: 25, Train Loss: 3.458902\n",
      "Epoch: 0, Batch: 26, Train Loss: 3.416266\n",
      "Epoch: 0, Batch: 27, Train Loss: 3.400667\n",
      "Epoch: 0, Batch: 28, Train Loss: 3.393673\n",
      "Epoch: 0, Batch: 29, Train Loss: 3.389321\n",
      "Epoch: 0, Batch: 30, Train Loss: 3.385863\n",
      "Epoch: 0, Batch: 30, Train Loss: 3.385863, Validation Loss: 3.334599\n",
      "Then otoere taine   ate ane toto e oe      ae ennss toe  ea tan o a the ta otierise is  a eot aon oo e \n",
      "\n",
      "Epoch: 0, Batch: 31, Train Loss: 3.374222\n",
      "Epoch: 0, Batch: 32, Train Loss: 3.356233\n",
      "Epoch: 0, Batch: 33, Train Loss: 3.344049\n",
      "Epoch: 0, Batch: 34, Train Loss: 3.321525\n",
      "Epoch: 0, Batch: 35, Train Loss: 3.278094\n",
      "Epoch: 0, Batch: 36, Train Loss: 3.254884\n",
      "Epoch: 0, Batch: 37, Train Loss: 3.224978\n",
      "Epoch: 0, Batch: 38, Train Loss: 3.200872\n",
      "Epoch: 0, Batch: 39, Train Loss: 3.188595\n",
      "Epoch: 0, Batch: 40, Train Loss: 3.178968\n",
      "Epoch: 0, Batch: 40, Train Loss: 3.178968, Validation Loss: 3.118160\n",
      "Theret [[otit ons ites [hare is  [os at o tinter te a in  oreterin  a on tines oo o torese inte the ons\n",
      "\n",
      "Epoch: 0, Batch: 41, Train Loss: 3.162862\n",
      "Epoch: 0, Batch: 42, Train Loss: 3.111224\n",
      "Epoch: 0, Batch: 43, Train Loss: 3.088966\n",
      "Epoch: 0, Batch: 44, Train Loss: 3.077910\n",
      "Epoch: 0, Batch: 45, Train Loss: 3.068243\n",
      "Epoch: 0, Batch: 46, Train Loss: 3.035938\n",
      "Epoch: 0, Batch: 47, Train Loss: 3.043585\n",
      "Epoch: 0, Batch: 48, Train Loss: 3.006890\n",
      "Epoch: 0, Batch: 49, Train Loss: 2.979468\n",
      "Epoch: 0, Batch: 50, Train Loss: 2.960790\n",
      "Epoch: 0, Batch: 50, Train Loss: 2.960790, Validation Loss: 2.930683\n",
      "Thes arerer ther inte f thon or ane a and on th  or ale the torthe art is fante an of th ase thed the t\n",
      "\n",
      "Epoch: 0, Batch: 51, Train Loss: 2.949703\n",
      "Epoch: 0, Batch: 52, Train Loss: 2.936311\n",
      "Epoch: 0, Batch: 53, Train Loss: 2.925877\n",
      "Epoch: 0, Batch: 54, Train Loss: 2.906663\n",
      "Epoch: 0, Batch: 55, Train Loss: 2.890380\n",
      "Epoch: 0, Batch: 56, Train Loss: 2.878025\n",
      "Epoch: 0, Batch: 57, Train Loss: 2.871078\n",
      "Epoch: 0, Batch: 58, Train Loss: 2.853111\n",
      "Epoch: 0, Batch: 59, Train Loss: 2.841599\n",
      "Epoch: 0, Batch: 60, Train Loss: 2.819963\n",
      "Epoch: 0, Batch: 60, Train Loss: 2.819963, Validation Loss: 2.776717\n",
      "Ther icare thith artit ofe terthe silting tor th tha the the and and tan ton th in ant or tor in of tor\n",
      "\n",
      "Epoch: 0, Batch: 61, Train Loss: 2.807519\n",
      "Epoch: 0, Batch: 62, Train Loss: 2.796464\n",
      "Epoch: 0, Batch: 63, Train Loss: 2.794075\n",
      "Epoch: 0, Batch: 64, Train Loss: 2.756455\n",
      "Epoch: 0, Batch: 65, Train Loss: 2.757622\n",
      "Epoch: 0, Batch: 66, Train Loss: 2.744629\n",
      "Epoch: 0, Batch: 67, Train Loss: 2.736554\n",
      "Epoch: 0, Batch: 68, Train Loss: 2.732385\n",
      "Epoch: 0, Batch: 69, Train Loss: 2.703709\n",
      "Epoch: 0, Batch: 70, Train Loss: 2.695677\n",
      "Epoch: 0, Batch: 70, Train Loss: 2.695677, Validation Loss: 2.654368\n",
      "The cinstatereris andere the the a a sered of in the teranse th in the of an the the angendin the te in\n",
      "\n",
      "Epoch: 0, Batch: 71, Train Loss: 2.669955\n",
      "Epoch: 0, Batch: 72, Train Loss: 2.667726\n",
      "Epoch: 0, Batch: 73, Train Loss: 2.656650\n",
      "Epoch: 0, Batch: 74, Train Loss: 2.651579\n",
      "Epoch: 0, Batch: 75, Train Loss: 2.644864\n",
      "Epoch: 0, Batch: 76, Train Loss: 2.639655\n",
      "Epoch: 0, Batch: 77, Train Loss: 2.642407\n",
      "Epoch: 0, Batch: 78, Train Loss: 2.617432\n",
      "Epoch: 0, Batch: 79, Train Loss: 2.602442\n",
      "Epoch: 0, Batch: 80, Train Loss: 2.598273\n",
      "Epoch: 0, Batch: 80, Train Loss: 2.598273, Validation Loss: 2.543944\n",
      "The strento the colstred as as in that ind tath ofrit the to colione sencitis of the on tor a a the ard\n",
      "\n",
      "Epoch: 0, Batch: 81, Train Loss: 2.589993\n",
      "Epoch: 0, Batch: 82, Train Loss: 2.575349\n",
      "Epoch: 0, Batch: 83, Train Loss: 2.563919\n",
      "Epoch: 0, Batch: 84, Train Loss: 2.564582\n",
      "Epoch: 0, Batch: 85, Train Loss: 2.556112\n",
      "Epoch: 0, Batch: 86, Train Loss: 2.560258\n",
      "Epoch: 0, Batch: 87, Train Loss: 2.541670\n",
      "Epoch: 0, Batch: 88, Train Loss: 2.502534\n",
      "Epoch: 0, Batch: 89, Train Loss: 2.516909\n",
      "Epoch: 0, Batch: 90, Train Loss: 2.512046\n",
      "Epoch: 0, Batch: 90, Train Loss: 2.512046, Validation Loss: 2.463567\n",
      "The this the iftor al cand ciled of a the astor a the ancion and of to pesite a starto the of tes to th\n",
      "\n",
      "Epoch: 0, Batch: 91, Train Loss: 2.507840\n",
      "Epoch: 0, Batch: 92, Train Loss: 2.488373\n",
      "Epoch: 0, Batch: 93, Train Loss: 2.487273\n",
      "Epoch: 0, Batch: 94, Train Loss: 2.467896\n",
      "Epoch: 0, Batch: 95, Train Loss: 2.475980\n",
      "Epoch: 0, Batch: 96, Train Loss: 2.444363\n",
      "Epoch: 0, Batch: 97, Train Loss: 2.436338\n",
      "Epoch: 0, Batch: 98, Train Loss: 2.425845\n",
      "Epoch: 0, Batch: 99, Train Loss: 2.425171\n",
      "Epoch: 0, Batch: 100, Train Loss: 2.412523\n",
      "Epoch: 0, Batch: 100, Train Loss: 2.412523, Validation Loss: 2.379524\n",
      "The and a porist thit ond congle of suntal sid sore al the and the cest of alsher of alligs, [[arenting\n",
      "\n",
      "Epoch: 0, Batch: 101, Train Loss: 2.413638\n",
      "Epoch: 0, Batch: 102, Train Loss: 2.417065\n",
      "Epoch: 0, Batch: 103, Train Loss: 2.407859\n",
      "Epoch: 0, Batch: 104, Train Loss: 2.399647\n",
      "Epoch: 0, Batch: 105, Train Loss: 2.404905\n",
      "Epoch: 0, Batch: 106, Train Loss: 2.380390\n",
      "Epoch: 0, Batch: 107, Train Loss: 2.379666\n",
      "Epoch: 0, Batch: 108, Train Loss: 2.373846\n",
      "Epoch: 0, Batch: 109, Train Loss: 2.371035\n",
      "Epoch: 0, Batch: 110, Train Loss: 2.369978\n",
      "Epoch: 0, Batch: 110, Train Loss: 2.369978, Validation Loss: 2.311583\n",
      "The as of the corntitian a the tent as of the oneler of the condutines is as the the the and the cerish\n",
      "\n",
      "Epoch: 0, Batch: 111, Train Loss: 2.368899\n",
      "Epoch: 0, Batch: 112, Train Loss: 2.353430\n",
      "Epoch: 0, Batch: 113, Train Loss: 2.380927\n",
      "Epoch: 0, Batch: 114, Train Loss: 2.364370\n",
      "Epoch: 0, Batch: 115, Train Loss: 2.352457\n",
      "Epoch: 0, Batch: 116, Train Loss: 2.353499\n",
      "Epoch: 0, Batch: 117, Train Loss: 2.326378\n",
      "Epoch: 0, Batch: 118, Train Loss: 2.319822\n",
      "Epoch: 0, Batch: 119, Train Loss: 2.328308\n",
      "Epoch: 0, Batch: 120, Train Loss: 2.288625\n",
      "Epoch: 0, Batch: 120, Train Loss: 2.288625, Validation Loss: 2.259825\n",
      "Thent of tho the [[Concent in alesse as a serter tered thas imastented to the as and to cares of [[Conc\n",
      "\n",
      "Epoch: 0, Batch: 121, Train Loss: 2.276637\n",
      "Epoch: 0, Batch: 122, Train Loss: 2.271728\n",
      "Epoch: 0, Batch: 123, Train Loss: 2.289022\n",
      "Epoch: 0, Batch: 124, Train Loss: 2.289622\n",
      "Epoch: 0, Batch: 125, Train Loss: 2.266596\n",
      "Epoch: 0, Batch: 126, Train Loss: 2.271818\n",
      "Epoch: 0, Batch: 127, Train Loss: 2.269656\n",
      "Epoch: 0, Batch: 128, Train Loss: 2.260406\n",
      "Epoch: 0, Batch: 129, Train Loss: 2.252348\n",
      "Epoch: 0, Batch: 130, Train Loss: 2.253985\n",
      "Epoch: 0, Batch: 130, Train Loss: 2.253985, Validation Loss: 2.205917\n",
      "The on the [[Sean]], [[Ateriona]], toen aral certias of to the cempenis of the thear as tare and. A [[S\n",
      "\n",
      "Epoch: 0, Batch: 131, Train Loss: 2.269786\n",
      "Epoch: 0, Batch: 132, Train Loss: 2.252212\n",
      "Epoch: 0, Batch: 133, Train Loss: 2.239567\n",
      "Epoch: 0, Batch: 134, Train Loss: 2.233119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 135, Train Loss: 2.225668\n",
      "Epoch: 0, Batch: 136, Train Loss: 2.236437\n",
      "Epoch: 0, Batch: 137, Train Loss: 2.249898\n",
      "Epoch: 0, Batch: 138, Train Loss: 2.237553\n",
      "Epoch: 0, Batch: 139, Train Loss: 2.226410\n",
      "Epoch: 0, Batch: 140, Train Loss: 2.199039\n",
      "Epoch: 0, Batch: 140, Train Loss: 2.199039, Validation Loss: 2.162803\n",
      "The to to coment in the [[Colestamaty]], in [[Charniss]] if to the chite of the stont crondaring and th\n",
      "\n",
      "Epoch: 0, Batch: 141, Train Loss: 2.192783\n",
      "Epoch: 0, Batch: 142, Train Loss: 2.190289\n",
      "Epoch: 0, Batch: 143, Train Loss: 2.189692\n",
      "Epoch: 0, Batch: 144, Train Loss: 2.185420\n",
      "Epoch: 0, Batch: 145, Train Loss: 2.188447\n",
      "Epoch: 0, Batch: 146, Train Loss: 2.175414\n",
      "Epoch: 0, Batch: 147, Train Loss: 2.180696\n",
      "Epoch: 0, Batch: 148, Train Loss: 2.159325\n",
      "Epoch: 0, Batch: 149, Train Loss: 2.179427\n",
      "Epoch: 0, Batch: 150, Train Loss: 2.141873\n",
      "Epoch: 0, Batch: 150, Train Loss: 2.141873, Validation Loss: 2.120127\n",
      "The stope one of [[Corso angie alcalita Andine]], [[Cared anstoustrom in Angine of the Chintal the Agri\n",
      "\n",
      "Epoch: 0, Batch: 151, Train Loss: 2.165571\n",
      "Epoch: 0, Batch: 152, Train Loss: 2.172213\n",
      "Epoch: 0, Batch: 153, Train Loss: 2.171317\n",
      "Epoch: 0, Batch: 154, Train Loss: 2.143974\n",
      "Epoch: 0, Batch: 155, Train Loss: 2.158409\n",
      "Epoch: 0, Batch: 156, Train Loss: 2.133641\n",
      "Epoch: 0, Batch: 157, Train Loss: 2.144275\n",
      "Epoch: 0, Batch: 158, Train Loss: 2.143089\n",
      "Epoch: 0, Batch: 159, Train Loss: 2.146683\n",
      "Epoch: 0, Batch: 160, Train Loss: 2.126441\n",
      "Epoch: 0, Batch: 160, Train Loss: 2.126441, Validation Loss: 2.090966\n",
      "The an ale the the are a dual ware a port tale wis a thoe of stard indor the [[Aressing a surare]], the\n",
      "\n",
      "Epoch: 0, Batch: 161, Train Loss: 2.147608\n",
      "Epoch: 0, Batch: 162, Train Loss: 2.145529\n",
      "Epoch: 0, Batch: 163, Train Loss: 2.119622\n",
      "Epoch: 0, Batch: 164, Train Loss: 2.139821\n",
      "Epoch: 0, Batch: 165, Train Loss: 2.103877\n",
      "Epoch: 0, Batch: 166, Train Loss: 2.101801\n",
      "Epoch: 0, Batch: 167, Train Loss: 2.104901\n",
      "Epoch: 0, Batch: 168, Train Loss: 2.125072\n",
      "Epoch: 0, Batch: 169, Train Loss: 2.110566\n",
      "Epoch: 0, Batch: 170, Train Loss: 2.110549\n",
      "Epoch: 0, Batch: 170, Train Loss: 2.110549, Validation Loss: 2.053853\n",
      "The simes.  In the standed in the [[Borne Barch and Amara]]\n",
      "*[[Cherica]] in hend on teanter the [[Sinti\n",
      "\n",
      "Epoch: 0, Batch: 171, Train Loss: 2.101010\n",
      "Epoch: 0, Batch: 172, Train Loss: 2.127884\n",
      "Epoch: 0, Batch: 173, Train Loss: 2.101089\n",
      "Epoch: 0, Batch: 174, Train Loss: 2.117906\n",
      "Epoch: 0, Batch: 175, Train Loss: 2.096071\n",
      "Epoch: 0, Batch: 176, Train Loss: 2.098891\n",
      "Epoch: 0, Batch: 177, Train Loss: 2.099982\n",
      "Epoch: 0, Batch: 178, Train Loss: 2.081482\n",
      "Epoch: 0, Batch: 179, Train Loss: 2.092959\n",
      "Epoch: 0, Batch: 180, Train Loss: 2.086271\n",
      "Epoch: 0, Batch: 180, Train Loss: 2.086271, Validation Loss: 2.024307\n",
      "The [[Siderina Mears]] and a concontires. The [[Centreatic Selent]], the alsue a colpented and [[Amelis\n",
      "\n",
      "Epoch: 0, Batch: 181, Train Loss: 2.080930\n",
      "Epoch: 0, Batch: 182, Train Loss: 2.096185\n",
      "Epoch: 0, Batch: 183, Train Loss: 2.076533\n",
      "Epoch: 0, Batch: 184, Train Loss: 2.066878\n",
      "Epoch: 0, Batch: 185, Train Loss: 2.058956\n",
      "Epoch: 0, Batch: 186, Train Loss: 2.060647\n",
      "Epoch: 0, Batch: 187, Train Loss: 2.065254\n",
      "Epoch: 0, Batch: 188, Train Loss: 2.043221\n",
      "Epoch: 0, Batch: 189, Train Loss: 2.053133\n",
      "Epoch: 0, Batch: 190, Train Loss: 2.060409\n",
      "Epoch: 0, Batch: 190, Train Loss: 2.060409, Validation Loss: 1.995932\n",
      "The [[Malena]] of [[Areco and Sectrice]]\n",
      "*[http://www.worgiciallas/orgesistrimalischard.om/chute.cances\n",
      "\n",
      "Epoch: 0, Batch: 191, Train Loss: 2.048702\n",
      "Epoch: 0, Batch: 192, Train Loss: 2.045295\n",
      "Epoch: 0, Batch: 193, Train Loss: 2.026946\n",
      "Epoch: 0, Batch: 194, Train Loss: 2.016454\n",
      "Epoch: 0, Batch: 195, Train Loss: 2.018417\n",
      "Epoch: 0, Batch: 196, Train Loss: 2.019171\n",
      "Epoch: 0, Batch: 197, Train Loss: 2.033351\n",
      "Epoch: 0, Batch: 198, Train Loss: 2.032853\n",
      "Epoch: 0, Batch: 199, Train Loss: 2.025138\n",
      "Epoch: 0, Batch: 200, Train Loss: 2.014063\n",
      "Epoch: 0, Batch: 200, Train Loss: 2.014063, Validation Loss: 1.967030\n",
      "Thers or this sigle the pacing and the centers allarsed tert the secenty of the [[1806]], [[1900]])., a\n",
      "\n",
      "Epoch: 0, Batch: 201, Train Loss: 2.024044\n",
      "Epoch: 0, Batch: 202, Train Loss: 1.997821\n",
      "Epoch: 0, Batch: 203, Train Loss: 2.000961\n",
      "Epoch: 0, Batch: 204, Train Loss: 2.014365\n",
      "Epoch: 0, Batch: 205, Train Loss: 2.003361\n",
      "Epoch: 0, Batch: 206, Train Loss: 2.013370\n",
      "Epoch: 0, Batch: 207, Train Loss: 1.992801\n",
      "Epoch: 0, Batch: 208, Train Loss: 2.001316\n",
      "Epoch: 0, Batch: 209, Train Loss: 2.008152\n",
      "Epoch: 0, Batch: 210, Train Loss: 1.996360\n",
      "Epoch: 0, Batch: 210, Train Loss: 1.996360, Validation Loss: 1.941651\n",
      "The stelted and the [[Ancand Bastalles]] in [[Collarial anconistities]] is there the sicled of the sing\n",
      "\n",
      "Epoch: 0, Batch: 211, Train Loss: 1.987669\n",
      "Epoch: 0, Batch: 212, Train Loss: 1.979058\n",
      "Epoch: 0, Batch: 213, Train Loss: 1.969076\n",
      "Epoch: 0, Batch: 214, Train Loss: 1.976139\n",
      "Epoch: 0, Batch: 215, Train Loss: 1.980480\n",
      "Epoch: 0, Batch: 216, Train Loss: 1.983618\n",
      "Epoch: 0, Batch: 217, Train Loss: 1.967036\n",
      "Epoch: 0, Batch: 218, Train Loss: 1.965523\n",
      "Epoch: 0, Batch: 219, Train Loss: 1.978203\n",
      "Epoch: 0, Batch: 220, Train Loss: 1.977960\n",
      "Epoch: 0, Batch: 220, Train Loss: 1.977960, Validation Loss: 1.924043\n",
      "The comment and that an ofter and that that that was an artive in the alter that the [[Pithorames|Cista\n",
      "\n",
      "Epoch: 0, Batch: 221, Train Loss: 1.974851\n",
      "Epoch: 0, Batch: 222, Train Loss: 1.969532\n",
      "Epoch: 0, Batch: 223, Train Loss: 1.982771\n",
      "Epoch: 0, Batch: 224, Train Loss: 1.978704\n",
      "Epoch: 0, Batch: 225, Train Loss: 1.961145\n",
      "Epoch: 0, Batch: 226, Train Loss: 1.965644\n",
      "Epoch: 0, Batch: 227, Train Loss: 1.960233\n",
      "Epoch: 0, Batch: 228, Train Loss: 1.952781\n",
      "Epoch: 0, Batch: 229, Train Loss: 1.944250\n",
      "Epoch: 0, Batch: 230, Train Loss: 1.936347\n",
      "Epoch: 0, Batch: 230, Train Loss: 1.936347, Validation Loss: 1.898515\n",
      "The [httegery:Appic of Call of Commential cropic tings of trancester and Streated to the struct of [[19\n",
      "\n",
      "Epoch: 0, Batch: 231, Train Loss: 1.946846\n",
      "Epoch: 0, Batch: 232, Train Loss: 1.934417\n",
      "Epoch: 0, Batch: 233, Train Loss: 1.935582\n",
      "Epoch: 0, Batch: 234, Train Loss: 1.935635\n",
      "Epoch: 0, Batch: 235, Train Loss: 1.931076\n",
      "Epoch: 0, Batch: 236, Train Loss: 1.938417\n",
      "Epoch: 0, Batch: 237, Train Loss: 1.912923\n",
      "Epoch: 0, Batch: 238, Train Loss: 1.929327\n",
      "Epoch: 0, Batch: 239, Train Loss: 1.941789\n",
      "Epoch: 0, Batch: 240, Train Loss: 1.913517\n",
      "Epoch: 0, Batch: 240, Train Loss: 1.913517, Validation Loss: 1.876130\n",
      "The ascent of the procoped is intermothated of contrest in the [[Mall Sere of Solant]], and a soute the\n",
      "\n",
      "Epoch: 0, Batch: 241, Train Loss: 1.938337\n",
      "Epoch: 0, Batch: 242, Train Loss: 1.916008\n",
      "Epoch: 0, Batch: 243, Train Loss: 1.913942\n",
      "Epoch: 0, Batch: 244, Train Loss: 1.914239\n",
      "Epoch: 0, Batch: 245, Train Loss: 1.915121\n",
      "Epoch: 0, Batch: 246, Train Loss: 1.916469\n",
      "Epoch: 0, Batch: 247, Train Loss: 1.929743\n",
      "Epoch: 0, Batch: 248, Train Loss: 1.933106\n",
      "Epoch: 0, Batch: 249, Train Loss: 1.895605\n",
      "Epoch: 0, Batch: 250, Train Loss: 1.900828\n",
      "Epoch: 0, Batch: 250, Train Loss: 1.900828, Validation Loss: 1.856693\n",
      "The commetters arimal inclodined by [[Sonaters Servent]]. The fonter from thit with carreath corred to \n",
      "\n",
      "Epoch: 0, Batch: 251, Train Loss: 1.894333\n",
      "Epoch: 0, Batch: 252, Train Loss: 1.889264\n",
      "Epoch: 0, Batch: 253, Train Loss: 1.888326\n",
      "Epoch: 0, Batch: 254, Train Loss: 1.887864\n",
      "Epoch: 0, Batch: 255, Train Loss: 1.901256\n",
      "Epoch: 0, Batch: 256, Train Loss: 1.878812\n",
      "Epoch: 0, Batch: 257, Train Loss: 1.866941\n",
      "Epoch: 0, Batch: 258, Train Loss: 1.894632\n",
      "Epoch: 0, Batch: 259, Train Loss: 1.907104\n",
      "Epoch: 0, Batch: 260, Train Loss: 1.900990\n",
      "Epoch: 0, Batch: 260, Train Loss: 1.900990, Validation Loss: 1.839083\n",
      "The comment asters trom a such if [[Sillia Pirination of Colorator|Bradition Churser of Better Michrist\n",
      "\n",
      "Epoch: 0, Batch: 261, Train Loss: 1.897816\n",
      "Epoch: 0, Batch: 262, Train Loss: 1.897916\n",
      "Epoch: 0, Batch: 263, Train Loss: 1.876816\n",
      "Epoch: 0, Batch: 264, Train Loss: 1.875076\n",
      "Epoch: 0, Batch: 265, Train Loss: 1.886296\n",
      "Epoch: 0, Batch: 266, Train Loss: 1.863502\n",
      "Epoch: 0, Batch: 267, Train Loss: 1.867707\n",
      "Epoch: 0, Batch: 268, Train Loss: 1.864137\n",
      "Epoch: 0, Batch: 269, Train Loss: 1.871731\n",
      "Epoch: 0, Batch: 270, Train Loss: 1.892589\n",
      "Epoch: 0, Batch: 270, Train Loss: 1.892589, Validation Loss: 1.822022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The [[Mirdory alding in Counse]] in [[Catian and are and tertications overing these and the also the su\n",
      "\n",
      "Epoch: 0, Batch: 271, Train Loss: 1.871573\n",
      "Epoch: 0, Batch: 272, Train Loss: 1.863790\n",
      "Epoch: 0, Batch: 273, Train Loss: 1.864587\n",
      "Epoch: 0, Batch: 274, Train Loss: 1.869111\n",
      "Epoch: 0, Batch: 275, Train Loss: 1.836743\n",
      "Epoch: 0, Batch: 276, Train Loss: 1.849661\n",
      "Epoch: 0, Batch: 277, Train Loss: 1.862458\n",
      "Epoch: 0, Batch: 278, Train Loss: 1.843054\n",
      "Epoch: 0, Batch: 279, Train Loss: 1.857813\n",
      "Epoch: 0, Batch: 280, Train Loss: 1.826992\n",
      "Epoch: 0, Batch: 280, Train Loss: 1.826992, Validation Loss: 1.802058\n",
      "The are ontinessive some and as the [[Brentaning]], in his also computer interved the framery the [[Ame\n",
      "\n",
      "Epoch: 0, Batch: 281, Train Loss: 1.855003\n",
      "Epoch: 0, Batch: 282, Train Loss: 1.863006\n",
      "Epoch: 0, Batch: 283, Train Loss: 1.856808\n",
      "Epoch: 0, Batch: 284, Train Loss: 1.851184\n",
      "Epoch: 0, Batch: 285, Train Loss: 1.852801\n",
      "Epoch: 0, Batch: 286, Train Loss: 1.841465\n",
      "Epoch: 0, Batch: 287, Train Loss: 1.843788\n",
      "Epoch: 0, Batch: 288, Train Loss: 1.835496\n",
      "Epoch: 0, Batch: 289, Train Loss: 1.849250\n",
      "Epoch: 0, Batch: 290, Train Loss: 1.859191\n",
      "Epoch: 0, Batch: 290, Train Loss: 1.859191, Validation Loss: 1.788159\n",
      "The tore of the transurusting these is oppreeses is the carry and can of the [[Aurio]]''\n",
      "\n",
      "[[1988]]     \n",
      "\n",
      "Epoch: 0, Batch: 291, Train Loss: 1.850742\n",
      "Epoch: 0, Batch: 292, Train Loss: 1.840454\n",
      "Epoch: 0, Batch: 293, Train Loss: 1.839782\n",
      "Epoch: 0, Batch: 294, Train Loss: 1.834167\n",
      "Epoch: 0, Batch: 295, Train Loss: 1.827168\n",
      "Epoch: 0, Batch: 296, Train Loss: 1.813807\n",
      "Epoch: 0, Batch: 297, Train Loss: 1.830495\n",
      "Epoch: 0, Batch: 298, Train Loss: 1.820228\n",
      "Epoch: 0, Batch: 299, Train Loss: 1.821141\n",
      "Epoch: 0, Batch: 300, Train Loss: 1.815055\n",
      "Epoch: 0, Batch: 300, Train Loss: 1.815055, Validation Loss: 1.771845\n",
      "The state as the such of the acternition in the [[Machine]] ances a such of the tattens art standed tha\n",
      "\n",
      "Epoch: 0, Batch: 301, Train Loss: 1.821056\n",
      "Epoch: 0, Batch: 302, Train Loss: 1.811407\n",
      "Epoch: 0, Batch: 303, Train Loss: 1.823694\n",
      "Epoch: 0, Batch: 304, Train Loss: 1.811962\n",
      "Epoch: 0, Batch: 305, Train Loss: 1.811508\n",
      "Epoch: 0, Batch: 306, Train Loss: 1.813487\n",
      "Epoch: 0, Batch: 307, Train Loss: 1.810999\n",
      "Epoch: 0, Batch: 308, Train Loss: 1.835273\n",
      "Epoch: 0, Batch: 309, Train Loss: 1.833163\n",
      "Epoch: 0, Batch: 310, Train Loss: 1.822388\n",
      "Epoch: 0, Batch: 310, Train Loss: 1.822388, Validation Loss: 1.756276\n",
      "The a portally, the caning style and article. It wis incollor to thesis the soluce to serven in conferi\n",
      "\n",
      "Epoch: 0, Batch: 311, Train Loss: 1.815318\n",
      "Epoch: 0, Batch: 312, Train Loss: 1.804898\n",
      "Epoch: 0, Batch: 313, Train Loss: 1.837809\n",
      "Epoch: 0, Batch: 314, Train Loss: 1.808410\n",
      "Epoch: 0, Batch: 315, Train Loss: 1.831124\n",
      "Epoch: 0, Batch: 316, Train Loss: 1.814198\n",
      "Epoch: 0, Batch: 317, Train Loss: 1.816311\n",
      "Epoch: 0, Batch: 318, Train Loss: 1.840042\n",
      "Epoch: 0, Batch: 319, Train Loss: 1.811627\n",
      "Epoch: 0, Batch: 320, Train Loss: 1.786507\n",
      "Epoch: 0, Batch: 320, Train Loss: 1.786507, Validation Loss: 1.743889\n",
      "The [[Calection (and country]] (1586), [[American Aries]] (1999 in the Cate Seles as ''[[Mall Amilan Se\n",
      "\n",
      "Epoch: 0, Batch: 321, Train Loss: 1.793766\n",
      "Epoch: 0, Batch: 322, Train Loss: 1.776866\n",
      "Epoch: 0, Batch: 323, Train Loss: 1.812507\n",
      "Epoch: 0, Batch: 324, Train Loss: 1.814694\n",
      "Epoch: 0, Batch: 325, Train Loss: 1.802520\n",
      "Epoch: 0, Batch: 326, Train Loss: 1.806621\n",
      "Epoch: 0, Batch: 327, Train Loss: 1.799072\n",
      "Epoch: 0, Batch: 328, Train Loss: 1.814986\n",
      "Epoch: 0, Batch: 329, Train Loss: 1.778898\n",
      "Epoch: 0, Batch: 330, Train Loss: 1.780370\n",
      "Epoch: 0, Batch: 330, Train Loss: 1.780370, Validation Loss: 1.729119\n",
      "The [[States Crines]] wishore. It in [[101]] [[1795]]. At the [[Changainer Austrie]] and [[1975]] and [\n",
      "\n",
      "Epoch: 0, Batch: 331, Train Loss: 1.793905\n",
      "Epoch: 0, Batch: 332, Train Loss: 1.777716\n",
      "Epoch: 0, Batch: 333, Train Loss: 1.774511\n",
      "Epoch: 0, Batch: 334, Train Loss: 1.783496\n",
      "Epoch: 0, Batch: 335, Train Loss: 1.765959\n",
      "Epoch: 0, Batch: 336, Train Loss: 1.759681\n",
      "Epoch: 0, Batch: 337, Train Loss: 1.753681\n",
      "Epoch: 0, Batch: 338, Train Loss: 1.756011\n",
      "Epoch: 0, Batch: 339, Train Loss: 1.769574\n",
      "Epoch: 0, Batch: 340, Train Loss: 1.776122\n",
      "Epoch: 0, Batch: 340, Train Loss: 1.776122, Validation Loss: 1.721646\n",
      "The suppirarial includent of contritutured by the [[Arina Chrolents Christian Councer]] worth instrecte\n",
      "\n",
      "Epoch: 0, Batch: 341, Train Loss: 1.771781\n",
      "Epoch: 0, Batch: 342, Train Loss: 1.786704\n",
      "Epoch: 0, Batch: 343, Train Loss: 1.785193\n",
      "Epoch: 0, Batch: 344, Train Loss: 1.790369\n",
      "Epoch: 0, Batch: 345, Train Loss: 1.775663\n",
      "Epoch: 0, Batch: 346, Train Loss: 1.755053\n",
      "Epoch: 0, Batch: 347, Train Loss: 1.758639\n",
      "Epoch: 0, Batch: 348, Train Loss: 1.770531\n",
      "Epoch: 0, Batch: 349, Train Loss: 1.773394\n",
      "Epoch: 0, Batch: 350, Train Loss: 1.777914\n",
      "Epoch: 0, Batch: 350, Train Loss: 1.777914, Validation Loss: 1.708669\n",
      "The chese of can that the stitutian artitures, and as a [[plate of compenity]].\n",
      "\n",
      "In the [[Carren Angia]\n",
      "\n",
      "Epoch: 0, Batch: 351, Train Loss: 1.767696\n",
      "Epoch: 0, Batch: 352, Train Loss: 1.761043\n",
      "Epoch: 0, Batch: 353, Train Loss: 1.764929\n",
      "Epoch: 0, Batch: 354, Train Loss: 1.775356\n",
      "Epoch: 0, Batch: 355, Train Loss: 1.751489\n",
      "Epoch: 0, Batch: 356, Train Loss: 1.730198\n",
      "Epoch: 0, Batch: 357, Train Loss: 1.734839\n",
      "Epoch: 0, Batch: 358, Train Loss: 1.742821\n",
      "Epoch: 0, Batch: 359, Train Loss: 1.728414\n",
      "Epoch: 0, Batch: 360, Train Loss: 1.718415\n",
      "Epoch: 0, Batch: 360, Train Loss: 1.718415, Validation Loss: 1.697477\n",
      "The statual of circe only buinted by the [[Ancher Concrition for moron]], and [[Chatelent center and an\n",
      "\n",
      "Epoch: 0, Batch: 361, Train Loss: 1.747063\n",
      "Epoch: 0, Batch: 362, Train Loss: 1.746593\n",
      "Epoch: 0, Batch: 363, Train Loss: 1.728941\n",
      "Epoch: 0, Batch: 364, Train Loss: 1.742584\n",
      "Epoch: 0, Batch: 365, Train Loss: 1.731473\n",
      "Epoch: 0, Batch: 366, Train Loss: 1.728872\n",
      "Epoch: 0, Batch: 367, Train Loss: 1.718648\n",
      "Epoch: 0, Batch: 368, Train Loss: 1.728853\n",
      "Epoch: 0, Batch: 369, Train Loss: 1.748652\n",
      "Epoch: 0, Batch: 370, Train Loss: 1.739776\n",
      "Epoch: 0, Batch: 370, Train Loss: 1.739776, Validation Loss: 1.688258\n",
      "The [[compler]], and the caning tory that homal somists and are one. The first of [[simber direntic]]s \n",
      "\n",
      "Epoch: 0, Batch: 371, Train Loss: 1.769347\n",
      "Epoch: 0, Batch: 372, Train Loss: 1.759124\n",
      "Epoch: 0, Batch: 373, Train Loss: 1.735415\n",
      "Epoch: 0, Batch: 374, Train Loss: 1.732712\n",
      "Epoch: 0, Batch: 375, Train Loss: 1.737734\n",
      "Epoch: 0, Batch: 376, Train Loss: 1.737739\n",
      "Epoch: 0, Batch: 377, Train Loss: 1.727349\n",
      "Epoch: 0, Batch: 378, Train Loss: 1.729749\n",
      "Epoch: 0, Batch: 379, Train Loss: 1.715211\n",
      "Epoch: 0, Batch: 380, Train Loss: 1.695728\n",
      "Epoch: 0, Batch: 380, Train Loss: 1.695728, Validation Loss: 1.675053\n",
      "The the minital computed a critical corrono cortation, is others of the storiage of the allitator chirc\n",
      "\n",
      "Epoch: 0, Batch: 381, Train Loss: 1.723231\n",
      "Epoch: 0, Batch: 382, Train Loss: 1.721869\n",
      "Epoch: 0, Batch: 383, Train Loss: 1.703992\n",
      "Epoch: 0, Batch: 384, Train Loss: 1.699365\n",
      "Epoch: 0, Batch: 385, Train Loss: 1.712724\n",
      "Epoch: 0, Batch: 386, Train Loss: 1.729020\n",
      "Epoch: 0, Batch: 387, Train Loss: 1.735348\n",
      "Epoch: 0, Batch: 388, Train Loss: 1.712657\n",
      "Epoch: 0, Batch: 389, Train Loss: 1.729056\n",
      "Epoch: 0, Batch: 390, Train Loss: 1.712542\n",
      "Epoch: 0, Batch: 390, Train Loss: 1.712542, Validation Loss: 1.668410\n",
      "The the [[Colegeria]] and [[Sid Amino]] if all and seess in a sense and the feater in [[Carister Sonshi\n",
      "\n",
      "Epoch: 0, Batch: 391, Train Loss: 1.691090\n",
      "Epoch: 0, Batch: 392, Train Loss: 1.700628\n",
      "Epoch: 0, Batch: 393, Train Loss: 1.722938\n",
      "Epoch: 0, Batch: 394, Train Loss: 1.730122\n",
      "Epoch: 0, Batch: 395, Train Loss: 1.718161\n",
      "Epoch: 0, Batch: 396, Train Loss: 1.715988\n",
      "Epoch: 0, Batch: 397, Train Loss: 1.682183\n",
      "Epoch: 0, Batch: 398, Train Loss: 1.690852\n",
      "Epoch: 0, Batch: 399, Train Loss: 1.690375\n",
      "Epoch: 0, Batch: 400, Train Loss: 1.707611\n",
      "Epoch: 0, Batch: 400, Train Loss: 1.707611, Validation Loss: 1.657250\n",
      "The though houdhing who to be supcricial as a song on the socialer. Argostral to the presence is servit\n",
      "\n",
      "Epoch: 0, Batch: 401, Train Loss: 1.715156\n",
      "Epoch: 0, Batch: 402, Train Loss: 1.708116\n",
      "Epoch: 0, Batch: 403, Train Loss: 1.710664\n",
      "Epoch: 0, Batch: 404, Train Loss: 1.716097\n",
      "Epoch: 0, Batch: 405, Train Loss: 1.697061\n",
      "Epoch: 0, Batch: 406, Train Loss: 1.703642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 407, Train Loss: 1.719267\n",
      "Epoch: 0, Batch: 408, Train Loss: 1.717492\n",
      "Epoch: 0, Batch: 409, Train Loss: 1.715440\n",
      "Epoch: 0, Batch: 410, Train Loss: 1.727224\n",
      "Epoch: 0, Batch: 410, Train Loss: 1.727224, Validation Loss: 1.648085\n",
      "The [[Sovio Cirity and Charanche]], terperite in the [[Ander Strouth]] and [[Morelling]]\n",
      "\n",
      "= See [[Comma\n",
      "\n",
      "Epoch: 0, Batch: 411, Train Loss: 1.740094\n",
      "Epoch: 0, Batch: 412, Train Loss: 1.713094\n",
      "Epoch: 0, Batch: 413, Train Loss: 1.716528\n",
      "Epoch: 0, Batch: 414, Train Loss: 1.725482\n",
      "Epoch: 0, Batch: 415, Train Loss: 1.701125\n",
      "Epoch: 0, Batch: 416, Train Loss: 1.708837\n",
      "Epoch: 0, Batch: 417, Train Loss: 1.707830\n",
      "Epoch: 0, Batch: 418, Train Loss: 1.724313\n",
      "Epoch: 0, Batch: 419, Train Loss: 1.712538\n",
      "Epoch: 0, Batch: 420, Train Loss: 1.690246\n",
      "Epoch: 0, Batch: 420, Train Loss: 1.690246, Validation Loss: 1.638817\n",
      "The [[Charlen]] and the [[Servia]], [[1990]], [[Colland Centularia Corter|Streen Pritaris]], [[Barline]\n",
      "\n",
      "Epoch: 0, Batch: 421, Train Loss: 1.694981\n",
      "Epoch: 0, Batch: 422, Train Loss: 1.702968\n",
      "Epoch: 0, Batch: 423, Train Loss: 1.681992\n",
      "Epoch: 0, Batch: 424, Train Loss: 1.687754\n",
      "Epoch: 0, Batch: 425, Train Loss: 1.674110\n",
      "Epoch: 0, Batch: 426, Train Loss: 1.692154\n",
      "Epoch: 0, Batch: 427, Train Loss: 1.700399\n",
      "Epoch: 0, Batch: 428, Train Loss: 1.716330\n",
      "Epoch: 0, Batch: 429, Train Loss: 1.700548\n",
      "Epoch: 0, Batch: 430, Train Loss: 1.703456\n",
      "Epoch: 0, Batch: 430, Train Loss: 1.703456, Validation Loss: 1.629827\n",
      "The [[Book Canado]] [[Chilarante candransers]].  The play of the president within the third the meaders\n",
      "\n",
      "Epoch: 0, Batch: 431, Train Loss: 1.699520\n",
      "Epoch: 0, Batch: 432, Train Loss: 1.709846\n",
      "Epoch: 0, Batch: 433, Train Loss: 1.693868\n",
      "Epoch: 0, Batch: 434, Train Loss: 1.661787\n",
      "Epoch: 0, Batch: 435, Train Loss: 1.653696\n",
      "Epoch: 0, Batch: 436, Train Loss: 1.669648\n",
      "Epoch: 0, Batch: 437, Train Loss: 1.681591\n",
      "Epoch: 0, Batch: 438, Train Loss: 1.665901\n",
      "Epoch: 0, Batch: 439, Train Loss: 1.665357\n",
      "Epoch: 0, Batch: 440, Train Loss: 1.675716\n",
      "Epoch: 0, Batch: 440, Train Loss: 1.675716, Validation Loss: 1.625741\n",
      "The surding.\n",
      "\n",
      "The articreat to the continues as a [[communistion and tript case of the fields which the\n",
      "\n",
      "Epoch: 0, Batch: 441, Train Loss: 1.694197\n",
      "Epoch: 0, Batch: 442, Train Loss: 1.654851\n",
      "Epoch: 0, Batch: 443, Train Loss: 1.699501\n",
      "Epoch: 0, Batch: 444, Train Loss: 1.698776\n",
      "Epoch: 0, Batch: 445, Train Loss: 1.700053\n",
      "Epoch: 0, Batch: 446, Train Loss: 1.704276\n",
      "Epoch: 0, Batch: 447, Train Loss: 1.672608\n",
      "Epoch: 0, Batch: 448, Train Loss: 1.663436\n",
      "Epoch: 0, Batch: 449, Train Loss: 1.672141\n",
      "Epoch: 0, Batch: 450, Train Loss: 1.660960\n",
      "Epoch: 0, Batch: 450, Train Loss: 1.660960, Validation Loss: 1.615280\n",
      "The [[Creacures]], was the [[central caller]], [[Arabia]], which had allied on the sinner, and the coun\n",
      "\n",
      "Epoch: 0, Batch: 451, Train Loss: 1.667491\n",
      "Epoch: 0, Batch: 452, Train Loss: 1.665008\n",
      "Epoch: 0, Batch: 453, Train Loss: 1.669257\n",
      "Epoch: 0, Batch: 454, Train Loss: 1.647991\n",
      "Epoch: 0, Batch: 455, Train Loss: 1.646659\n",
      "Epoch: 0, Batch: 456, Train Loss: 1.673418\n",
      "Epoch: 0, Batch: 457, Train Loss: 1.658162\n",
      "Epoch: 0, Batch: 458, Train Loss: 1.669560\n",
      "Epoch: 0, Batch: 459, Train Loss: 1.666673\n",
      "Epoch: 0, Batch: 460, Train Loss: 1.651654\n",
      "Epoch: 0, Batch: 460, Train Loss: 1.651654, Validation Loss: 1.607501\n",
      "The to divide in the second and characterists at a certain the standard the foreign opens of a prophine\n",
      "\n",
      "Epoch: 0, Batch: 461, Train Loss: 1.654967\n",
      "Epoch: 0, Batch: 462, Train Loss: 1.674194\n",
      "Epoch: 0, Batch: 463, Train Loss: 1.682807\n",
      "Epoch: 0, Batch: 464, Train Loss: 1.650977\n",
      "Epoch: 0, Batch: 465, Train Loss: 1.652634\n",
      "Epoch: 0, Batch: 466, Train Loss: 1.647567\n",
      "Epoch: 0, Batch: 467, Train Loss: 1.662964\n",
      "Epoch: 0, Batch: 468, Train Loss: 1.680272\n",
      "Epoch: 0, Batch: 469, Train Loss: 1.656320\n",
      "Epoch: 0, Batch: 470, Train Loss: 1.670010\n",
      "Epoch: 0, Batch: 470, Train Loss: 1.670010, Validation Loss: 1.600487\n",
      "The country of the confective of command in a secion at lust a colrection.\n",
      "\n",
      "== And ares ==\n",
      "* [[Menitori\n",
      "\n",
      "Epoch: 0, Batch: 471, Train Loss: 1.644240\n",
      "Epoch: 0, Batch: 472, Train Loss: 1.646840\n",
      "Epoch: 0, Batch: 473, Train Loss: 1.634351\n",
      "Epoch: 0, Batch: 474, Train Loss: 1.651384\n",
      "Epoch: 0, Batch: 475, Train Loss: 1.674818\n",
      "Epoch: 0, Batch: 476, Train Loss: 1.671996\n",
      "Epoch: 0, Batch: 477, Train Loss: 1.650453\n",
      "Epoch: 0, Batch: 478, Train Loss: 1.654818\n",
      "Epoch: 0, Batch: 479, Train Loss: 1.663561\n",
      "Epoch: 0, Batch: 480, Train Loss: 1.670672\n",
      "Epoch: 0, Batch: 480, Train Loss: 1.670672, Validation Loss: 1.595292\n",
      "The [[Basin]], and also the [[August]] and [[San Schell Control of the American Cornetic Company]] in t\n",
      "\n",
      "Epoch: 0, Batch: 481, Train Loss: 1.643783\n",
      "Epoch: 0, Batch: 482, Train Loss: 1.654823\n",
      "Epoch: 0, Batch: 483, Train Loss: 1.651115\n",
      "Epoch: 0, Batch: 484, Train Loss: 1.648479\n",
      "Epoch: 0, Batch: 485, Train Loss: 1.661557\n",
      "Epoch: 0, Batch: 486, Train Loss: 1.640327\n",
      "Epoch: 0, Batch: 487, Train Loss: 1.625208\n",
      "Epoch: 0, Batch: 488, Train Loss: 1.643235\n",
      "Epoch: 0, Batch: 489, Train Loss: 1.650082\n",
      "Epoch: 0, Batch: 490, Train Loss: 1.633977\n",
      "Epoch: 0, Batch: 490, Train Loss: 1.633977, Validation Loss: 1.591202\n",
      "The see imposed of the there instative some comparement in solve a part and immident of the [[Allon Soc\n",
      "\n",
      "Epoch: 0, Batch: 491, Train Loss: 1.640977\n",
      "Epoch: 0, Batch: 492, Train Loss: 1.634274\n",
      "Epoch: 0, Batch: 493, Train Loss: 1.663546\n",
      "Epoch: 0, Batch: 494, Train Loss: 1.674193\n",
      "Epoch: 0, Batch: 495, Train Loss: 1.648656\n",
      "Epoch: 0, Batch: 496, Train Loss: 1.647358\n",
      "Epoch: 0, Batch: 497, Train Loss: 1.660633\n",
      "Epoch: 0, Batch: 498, Train Loss: 1.635698\n",
      "Epoch: 0, Batch: 499, Train Loss: 1.632908\n",
      "Epoch: 0, Batch: 500, Train Loss: 1.637305\n",
      "Epoch: 0, Batch: 500, Train Loss: 1.637305, Validation Loss: 1.582855\n",
      "The assing a [[compoter]]. \n",
      "\n",
      "The strackly a most consum of the coull of any passes, insurance is.\n",
      "\n",
      "===S\n",
      "\n",
      "Epoch: 0, Batch: 501, Train Loss: 1.634471\n",
      "Epoch: 0, Batch: 502, Train Loss: 1.640407\n",
      "Epoch: 0, Batch: 503, Train Loss: 1.643306\n",
      "Epoch: 0, Batch: 504, Train Loss: 1.632211\n",
      "Epoch: 0, Batch: 505, Train Loss: 1.635538\n",
      "Epoch: 0, Batch: 506, Train Loss: 1.619439\n",
      "Epoch: 0, Batch: 507, Train Loss: 1.619621\n",
      "Epoch: 0, Batch: 508, Train Loss: 1.609563\n",
      "Epoch: 0, Batch: 509, Train Loss: 1.625291\n",
      "Epoch: 0, Batch: 510, Train Loss: 1.617585\n",
      "Epoch: 0, Batch: 510, Train Loss: 1.617585, Validation Loss: 1.576507\n",
      "The [[Music Astral]].\n",
      "\n",
      "The surnevels, and there are such as [[stack]] and [[paradomination]], and the [\n",
      "\n",
      "Epoch: 0, Batch: 511, Train Loss: 1.631466\n",
      "Epoch: 0, Batch: 512, Train Loss: 1.624450\n",
      "Epoch: 0, Batch: 513, Train Loss: 1.636199\n",
      "Epoch: 0, Batch: 514, Train Loss: 1.618160\n",
      "Epoch: 0, Batch: 515, Train Loss: 1.611846\n",
      "Epoch: 0, Batch: 516, Train Loss: 1.607621\n",
      "Epoch: 0, Batch: 517, Train Loss: 1.623651\n",
      "Epoch: 0, Batch: 518, Train Loss: 1.625759\n",
      "Epoch: 0, Batch: 519, Train Loss: 1.627549\n",
      "Epoch: 0, Batch: 520, Train Loss: 1.630900\n",
      "Epoch: 0, Batch: 520, Train Loss: 1.630900, Validation Loss: 1.571246\n",
      "The [[Alland Short America (dunce of American)|Marous Almensoth]] of [[Stationary]], and [[Armino Seat \n",
      "\n",
      "Epoch: 0, Batch: 521, Train Loss: 1.617199\n",
      "Epoch: 0, Batch: 522, Train Loss: 1.609370\n",
      "Epoch: 0, Batch: 523, Train Loss: 1.612179\n",
      "Epoch: 0, Batch: 524, Train Loss: 1.605440\n",
      "Epoch: 0, Batch: 525, Train Loss: 1.623960\n",
      "Epoch: 0, Batch: 526, Train Loss: 1.619919\n",
      "Epoch: 0, Batch: 527, Train Loss: 1.615019\n",
      "Epoch: 0, Batch: 528, Train Loss: 1.601375\n",
      "Epoch: 0, Batch: 529, Train Loss: 1.599995\n",
      "Epoch: 0, Batch: 530, Train Loss: 1.607382\n",
      "Epoch: 0, Batch: 530, Train Loss: 1.607382, Validation Loss: 1.564511\n",
      "The also commuter and sometric and competitions of the [[Cornor Stronglos]], the [[Ballail]] of the sol\n",
      "\n",
      "Epoch: 0, Batch: 531, Train Loss: 1.599659\n",
      "Epoch: 0, Batch: 532, Train Loss: 1.592780\n",
      "Epoch: 0, Batch: 533, Train Loss: 1.631413\n",
      "Epoch: 0, Batch: 534, Train Loss: 1.631042\n",
      "Epoch: 0, Batch: 535, Train Loss: 1.615062\n",
      "Epoch: 0, Batch: 536, Train Loss: 1.608341\n",
      "Epoch: 0, Batch: 537, Train Loss: 1.596091\n",
      "Epoch: 0, Batch: 538, Train Loss: 1.593896\n",
      "Epoch: 0, Batch: 539, Train Loss: 1.599985\n",
      "Epoch: 0, Batch: 540, Train Loss: 1.596445\n",
      "Epoch: 0, Batch: 540, Train Loss: 1.596445, Validation Loss: 1.558111\n",
      "The any consincred terminated attack the first that they and the film antal to conference, and support \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 541, Train Loss: 1.579258\n",
      "Epoch: 0, Batch: 542, Train Loss: 1.596283\n",
      "Epoch: 0, Batch: 543, Train Loss: 1.595293\n",
      "Epoch: 0, Batch: 544, Train Loss: 1.603595\n",
      "Epoch: 0, Batch: 545, Train Loss: 1.604236\n",
      "Epoch: 0, Batch: 546, Train Loss: 1.592978\n",
      "Epoch: 0, Batch: 547, Train Loss: 1.605393\n",
      "Epoch: 0, Batch: 548, Train Loss: 1.617438\n",
      "Epoch: 0, Batch: 549, Train Loss: 1.621979\n",
      "Epoch: 0, Batch: 550, Train Loss: 1.623667\n",
      "Epoch: 0, Batch: 550, Train Loss: 1.623667, Validation Loss: 1.555794\n",
      "The [[Charle Artic Broth]], a toth all are the certain that ways are the car of this continoons and pre\n",
      "\n",
      "Epoch: 0, Batch: 551, Train Loss: 1.606227\n",
      "Epoch: 0, Batch: 552, Train Loss: 1.604063\n",
      "Epoch: 0, Batch: 553, Train Loss: 1.594911\n",
      "Epoch: 0, Batch: 554, Train Loss: 1.590199\n",
      "Epoch: 0, Batch: 555, Train Loss: 1.609511\n",
      "Epoch: 0, Batch: 556, Train Loss: 1.605970\n",
      "Epoch: 0, Batch: 557, Train Loss: 1.611484\n",
      "Epoch: 0, Batch: 558, Train Loss: 1.592980\n",
      "Epoch: 0, Batch: 559, Train Loss: 1.618293\n",
      "Epoch: 0, Batch: 560, Train Loss: 1.619172\n",
      "Epoch: 0, Batch: 560, Train Loss: 1.619172, Validation Loss: 1.547751\n",
      "The [[Machard And South Category|Phonochuse]], the [[Spechance of Augume of Birgh]], as this and such a\n",
      "\n",
      "Epoch: 0, Batch: 561, Train Loss: 1.605509\n",
      "Epoch: 0, Batch: 562, Train Loss: 1.605258\n",
      "Epoch: 0, Batch: 563, Train Loss: 1.597933\n",
      "Epoch: 0, Batch: 564, Train Loss: 1.607543\n",
      "Epoch: 0, Batch: 565, Train Loss: 1.599973\n",
      "Epoch: 0, Batch: 566, Train Loss: 1.565063\n",
      "Epoch: 0, Batch: 567, Train Loss: 1.565004\n",
      "Epoch: 0, Batch: 568, Train Loss: 1.601040\n",
      "Epoch: 0, Batch: 569, Train Loss: 1.585330\n",
      "Epoch: 0, Batch: 570, Train Loss: 1.595569\n",
      "Epoch: 0, Batch: 570, Train Loss: 1.595569, Validation Loss: 1.544743\n",
      "The anainance of this is that is one of a manuals as a subject of the south sources that all the factiv\n",
      "\n",
      "Epoch: 0, Batch: 571, Train Loss: 1.604121\n",
      "Epoch: 0, Batch: 572, Train Loss: 1.598713\n",
      "Epoch: 0, Batch: 573, Train Loss: 1.582029\n",
      "Epoch: 0, Batch: 574, Train Loss: 1.591231\n",
      "Epoch: 0, Batch: 575, Train Loss: 1.601050\n",
      "Epoch: 0, Batch: 576, Train Loss: 1.577833\n",
      "Epoch: 0, Batch: 577, Train Loss: 1.604318\n",
      "Epoch: 0, Batch: 578, Train Loss: 1.593315\n",
      "Epoch: 0, Batch: 579, Train Loss: 1.600565\n",
      "Epoch: 0, Batch: 580, Train Loss: 1.583701\n",
      "Epoch: 0, Batch: 580, Train Loss: 1.583701, Validation Loss: 1.541418\n",
      "The calles of the past of [[commerical minas]] and three change of the [[Pollystianism]]s in the first \n",
      "\n",
      "Epoch: 0, Batch: 581, Train Loss: 1.596280\n",
      "Epoch: 0, Batch: 582, Train Loss: 1.598136\n",
      "Epoch: 0, Batch: 583, Train Loss: 1.576531\n",
      "Epoch: 0, Batch: 584, Train Loss: 1.567480\n",
      "Epoch: 0, Batch: 585, Train Loss: 1.598349\n",
      "Epoch: 0, Batch: 586, Train Loss: 1.554232\n",
      "Epoch: 0, Batch: 587, Train Loss: 1.572225\n",
      "Epoch: 0, Batch: 588, Train Loss: 1.560400\n",
      "Epoch: 0, Batch: 589, Train Loss: 1.582012\n",
      "Epoch: 0, Batch: 590, Train Loss: 1.585708\n",
      "Epoch: 0, Batch: 590, Train Loss: 1.585708, Validation Loss: 1.536867\n",
      "The cases on colrection.\n",
      "\n",
      "The computer chemical particle, battery on the counties as will be converted \n",
      "\n",
      "Epoch: 0, Batch: 591, Train Loss: 1.596634\n",
      "Epoch: 0, Batch: 592, Train Loss: 1.599491\n",
      "Epoch: 0, Batch: 593, Train Loss: 1.594422\n",
      "Epoch: 0, Batch: 594, Train Loss: 1.592294\n",
      "Epoch: 0, Batch: 595, Train Loss: 1.602775\n",
      "Epoch: 0, Batch: 596, Train Loss: 1.587133\n",
      "Epoch: 0, Batch: 597, Train Loss: 1.584269\n",
      "Epoch: 0, Batch: 598, Train Loss: 1.579802\n",
      "Epoch: 0, Batch: 599, Train Loss: 1.587215\n",
      "Epoch: 0, Batch: 600, Train Loss: 1.581576\n",
      "Epoch: 0, Batch: 600, Train Loss: 1.581576, Validation Loss: 1.533617\n",
      "The actual achumar in [[Carrolic]], and in [[Judialans]], and [[Chinistas Carlia America]].  The powers\n",
      "\n",
      "Epoch: 0, Batch: 601, Train Loss: 1.602254\n",
      "Epoch: 0, Batch: 602, Train Loss: 1.593915\n",
      "Epoch: 0, Batch: 603, Train Loss: 1.608356\n",
      "Epoch: 0, Batch: 604, Train Loss: 1.571496\n",
      "Epoch: 0, Batch: 605, Train Loss: 1.588702\n",
      "Epoch: 0, Batch: 606, Train Loss: 1.551554\n",
      "Epoch: 0, Batch: 607, Train Loss: 1.572050\n",
      "Epoch: 0, Batch: 608, Train Loss: 1.580445\n",
      "Epoch: 0, Batch: 609, Train Loss: 1.562676\n",
      "Epoch: 0, Batch: 610, Train Loss: 1.573251\n",
      "Epoch: 0, Batch: 610, Train Loss: 1.573251, Validation Loss: 1.527254\n",
      "The called by [[Septemble Boundaria]] and [[Bent Silona]] and [[Chemberland Parelie]]. Thus is complete\n",
      "\n",
      "Epoch: 0, Batch: 611, Train Loss: 1.585891\n",
      "Epoch: 0, Batch: 612, Train Loss: 1.561501\n",
      "Epoch: 0, Batch: 613, Train Loss: 1.564025\n",
      "Epoch: 0, Batch: 614, Train Loss: 1.580719\n",
      "Epoch: 0, Batch: 615, Train Loss: 1.568750\n",
      "Epoch: 0, Batch: 616, Train Loss: 1.598452\n",
      "Epoch: 0, Batch: 617, Train Loss: 1.593368\n",
      "Epoch: 0, Batch: 618, Train Loss: 1.585092\n",
      "Epoch: 0, Batch: 619, Train Loss: 1.596365\n",
      "Epoch: 0, Batch: 620, Train Loss: 1.567489\n",
      "Epoch: 0, Batch: 620, Train Loss: 1.567489, Validation Loss: 1.521894\n",
      "The contract through the [[Marin Changel]].  The social of these species of [[societic stones|Contentio\n",
      "\n",
      "Epoch: 0, Batch: 621, Train Loss: 1.554953\n",
      "Epoch: 0, Batch: 622, Train Loss: 1.567088\n",
      "Epoch: 0, Batch: 623, Train Loss: 1.561855\n",
      "Epoch: 0, Batch: 624, Train Loss: 1.566517\n",
      "Epoch: 0, Batch: 625, Train Loss: 1.578967\n",
      "Epoch: 0, Batch: 626, Train Loss: 1.559716\n",
      "Epoch: 0, Batch: 627, Train Loss: 1.581796\n",
      "Epoch: 0, Batch: 628, Train Loss: 1.580401\n",
      "Epoch: 0, Batch: 629, Train Loss: 1.567783\n",
      "Epoch: 0, Batch: 630, Train Loss: 1.571860\n",
      "Epoch: 0, Batch: 630, Train Loss: 1.571860, Validation Loss: 1.519797\n",
      "The are can be seen as into a more property.  This was standing it is, intences to be internationally i\n",
      "\n",
      "Epoch: 0, Batch: 631, Train Loss: 1.568765\n",
      "Epoch: 0, Batch: 632, Train Loss: 1.558847\n",
      "Epoch: 0, Batch: 633, Train Loss: 1.553255\n",
      "Epoch: 0, Batch: 634, Train Loss: 1.567192\n",
      "Epoch: 0, Batch: 635, Train Loss: 1.562601\n",
      "Epoch: 0, Batch: 636, Train Loss: 1.567883\n",
      "Epoch: 0, Batch: 637, Train Loss: 1.574579\n",
      "Epoch: 0, Batch: 638, Train Loss: 1.585323\n",
      "Epoch: 0, Batch: 639, Train Loss: 1.575837\n",
      "Epoch: 0, Batch: 640, Train Loss: 1.575230\n",
      "Epoch: 0, Batch: 640, Train Loss: 1.575230, Validation Loss: 1.515746\n",
      "The and arches of sides of this accounters.  The [[Statistic]], also the [[More Contest]].\n",
      "\n",
      "=== English\n",
      "\n",
      "Epoch: 0, Batch: 641, Train Loss: 1.569839\n",
      "Epoch: 0, Batch: 642, Train Loss: 1.549153\n",
      "Epoch: 0, Batch: 643, Train Loss: 1.578191\n",
      "Epoch: 0, Batch: 644, Train Loss: 1.560507\n",
      "Epoch: 0, Batch: 645, Train Loss: 1.569697\n",
      "Epoch: 0, Batch: 646, Train Loss: 1.578338\n",
      "Epoch: 0, Batch: 647, Train Loss: 1.558883\n",
      "Epoch: 0, Batch: 648, Train Loss: 1.548178\n",
      "Epoch: 0, Batch: 649, Train Loss: 1.566711\n",
      "Epoch: 0, Batch: 650, Train Loss: 1.582014\n",
      "Epoch: 0, Batch: 650, Train Loss: 1.582014, Validation Loss: 1.511123\n",
      "The caller a model, it also intraded thought techniques that his team and stroke and such is town of co\n",
      "\n",
      "Epoch: 0, Batch: 651, Train Loss: 1.542838\n",
      "Epoch: 0, Batch: 652, Train Loss: 1.548227\n",
      "Epoch: 0, Batch: 653, Train Loss: 1.553084\n",
      "Epoch: 0, Batch: 654, Train Loss: 1.584699\n",
      "Epoch: 0, Batch: 655, Train Loss: 1.561656\n",
      "Epoch: 0, Batch: 656, Train Loss: 1.566287\n",
      "Epoch: 0, Batch: 657, Train Loss: 1.564328\n",
      "Epoch: 0, Batch: 658, Train Loss: 1.553680\n",
      "Epoch: 0, Batch: 659, Train Loss: 1.564731\n",
      "Epoch: 0, Batch: 660, Train Loss: 1.563264\n",
      "Epoch: 0, Batch: 660, Train Loss: 1.563264, Validation Loss: 1.509705\n",
      "The parties of the conclusive of a printing point of the present than the consence, without this supple\n",
      "\n",
      "Epoch: 0, Batch: 661, Train Loss: 1.549315\n",
      "Epoch: 0, Batch: 662, Train Loss: 1.549404\n",
      "Epoch: 0, Batch: 663, Train Loss: 1.538932\n",
      "Epoch: 0, Batch: 664, Train Loss: 1.549518\n",
      "Epoch: 0, Batch: 665, Train Loss: 1.560480\n",
      "Epoch: 0, Batch: 666, Train Loss: 1.574455\n",
      "Epoch: 0, Batch: 667, Train Loss: 1.551734\n",
      "Epoch: 0, Batch: 668, Train Loss: 1.543602\n",
      "Epoch: 0, Batch: 669, Train Loss: 1.558721\n",
      "Epoch: 0, Batch: 670, Train Loss: 1.554505\n",
      "Epoch: 0, Batch: 670, Train Loss: 1.554505, Validation Loss: 1.505974\n",
      "The three sources around a sense and improved in [[1980 of Alexander Musical Studio|Mentary States]] ar\n",
      "\n",
      "Epoch: 0, Batch: 671, Train Loss: 1.560513\n",
      "Epoch: 0, Batch: 672, Train Loss: 1.565240\n",
      "Epoch: 0, Batch: 673, Train Loss: 1.575920\n",
      "Epoch: 0, Batch: 674, Train Loss: 1.545807\n",
      "Epoch: 0, Batch: 675, Train Loss: 1.546481\n",
      "Epoch: 0, Batch: 676, Train Loss: 1.544742\n",
      "Epoch: 0, Batch: 677, Train Loss: 1.553872\n",
      "Epoch: 0, Batch: 678, Train Loss: 1.553153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 679, Train Loss: 1.566963\n",
      "Epoch: 0, Batch: 680, Train Loss: 1.553434\n",
      "Epoch: 0, Batch: 680, Train Loss: 1.553434, Validation Loss: 1.503200\n",
      "There in [[Sensa]]. \n",
      "* [[1987]].  In [[1966]])\n",
      "* [[Asian Monards]] (1962)\n",
      "*[http://polintics.com/ The S\n",
      "\n",
      "Epoch: 0, Batch: 681, Train Loss: 1.520113\n",
      "Epoch: 0, Batch: 682, Train Loss: 1.530213\n",
      "Epoch: 0, Batch: 683, Train Loss: 1.547499\n",
      "Epoch: 0, Batch: 684, Train Loss: 1.542317\n",
      "Epoch: 0, Batch: 685, Train Loss: 1.556677\n",
      "Epoch: 0, Batch: 686, Train Loss: 1.557138\n",
      "Epoch: 0, Batch: 687, Train Loss: 1.555717\n",
      "Epoch: 0, Batch: 688, Train Loss: 1.546523\n",
      "Epoch: 0, Batch: 689, Train Loss: 1.553635\n",
      "Epoch: 0, Batch: 690, Train Loss: 1.567227\n",
      "Epoch: 0, Batch: 690, Train Loss: 1.567227, Validation Loss: 1.501736\n",
      "The Councy in this is originally repulsed in the saility of carrow. This will a pertost the famouthern \n",
      "\n",
      "Epoch: 0, Batch: 691, Train Loss: 1.548873\n",
      "Epoch: 0, Batch: 692, Train Loss: 1.562877\n",
      "Epoch: 0, Batch: 693, Train Loss: 1.549139\n",
      "Epoch: 0, Batch: 694, Train Loss: 1.550736\n",
      "Epoch: 0, Batch: 695, Train Loss: 1.560269\n",
      "Epoch: 0, Batch: 696, Train Loss: 1.563547\n",
      "Epoch: 0, Batch: 697, Train Loss: 1.549068\n",
      "Epoch: 0, Batch: 698, Train Loss: 1.557779\n",
      "Epoch: 0, Batch: 699, Train Loss: 1.535140\n",
      "Epoch: 0, Batch: 700, Train Loss: 1.539065\n",
      "Epoch: 0, Batch: 700, Train Loss: 1.539065, Validation Loss: 1.496058\n",
      "The [[Conder Stole Clared States]] and [[Mild Boto]], [[Maying March 1978]] ([[1970]] (''[[Chrain Sterl\n",
      "\n",
      "Epoch: 0, Batch: 701, Train Loss: 1.538385\n",
      "Epoch: 0, Batch: 702, Train Loss: 1.554158\n",
      "Epoch: 0, Batch: 703, Train Loss: 1.546558\n",
      "Epoch: 0, Batch: 704, Train Loss: 1.536160\n",
      "Epoch: 0, Batch: 705, Train Loss: 1.536281\n",
      "Epoch: 0, Batch: 706, Train Loss: 1.521645\n",
      "Epoch: 0, Batch: 707, Train Loss: 1.519804\n",
      "Epoch: 0, Batch: 708, Train Loss: 1.526624\n",
      "Epoch: 0, Batch: 709, Train Loss: 1.516292\n",
      "Epoch: 0, Batch: 710, Train Loss: 1.505308\n",
      "Epoch: 0, Batch: 710, Train Loss: 1.505308, Validation Loss: 1.494180\n",
      "The [[Presidentional Christianity of Computer]] and [[South Mittal Coup (that to the [[Chambring of Bus\n",
      "\n",
      "Epoch: 0, Batch: 711, Train Loss: 1.518190\n",
      "Epoch: 0, Batch: 712, Train Loss: 1.508993\n",
      "Epoch: 0, Batch: 713, Train Loss: 1.494827\n",
      "Epoch: 0, Batch: 714, Train Loss: 1.518901\n",
      "Epoch: 0, Batch: 715, Train Loss: 1.510628\n",
      "Epoch: 0, Batch: 716, Train Loss: 1.527027\n",
      "Epoch: 0, Batch: 717, Train Loss: 1.548075\n",
      "Epoch: 0, Batch: 718, Train Loss: 1.546550\n",
      "Epoch: 0, Batch: 719, Train Loss: 1.557516\n",
      "Epoch: 0, Batch: 720, Train Loss: 1.555988\n",
      "Epoch: 0, Batch: 720, Train Loss: 1.555988, Validation Loss: 1.490579\n",
      "The [http://www.channillastoco.com/ Astropology and Amart] ([[Christian Carter Steam]]), [[Southern Pal\n",
      "\n",
      "Epoch: 0, Batch: 721, Train Loss: 1.560257\n",
      "Epoch: 0, Batch: 722, Train Loss: 1.559322\n",
      "Epoch: 0, Batch: 723, Train Loss: 1.549278\n",
      "Epoch: 0, Batch: 724, Train Loss: 1.539326\n",
      "Epoch: 0, Batch: 725, Train Loss: 1.533551\n",
      "Epoch: 0, Batch: 726, Train Loss: 1.527315\n",
      "Epoch: 0, Batch: 727, Train Loss: 1.536298\n",
      "Epoch: 0, Batch: 728, Train Loss: 1.545151\n",
      "Epoch: 0, Batch: 729, Train Loss: 1.537354\n",
      "Epoch: 0, Batch: 730, Train Loss: 1.544175\n",
      "Epoch: 0, Batch: 730, Train Loss: 1.544175, Validation Loss: 1.486510\n",
      "The acts and the [[State of the George of Archip Standers (correct)|Carrio]] and [[Bannard]], [[Arices]\n",
      "\n",
      "Epoch: 0, Batch: 731, Train Loss: 1.521710\n",
      "Epoch: 0, Batch: 732, Train Loss: 1.546227\n",
      "Epoch: 0, Batch: 733, Train Loss: 1.545459\n",
      "Epoch: 0, Batch: 734, Train Loss: 1.523720\n",
      "Epoch: 0, Batch: 735, Train Loss: 1.523066\n",
      "Epoch: 0, Batch: 736, Train Loss: 1.550684\n",
      "Epoch: 0, Batch: 737, Train Loss: 1.548696\n",
      "Epoch: 0, Batch: 738, Train Loss: 1.567617\n",
      "Epoch: 0, Batch: 739, Train Loss: 1.544084\n",
      "Epoch: 0, Batch: 740, Train Loss: 1.535493\n",
      "Epoch: 0, Batch: 740, Train Loss: 1.535493, Validation Loss: 1.482152\n",
      "The areas of [[Sampies]], [[Sergies of Comparities]].\n",
      "\n",
      "The sense of [[Andra Mark Anders (College and Co\n",
      "\n",
      "Epoch: 0, Batch: 741, Train Loss: 1.518426\n",
      "Epoch: 0, Batch: 742, Train Loss: 1.524124\n",
      "Epoch: 0, Batch: 743, Train Loss: 1.537486\n",
      "Epoch: 0, Batch: 744, Train Loss: 1.519299\n",
      "Epoch: 0, Batch: 745, Train Loss: 1.527794\n",
      "Epoch: 0, Batch: 746, Train Loss: 1.536252\n",
      "Epoch: 0, Batch: 747, Train Loss: 1.525591\n",
      "Epoch: 0, Batch: 748, Train Loss: 1.503282\n",
      "Epoch: 0, Batch: 749, Train Loss: 1.525202\n",
      "Epoch: 0, Batch: 750, Train Loss: 1.519877\n",
      "Epoch: 0, Batch: 750, Train Loss: 1.519877, Validation Loss: 1.481095\n",
      "The simple and stature of the particular to the time in those and the crain is the faction of a stand t\n",
      "\n",
      "Epoch: 0, Batch: 751, Train Loss: 1.527421\n",
      "Epoch: 0, Batch: 752, Train Loss: 1.532125\n",
      "Epoch: 0, Batch: 753, Train Loss: 1.531320\n",
      "Epoch: 0, Batch: 754, Train Loss: 1.519494\n",
      "Epoch: 0, Batch: 755, Train Loss: 1.518883\n",
      "Epoch: 0, Batch: 756, Train Loss: 1.537443\n",
      "Epoch: 0, Batch: 757, Train Loss: 1.521674\n",
      "Epoch: 0, Batch: 758, Train Loss: 1.552976\n",
      "Epoch: 0, Batch: 759, Train Loss: 1.556355\n",
      "Epoch: 0, Batch: 760, Train Loss: 1.523844\n",
      "Epoch: 0, Batch: 760, Train Loss: 1.523844, Validation Loss: 1.476947\n",
      "The company is a source and to part of their algorithm was a can rouss. Although the content to the fam\n",
      "\n",
      "Epoch: 0, Batch: 761, Train Loss: 1.550807\n",
      "Epoch: 0, Batch: 762, Train Loss: 1.524980\n",
      "Epoch: 0, Batch: 763, Train Loss: 1.522202\n",
      "Epoch: 0, Batch: 764, Train Loss: 1.514258\n",
      "Epoch: 0, Batch: 765, Train Loss: 1.492419\n",
      "Epoch: 0, Batch: 766, Train Loss: 1.480016\n",
      "Epoch: 0, Batch: 767, Train Loss: 1.495195\n",
      "Epoch: 0, Batch: 768, Train Loss: 1.511779\n",
      "Epoch: 0, Batch: 769, Train Loss: 1.495801\n",
      "Epoch: 0, Batch: 770, Train Loss: 1.479977\n",
      "Epoch: 0, Batch: 770, Train Loss: 1.479977, Validation Loss: 1.475129\n",
      "The service of [[Bibral]] ([[1898]]) in [[1882 in [[1982]]. The [[Server Church]] in [[1972 American]]\n",
      "\n",
      "\n",
      "Epoch: 0, Batch: 771, Train Loss: 1.472908\n",
      "Epoch: 0, Batch: 772, Train Loss: 1.499755\n",
      "Epoch: 0, Batch: 773, Train Loss: 1.499262\n",
      "Epoch: 0, Batch: 774, Train Loss: 1.482577\n",
      "Epoch: 0, Batch: 775, Train Loss: 1.489605\n",
      "Epoch: 0, Batch: 776, Train Loss: 1.484365\n",
      "Epoch: 0, Batch: 777, Train Loss: 1.472785\n",
      "Epoch: 0, Batch: 778, Train Loss: 1.478015\n",
      "Epoch: 0, Batch: 779, Train Loss: 1.482379\n",
      "Epoch: 0, Batch: 780, Train Loss: 1.478992\n",
      "Epoch: 0, Batch: 780, Train Loss: 1.478992, Validation Loss: 1.473146\n",
      "The tradited in particular through the passion-product with the strent of the set of the [[Algoria|Conn\n",
      "\n",
      "Epoch: 0, Batch: 781, Train Loss: 1.467531\n",
      "Epoch: 0, Batch: 782, Train Loss: 1.507838\n",
      "Epoch: 0, Batch: 783, Train Loss: 1.486946\n",
      "Epoch: 0, Batch: 784, Train Loss: 1.494197\n",
      "Epoch: 0, Batch: 785, Train Loss: 1.486860\n",
      "Epoch: 0, Batch: 786, Train Loss: 1.486169\n",
      "Epoch: 0, Batch: 787, Train Loss: 1.490960\n",
      "Epoch: 0, Batch: 788, Train Loss: 1.502350\n",
      "Epoch: 0, Batch: 789, Train Loss: 1.507796\n",
      "Epoch: 0, Batch: 790, Train Loss: 1.511661\n",
      "Epoch: 0, Batch: 790, Train Loss: 1.511661, Validation Loss: 1.469002\n",
      "The sense of [[Augenistari and, American]] is, high standing, this it is commissive to contribute a mea\n",
      "\n",
      "Epoch: 0, Batch: 791, Train Loss: 1.536750\n",
      "Epoch: 0, Batch: 792, Train Loss: 1.500524\n",
      "Epoch: 0, Batch: 793, Train Loss: 1.495391\n",
      "Epoch: 0, Batch: 794, Train Loss: 1.514363\n",
      "Epoch: 0, Batch: 795, Train Loss: 1.525203\n",
      "Epoch: 0, Batch: 796, Train Loss: 1.532380\n",
      "Epoch: 0, Batch: 797, Train Loss: 1.519928\n",
      "Epoch: 0, Batch: 798, Train Loss: 1.517025\n",
      "Epoch: 0, Batch: 799, Train Loss: 1.508203\n",
      "Epoch: 0, Batch: 800, Train Loss: 1.508780\n",
      "Epoch: 0, Batch: 800, Train Loss: 1.508780, Validation Loss: 1.465652\n",
      "The commentations in [[College of Community. The Chambines American]] (b. 1988) [[1980]] (2001). \n",
      "\n",
      "[[Im\n",
      "\n",
      "Epoch: 0, Batch: 801, Train Loss: 1.513332\n",
      "Epoch: 0, Batch: 802, Train Loss: 1.512384\n",
      "Epoch: 0, Batch: 803, Train Loss: 1.491406\n",
      "Epoch: 0, Batch: 804, Train Loss: 1.485486\n",
      "Epoch: 0, Batch: 805, Train Loss: 1.487075\n",
      "Epoch: 0, Batch: 806, Train Loss: 1.514351\n",
      "Epoch: 0, Batch: 807, Train Loss: 1.509858\n",
      "Epoch: 0, Batch: 808, Train Loss: 1.490140\n",
      "Epoch: 0, Batch: 809, Train Loss: 1.499804\n",
      "Epoch: 0, Batch: 810, Train Loss: 1.498865\n",
      "Epoch: 0, Batch: 810, Train Loss: 1.498865, Validation Loss: 1.461540\n",
      "The struke of the [[Storie City of Calaca]], was a music permisse in [[1989]], and the [[Armedo Califor\n",
      "\n",
      "Epoch: 0, Batch: 811, Train Loss: 1.492337\n",
      "Epoch: 0, Batch: 812, Train Loss: 1.511299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 813, Train Loss: 1.501618\n",
      "Epoch: 0, Batch: 814, Train Loss: 1.536074\n",
      "Epoch: 0, Batch: 815, Train Loss: 1.512901\n",
      "Epoch: 0, Batch: 816, Train Loss: 1.520002\n",
      "Epoch: 0, Batch: 817, Train Loss: 1.535629\n",
      "Epoch: 0, Batch: 818, Train Loss: 1.504126\n",
      "Epoch: 0, Batch: 819, Train Loss: 1.531224\n",
      "Epoch: 0, Batch: 820, Train Loss: 1.516116\n",
      "Epoch: 0, Batch: 820, Train Loss: 1.516116, Validation Loss: 1.459121\n",
      "The season way asses that all having also companies to the statistic capounts that there were also succ\n",
      "\n",
      "Epoch: 0, Batch: 821, Train Loss: 1.524034\n",
      "Epoch: 0, Batch: 822, Train Loss: 1.522433\n",
      "Epoch: 0, Batch: 823, Train Loss: 1.509391\n",
      "Epoch: 0, Batch: 824, Train Loss: 1.505304\n",
      "Epoch: 0, Batch: 825, Train Loss: 1.504586\n",
      "Epoch: 0, Batch: 826, Train Loss: 1.508713\n",
      "Epoch: 0, Batch: 827, Train Loss: 1.501060\n",
      "Epoch: 0, Batch: 828, Train Loss: 1.497540\n",
      "Epoch: 0, Batch: 829, Train Loss: 1.498721\n",
      "Epoch: 0, Batch: 830, Train Loss: 1.493461\n",
      "Epoch: 0, Batch: 830, Train Loss: 1.493461, Validation Loss: 1.458551\n",
      "The dramatic and conterestablishment of the [[Argo Steet]] and the [[Benesian problem]].\n",
      "\n",
      "Although the \n",
      "\n",
      "Epoch: 0, Batch: 831, Train Loss: 1.475484\n",
      "Epoch: 0, Batch: 832, Train Loss: 1.483069\n",
      "Epoch: 0, Batch: 833, Train Loss: 1.502395\n",
      "Epoch: 0, Batch: 834, Train Loss: 1.493469\n",
      "Epoch: 0, Batch: 835, Train Loss: 1.504509\n",
      "Epoch: 0, Batch: 836, Train Loss: 1.482769\n",
      "Epoch: 0, Batch: 837, Train Loss: 1.505050\n",
      "Epoch: 0, Batch: 838, Train Loss: 1.502300\n",
      "Epoch: 0, Batch: 839, Train Loss: 1.503347\n",
      "Epoch: 0, Batch: 840, Train Loss: 1.513335\n",
      "Epoch: 0, Batch: 840, Train Loss: 1.513335, Validation Loss: 1.455679\n",
      "There is also could to the company and conformation of [[Anglan]], [[Southern Cramician]] to beside it \n",
      "\n",
      "Epoch: 0, Batch: 841, Train Loss: 1.506119\n",
      "Epoch: 0, Batch: 842, Train Loss: 1.504030\n",
      "Epoch: 0, Batch: 843, Train Loss: 1.487955\n",
      "Epoch: 0, Batch: 844, Train Loss: 1.482799\n",
      "Epoch: 0, Batch: 845, Train Loss: 1.481260\n",
      "Epoch: 0, Batch: 846, Train Loss: 1.467880\n",
      "Epoch: 0, Batch: 847, Train Loss: 1.493433\n",
      "Epoch: 0, Batch: 848, Train Loss: 1.489360\n",
      "Epoch: 0, Batch: 849, Train Loss: 1.481346\n",
      "Epoch: 0, Batch: 850, Train Loss: 1.474454\n",
      "Epoch: 0, Batch: 850, Train Loss: 1.474454, Validation Loss: 1.452946\n",
      "There who was death to hose its connections. The film of his production to the state to a charactery of\n",
      "\n",
      "Epoch: 0, Batch: 851, Train Loss: 1.484722\n",
      "Epoch: 0, Batch: 852, Train Loss: 1.496177\n",
      "Epoch: 0, Batch: 853, Train Loss: 1.499742\n",
      "Epoch: 0, Batch: 854, Train Loss: 1.518495\n",
      "Epoch: 0, Batch: 855, Train Loss: 1.495299\n",
      "Epoch: 0, Batch: 856, Train Loss: 1.492545\n",
      "Epoch: 0, Batch: 857, Train Loss: 1.458923\n",
      "Epoch: 0, Batch: 858, Train Loss: 1.482438\n",
      "Epoch: 0, Batch: 859, Train Loss: 1.453562\n",
      "Epoch: 0, Batch: 860, Train Loss: 1.449801\n",
      "Epoch: 0, Batch: 860, Train Loss: 1.449801, Validation Loss: 1.452356\n",
      "The [[Paracees]], [[Consona]] and [[Alacher]] and [[March Antione]], [[Malaria]] on [[1979]] where alth\n",
      "\n",
      "Epoch: 0, Batch: 861, Train Loss: 1.459325\n",
      "Epoch: 0, Batch: 862, Train Loss: 1.472143\n",
      "Epoch: 0, Batch: 863, Train Loss: 1.484319\n",
      "Epoch: 0, Batch: 864, Train Loss: 1.498618\n",
      "Epoch: 0, Batch: 865, Train Loss: 1.482494\n",
      "Epoch: 0, Batch: 866, Train Loss: 1.496701\n",
      "Epoch: 0, Batch: 867, Train Loss: 1.492167\n",
      "Epoch: 0, Batch: 868, Train Loss: 1.480711\n",
      "Epoch: 0, Batch: 869, Train Loss: 1.495864\n",
      "Epoch: 0, Batch: 870, Train Loss: 1.503169\n",
      "Epoch: 0, Batch: 870, Train Loss: 1.503169, Validation Loss: 1.451461\n",
      "The [[Sardia Court]], who had been as the [[United Kingding]] to be the [[Basces]], and was complex fin\n",
      "\n",
      "Epoch: 0, Batch: 871, Train Loss: 1.485350\n",
      "Epoch: 0, Batch: 872, Train Loss: 1.489219\n",
      "Epoch: 0, Batch: 873, Train Loss: 1.475475\n",
      "Epoch: 0, Batch: 874, Train Loss: 1.480913\n",
      "Epoch: 0, Batch: 875, Train Loss: 1.486586\n",
      "Epoch: 0, Batch: 876, Train Loss: 1.473670\n",
      "Epoch: 0, Batch: 877, Train Loss: 1.488967\n",
      "Epoch: 0, Batch: 878, Train Loss: 1.497636\n",
      "Epoch: 0, Batch: 879, Train Loss: 1.488835\n",
      "Epoch: 0, Batch: 880, Train Loss: 1.499549\n",
      "Epoch: 0, Batch: 880, Train Loss: 1.499549, Validation Loss: 1.449185\n",
      "The sidely internacing community who world television, this, is a [[continued out of soldier] or [[poli\n",
      "\n",
      "Epoch: 0, Batch: 881, Train Loss: 1.499537\n",
      "Epoch: 0, Batch: 882, Train Loss: 1.485307\n",
      "Epoch: 0, Batch: 883, Train Loss: 1.482741\n",
      "Epoch: 0, Batch: 884, Train Loss: 1.482249\n",
      "Epoch: 0, Batch: 885, Train Loss: 1.478322\n",
      "Epoch: 0, Batch: 886, Train Loss: 1.506967\n",
      "Epoch: 0, Batch: 887, Train Loss: 1.480509\n",
      "Epoch: 0, Batch: 888, Train Loss: 1.485014\n",
      "Epoch: 0, Batch: 889, Train Loss: 1.480550\n",
      "Epoch: 0, Batch: 890, Train Loss: 1.485191\n",
      "Epoch: 0, Batch: 890, Train Loss: 1.485191, Validation Loss: 1.444873\n",
      "The [[Socalist]].  In the starting to their articles take of the top come is never of a positival sense\n",
      "\n",
      "Epoch: 0, Batch: 891, Train Loss: 1.488230\n",
      "Epoch: 0, Batch: 892, Train Loss: 1.474936\n",
      "Epoch: 0, Batch: 893, Train Loss: 1.467231\n",
      "Epoch: 0, Batch: 894, Train Loss: 1.477810\n",
      "Epoch: 0, Batch: 895, Train Loss: 1.478256\n",
      "Epoch: 0, Batch: 896, Train Loss: 1.485967\n",
      "Epoch: 0, Batch: 897, Train Loss: 1.481286\n",
      "Epoch: 0, Batch: 898, Train Loss: 1.486339\n",
      "Epoch: 0, Batch: 899, Train Loss: 1.488639\n",
      "Epoch: 0, Batch: 900, Train Loss: 1.479119\n",
      "Epoch: 0, Batch: 900, Train Loss: 1.479119, Validation Loss: 1.441523\n",
      "The [[Allin Carbaguarites|Greum]] or [[Social Bulliums|Calendar]], who such as ''[[The Charle Prime Por\n",
      "\n",
      "Epoch: 0, Batch: 901, Train Loss: 1.483732\n",
      "Epoch: 0, Batch: 902, Train Loss: 1.467218\n",
      "Epoch: 0, Batch: 903, Train Loss: 1.460298\n",
      "Epoch: 0, Batch: 904, Train Loss: 1.486713\n",
      "Epoch: 0, Batch: 905, Train Loss: 1.465325\n",
      "Epoch: 0, Batch: 906, Train Loss: 1.462390\n",
      "Epoch: 0, Batch: 907, Train Loss: 1.474880\n",
      "Epoch: 0, Batch: 908, Train Loss: 1.487309\n",
      "Epoch: 0, Batch: 909, Train Loss: 1.484259\n",
      "Epoch: 0, Batch: 910, Train Loss: 1.483642\n",
      "Epoch: 0, Batch: 910, Train Loss: 1.483642, Validation Loss: 1.440227\n",
      "The [[Munder]] in [[1948]] the took the power and increasingly disease or one of the collection of arch\n",
      "\n",
      "Epoch: 0, Batch: 911, Train Loss: 1.475102\n",
      "Epoch: 0, Batch: 912, Train Loss: 1.487258\n",
      "Epoch: 0, Batch: 913, Train Loss: 1.481433\n",
      "Epoch: 0, Batch: 914, Train Loss: 1.470583\n",
      "Epoch: 0, Batch: 915, Train Loss: 1.465430\n",
      "Epoch: 0, Batch: 916, Train Loss: 1.473435\n",
      "Epoch: 0, Batch: 917, Train Loss: 1.481876\n",
      "Epoch: 0, Batch: 918, Train Loss: 1.474064\n",
      "Epoch: 0, Batch: 919, Train Loss: 1.474848\n",
      "Epoch: 0, Batch: 920, Train Loss: 1.483310\n",
      "Epoch: 0, Batch: 920, Train Loss: 1.483310, Validation Loss: 1.440126\n",
      "These companies are chreases and the first structure. This settled although the court of the choacher s\n",
      "\n",
      "Epoch: 0, Batch: 921, Train Loss: 1.482765\n",
      "Epoch: 0, Batch: 922, Train Loss: 1.482671\n",
      "Epoch: 0, Batch: 923, Train Loss: 1.502906\n",
      "Epoch: 0, Batch: 924, Train Loss: 1.502706\n",
      "Epoch: 0, Batch: 925, Train Loss: 1.510645\n",
      "Epoch: 0, Batch: 926, Train Loss: 1.499791\n",
      "Epoch: 0, Batch: 927, Train Loss: 1.482671\n",
      "Epoch: 0, Batch: 928, Train Loss: 1.478629\n",
      "Epoch: 0, Batch: 929, Train Loss: 1.499735\n",
      "Epoch: 0, Batch: 930, Train Loss: 1.485951\n",
      "Epoch: 0, Batch: 930, Train Loss: 1.485951, Validation Loss: 1.437763\n",
      "The case of a search takus on started to complete for the portions of arrangencing and seen to the miss\n",
      "\n",
      "Epoch: 0, Batch: 931, Train Loss: 1.489558\n",
      "Epoch: 0, Batch: 932, Train Loss: 1.492675\n",
      "Epoch: 0, Batch: 933, Train Loss: 1.473194\n",
      "Epoch: 0, Batch: 934, Train Loss: 1.467979\n",
      "Epoch: 0, Batch: 935, Train Loss: 1.452149\n",
      "Epoch: 0, Batch: 936, Train Loss: 1.443504\n",
      "Epoch: 0, Batch: 937, Train Loss: 1.458115\n",
      "Epoch: 0, Batch: 938, Train Loss: 1.450958\n",
      "Epoch: 0, Batch: 939, Train Loss: 1.458835\n",
      "Epoch: 0, Batch: 940, Train Loss: 1.465606\n",
      "Epoch: 0, Batch: 940, Train Loss: 1.465606, Validation Loss: 1.434674\n",
      "The computer of the main territory in the period of that with the central player would attained in [[18\n",
      "\n",
      "Epoch: 0, Batch: 941, Train Loss: 1.452140\n",
      "Epoch: 0, Batch: 942, Train Loss: 1.448646\n",
      "Epoch: 0, Batch: 943, Train Loss: 1.447727\n",
      "Epoch: 0, Batch: 944, Train Loss: 1.483051\n",
      "Epoch: 0, Batch: 945, Train Loss: 1.460189\n",
      "Epoch: 0, Batch: 946, Train Loss: 1.465743\n",
      "Epoch: 0, Batch: 947, Train Loss: 1.454519\n",
      "Epoch: 0, Batch: 948, Train Loss: 1.457399\n",
      "Epoch: 0, Batch: 949, Train Loss: 1.439273\n",
      "Epoch: 0, Batch: 950, Train Loss: 1.451766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 950, Train Loss: 1.451766, Validation Loss: 1.430607\n",
      "The ant time at his case of them thought and property is context. This station in [[Mushican Parliament\n",
      "\n",
      "Epoch: 0, Batch: 951, Train Loss: 1.472218\n",
      "Epoch: 0, Batch: 952, Train Loss: 1.472297\n",
      "Epoch: 0, Batch: 953, Train Loss: 1.475721\n",
      "Epoch: 0, Batch: 954, Train Loss: 1.440582\n",
      "Epoch: 0, Batch: 955, Train Loss: 1.445019\n",
      "Epoch: 0, Batch: 956, Train Loss: 1.456233\n",
      "Epoch: 0, Batch: 957, Train Loss: 1.459765\n",
      "Epoch: 0, Batch: 958, Train Loss: 1.479149\n",
      "Epoch: 0, Batch: 959, Train Loss: 1.455780\n",
      "Epoch: 0, Batch: 960, Train Loss: 1.454243\n",
      "Epoch: 0, Batch: 960, Train Loss: 1.454243, Validation Loss: 1.429584\n",
      "The sea will be too things of the [[Britain]], in [[1984]]. Although it was an [[sambid]] of a [[proble\n",
      "\n",
      "Epoch: 0, Batch: 961, Train Loss: 1.456853\n",
      "Epoch: 0, Batch: 962, Train Loss: 1.486964\n",
      "Epoch: 0, Batch: 963, Train Loss: 1.476875\n",
      "Epoch: 0, Batch: 964, Train Loss: 1.491255\n",
      "Epoch: 0, Batch: 965, Train Loss: 1.483686\n",
      "Epoch: 0, Batch: 966, Train Loss: 1.463787\n",
      "Epoch: 0, Batch: 967, Train Loss: 1.472262\n",
      "Epoch: 0, Batch: 968, Train Loss: 1.486172\n",
      "Epoch: 0, Batch: 969, Train Loss: 1.458876\n",
      "Epoch: 0, Batch: 970, Train Loss: 1.454518\n",
      "Epoch: 0, Batch: 970, Train Loss: 1.454518, Validation Loss: 1.429249\n",
      "There is not beer sometimes ten in the possible, and it is such as [[panthes]] to be the case of compan\n",
      "\n",
      "Epoch: 0, Batch: 971, Train Loss: 1.452714\n",
      "Epoch: 0, Batch: 972, Train Loss: 1.465192\n",
      "Epoch: 0, Batch: 973, Train Loss: 1.439440\n",
      "Epoch: 0, Batch: 974, Train Loss: 1.453188\n",
      "Epoch: 0, Batch: 975, Train Loss: 1.483251\n",
      "Epoch: 0, Batch: 976, Train Loss: 1.454846\n",
      "Epoch: 0, Batch: 977, Train Loss: 1.452381\n",
      "Epoch: 0, Batch: 978, Train Loss: 1.455078\n",
      "Epoch: 0, Batch: 979, Train Loss: 1.464512\n",
      "Epoch: 0, Batch: 980, Train Loss: 1.473783\n",
      "Epoch: 0, Batch: 980, Train Loss: 1.473783, Validation Loss: 1.430162\n",
      "Therapine theorophine areas as the [[Bolomard]] (a [[mathematical concentration]]). In a color of a com\n",
      "\n",
      "Epoch: 0, Batch: 981, Train Loss: 1.465623\n",
      "Epoch: 0, Batch: 982, Train Loss: 1.466761\n",
      "Epoch: 0, Batch: 983, Train Loss: 1.459828\n",
      "Epoch: 0, Batch: 984, Train Loss: 1.495965\n",
      "Epoch: 0, Batch: 985, Train Loss: 1.455731\n",
      "Epoch: 0, Batch: 986, Train Loss: 1.479420\n",
      "Epoch: 0, Batch: 987, Train Loss: 1.466324\n",
      "Epoch: 0, Batch: 988, Train Loss: 1.449789\n",
      "Epoch: 0, Batch: 989, Train Loss: 1.449700\n",
      "Epoch: 0, Batch: 990, Train Loss: 1.438527\n",
      "Epoch: 0, Batch: 990, Train Loss: 1.438527, Validation Loss: 1.428463\n",
      "The [[Surray]] and others of this informative technical and compossible structure importants of some ac\n",
      "\n",
      "Epoch: 0, Batch: 991, Train Loss: 1.439278\n",
      "Epoch: 0, Batch: 992, Train Loss: 1.463392\n",
      "Epoch: 0, Batch: 993, Train Loss: 1.443437\n",
      "Epoch: 0, Batch: 994, Train Loss: 1.460175\n",
      "Epoch: 0, Batch: 995, Train Loss: 1.471362\n",
      "Epoch: 0, Batch: 996, Train Loss: 1.461089\n",
      "Epoch: 0, Batch: 997, Train Loss: 1.475567\n",
      "Epoch: 0, Batch: 998, Train Loss: 1.466336\n",
      "Epoch: 0, Batch: 999, Train Loss: 1.460452\n",
      "Epoch: 0, Batch: 1000, Train Loss: 1.452936\n",
      "Epoch: 0, Batch: 1000, Train Loss: 1.452936, Validation Loss: 1.425399\n",
      "The powers, the same to several secondary sound by the [[Carlo Banning]] of Company, held they track of\n",
      "\n",
      "Epoch: 0, Batch: 1001, Train Loss: 1.457132\n",
      "Epoch: 0, Batch: 1002, Train Loss: 1.450553\n",
      "Epoch: 0, Batch: 1003, Train Loss: 1.463934\n",
      "Epoch: 0, Batch: 1004, Train Loss: 1.454895\n",
      "Epoch: 0, Batch: 1005, Train Loss: 1.457617\n",
      "Epoch: 0, Batch: 1006, Train Loss: 1.485512\n",
      "Epoch: 0, Batch: 1007, Train Loss: 1.475037\n",
      "Epoch: 0, Batch: 1008, Train Loss: 1.457059\n",
      "Epoch: 0, Batch: 1009, Train Loss: 1.458529\n",
      "Epoch: 0, Batch: 1010, Train Loss: 1.440537\n",
      "Epoch: 0, Batch: 1010, Train Loss: 1.440537, Validation Loss: 1.418976\n",
      "The and anticome and combined by [[Ministry of Austria]], [[Minis Point August]] and [[Mark Beathesis|S\n",
      "\n",
      "Epoch: 0, Batch: 1011, Train Loss: 1.444593\n",
      "Epoch: 0, Batch: 1012, Train Loss: 1.453688\n",
      "Epoch: 0, Batch: 1013, Train Loss: 1.477407\n",
      "Epoch: 0, Batch: 1014, Train Loss: 1.454668\n",
      "Epoch: 0, Batch: 1015, Train Loss: 1.450139\n",
      "Epoch: 0, Batch: 1016, Train Loss: 1.440397\n",
      "Epoch: 0, Batch: 1017, Train Loss: 1.474181\n",
      "Epoch: 0, Batch: 1018, Train Loss: 1.459300\n",
      "Epoch: 0, Batch: 1019, Train Loss: 1.447334\n",
      "Epoch: 0, Batch: 1020, Train Loss: 1.455121\n",
      "Epoch: 0, Batch: 1020, Train Loss: 1.455121, Validation Loss: 1.417051\n",
      "The could of the former [[Sound See also|China|Christon]], an alternate situes.  At the [[Samalan Princ\n",
      "\n",
      "Epoch: 0, Batch: 1021, Train Loss: 1.471853\n",
      "Epoch: 0, Batch: 1022, Train Loss: 1.466571\n",
      "Epoch: 0, Batch: 1023, Train Loss: 1.467556\n",
      "Epoch: 0, Batch: 1024, Train Loss: 1.479673\n",
      "Epoch: 0, Batch: 1025, Train Loss: 1.456023\n",
      "Epoch: 0, Batch: 1026, Train Loss: 1.461488\n",
      "Epoch: 0, Batch: 1027, Train Loss: 1.445878\n",
      "Epoch: 0, Batch: 1028, Train Loss: 1.459072\n",
      "Epoch: 0, Batch: 1029, Train Loss: 1.496519\n",
      "Epoch: 0, Batch: 1030, Train Loss: 1.481625\n",
      "Epoch: 0, Batch: 1030, Train Loss: 1.481625, Validation Loss: 1.415880\n",
      "The [[Strate]], [[Christian Start Steek]].  There is the series of success and anti-parable production \n",
      "\n",
      "Epoch: 0, Batch: 1031, Train Loss: 1.479887\n",
      "Epoch: 0, Batch: 1032, Train Loss: 1.470894\n",
      "Epoch: 0, Batch: 1033, Train Loss: 1.469649\n",
      "Epoch: 0, Batch: 1034, Train Loss: 1.463424\n",
      "Epoch: 0, Batch: 1035, Train Loss: 1.462853\n",
      "Epoch: 0, Batch: 1036, Train Loss: 1.460448\n",
      "Epoch: 0, Batch: 1037, Train Loss: 1.480512\n",
      "Epoch: 0, Batch: 1038, Train Loss: 1.461483\n",
      "Epoch: 0, Batch: 1039, Train Loss: 1.462938\n",
      "Epoch: 0, Batch: 1040, Train Loss: 1.464334\n",
      "Epoch: 0, Batch: 1040, Train Loss: 1.464334, Validation Loss: 1.413708\n",
      "Thes was combated to be so as a perform that and the addition of consequent state into [[Bantista]]. As\n",
      "\n",
      "Epoch: 0, Batch: 1041, Train Loss: 1.464630\n",
      "Epoch: 0, Batch: 1042, Train Loss: 1.465475\n",
      "Epoch: 0, Batch: 1043, Train Loss: 1.465722\n",
      "Epoch: 0, Batch: 1044, Train Loss: 1.445163\n",
      "Epoch: 0, Batch: 1045, Train Loss: 1.467415\n",
      "Epoch: 0, Batch: 1046, Train Loss: 1.470144\n",
      "Epoch: 0, Batch: 1047, Train Loss: 1.454731\n",
      "Epoch: 0, Batch: 1048, Train Loss: 1.438431\n",
      "Epoch: 0, Batch: 1049, Train Loss: 1.436600\n",
      "Epoch: 0, Batch: 1050, Train Loss: 1.438398\n",
      "Epoch: 0, Batch: 1050, Train Loss: 1.438398, Validation Loss: 1.410962\n",
      "The population and station. As the [[Collic at Centing Constellation|Cinited Series]] in the [[Samary C\n",
      "\n",
      "Epoch: 0, Batch: 1051, Train Loss: 1.439440\n",
      "Epoch: 0, Batch: 1052, Train Loss: 1.441440\n",
      "Epoch: 0, Batch: 1053, Train Loss: 1.445037\n",
      "Epoch: 0, Batch: 1054, Train Loss: 1.449045\n",
      "Epoch: 0, Batch: 1055, Train Loss: 1.457629\n",
      "Epoch: 0, Batch: 1056, Train Loss: 1.459376\n",
      "Epoch: 0, Batch: 1057, Train Loss: 1.461993\n",
      "Epoch: 0, Batch: 1058, Train Loss: 1.437487\n",
      "Epoch: 0, Batch: 1059, Train Loss: 1.450208\n",
      "Epoch: 0, Batch: 1060, Train Loss: 1.487837\n",
      "Epoch: 0, Batch: 1060, Train Loss: 1.487837, Validation Loss: 1.410393\n",
      "The supernative selling implementations, and caused and police straye as that it is.  The theater tried\n",
      "\n",
      "Epoch: 0, Batch: 1061, Train Loss: 1.438977\n",
      "Epoch: 0, Batch: 1062, Train Loss: 1.444848\n",
      "Epoch: 0, Batch: 1063, Train Loss: 1.439887\n",
      "Epoch: 0, Batch: 1064, Train Loss: 1.430261\n",
      "Epoch: 0, Batch: 1065, Train Loss: 1.419963\n",
      "Epoch: 0, Batch: 1066, Train Loss: 1.428464\n",
      "Epoch: 0, Batch: 1067, Train Loss: 1.428693\n",
      "Epoch: 0, Batch: 1068, Train Loss: 1.412670\n",
      "Epoch: 0, Batch: 1069, Train Loss: 1.415876\n",
      "Epoch: 0, Batch: 1070, Train Loss: 1.436661\n",
      "Epoch: 0, Batch: 1070, Train Loss: 1.436661, Validation Loss: 1.408867\n",
      "The capacition was the [[Special Peace As on Austria|Area Concernational Contentional Accent Compussion\n",
      "\n",
      "Epoch: 0, Batch: 1071, Train Loss: 1.435928\n",
      "Epoch: 0, Batch: 1072, Train Loss: 1.425581\n",
      "Epoch: 0, Batch: 1073, Train Loss: 1.445485\n",
      "Epoch: 0, Batch: 1074, Train Loss: 1.445284\n",
      "Epoch: 0, Batch: 1075, Train Loss: 1.438996\n",
      "Epoch: 0, Batch: 1076, Train Loss: 1.427898\n",
      "Epoch: 0, Batch: 1077, Train Loss: 1.453232\n",
      "Epoch: 0, Batch: 1078, Train Loss: 1.445350\n",
      "Epoch: 0, Batch: 1079, Train Loss: 1.418840\n",
      "Epoch: 0, Batch: 1080, Train Loss: 1.430308\n",
      "Epoch: 0, Batch: 1080, Train Loss: 1.430308, Validation Loss: 1.409596\n",
      "The decision of the solid correposition was closely that any action to shept were a settler. He color s\n",
      "\n",
      "Epoch: 0, Batch: 1081, Train Loss: 1.438808\n",
      "Epoch: 0, Batch: 1082, Train Loss: 1.449290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 1083, Train Loss: 1.424655\n",
      "Epoch: 0, Batch: 1084, Train Loss: 1.428793\n",
      "Epoch: 0, Batch: 1085, Train Loss: 1.434705\n",
      "Epoch: 0, Batch: 1086, Train Loss: 1.437099\n",
      "Epoch: 0, Batch: 1087, Train Loss: 1.453760\n",
      "Epoch: 0, Batch: 1088, Train Loss: 1.426367\n",
      "Epoch: 0, Batch: 1089, Train Loss: 1.443293\n",
      "Epoch: 0, Batch: 1090, Train Loss: 1.429238\n",
      "Epoch: 0, Batch: 1090, Train Loss: 1.429238, Validation Loss: 1.406295\n",
      "The same and combat to coming in that was a [[patriame]] to the [[Society (stated)]], were are currentl\n",
      "\n",
      "Epoch: 0, Batch: 1091, Train Loss: 1.445670\n",
      "Epoch: 0, Batch: 1092, Train Loss: 1.450978\n",
      "Epoch: 0, Batch: 1093, Train Loss: 1.439157\n",
      "Epoch: 0, Batch: 1094, Train Loss: 1.446947\n",
      "Epoch: 0, Batch: 1095, Train Loss: 1.430885\n",
      "Epoch: 0, Batch: 1096, Train Loss: 1.457711\n",
      "Epoch: 0, Batch: 1097, Train Loss: 1.438607\n",
      "Epoch: 0, Batch: 1098, Train Loss: 1.439437\n",
      "Epoch: 0, Batch: 1099, Train Loss: 1.437906\n",
      "Epoch: 0, Batch: 1100, Train Loss: 1.447210\n",
      "Epoch: 0, Batch: 1100, Train Loss: 1.447210, Validation Loss: 1.407012\n",
      "There, is only incouraged to their situation of the players. As it is television of those of their resi\n",
      "\n",
      "Epoch: 0, Batch: 1101, Train Loss: 1.455868\n",
      "Epoch: 0, Batch: 1102, Train Loss: 1.433707\n",
      "Epoch: 0, Batch: 1103, Train Loss: 1.453092\n",
      "Epoch: 0, Batch: 1104, Train Loss: 1.447576\n",
      "Epoch: 0, Batch: 1105, Train Loss: 1.438083\n",
      "Epoch: 0, Batch: 1106, Train Loss: 1.447616\n",
      "Epoch: 0, Batch: 1107, Train Loss: 1.445381\n",
      "Epoch: 0, Batch: 1108, Train Loss: 1.465249\n",
      "Epoch: 0, Batch: 1109, Train Loss: 1.475632\n",
      "Epoch: 0, Batch: 1110, Train Loss: 1.475955\n",
      "Epoch: 0, Batch: 1110, Train Loss: 1.475955, Validation Loss: 1.402688\n",
      "The alternative term is contraded by atomallas. The populous instead are also provided an otther film, \n",
      "\n",
      "Epoch: 0, Batch: 1111, Train Loss: 1.481071\n",
      "Epoch: 0, Batch: 1112, Train Loss: 1.454576\n",
      "Epoch: 0, Batch: 1113, Train Loss: 1.464307\n",
      "Epoch: 0, Batch: 1114, Train Loss: 1.476261\n",
      "Epoch: 0, Batch: 1115, Train Loss: 1.454605\n",
      "Epoch: 0, Batch: 1116, Train Loss: 1.464716\n",
      "Epoch: 0, Batch: 1117, Train Loss: 1.468134\n",
      "Epoch: 0, Batch: 1118, Train Loss: 1.434317\n",
      "Epoch: 0, Batch: 1119, Train Loss: 1.441894\n",
      "Epoch: 0, Batch: 1120, Train Loss: 1.430628\n",
      "Epoch: 0, Batch: 1120, Train Loss: 1.430628, Validation Loss: 1.398279\n",
      "The allender in [[1878]] as a [[composing contempres]], and [[municipaeic]] countilian images and spini\n",
      "\n",
      "Epoch: 0, Batch: 1121, Train Loss: 1.438191\n",
      "Epoch: 0, Batch: 1122, Train Loss: 1.434003\n",
      "Epoch: 0, Batch: 1123, Train Loss: 1.446530\n",
      "Epoch: 0, Batch: 1124, Train Loss: 1.449491\n",
      "Epoch: 0, Batch: 1125, Train Loss: 1.452893\n",
      "Epoch: 0, Batch: 1126, Train Loss: 1.452968\n",
      "Epoch: 0, Batch: 1127, Train Loss: 1.453975\n",
      "Epoch: 0, Batch: 1128, Train Loss: 1.450101\n",
      "Epoch: 0, Batch: 1129, Train Loss: 1.469953\n",
      "Epoch: 0, Batch: 1130, Train Loss: 1.438597\n",
      "Epoch: 0, Batch: 1130, Train Loss: 1.438597, Validation Loss: 1.399250\n",
      "There also been a series of the female creature a field of [[Canada]], [[Salvani]] in the [[Commentive \n",
      "\n",
      "Epoch: 0, Batch: 1131, Train Loss: 1.452640\n",
      "Epoch: 0, Batch: 1132, Train Loss: 1.459879\n",
      "Epoch: 0, Batch: 1133, Train Loss: 1.442614\n",
      "Epoch: 0, Batch: 1134, Train Loss: 1.435490\n",
      "Epoch: 0, Batch: 1135, Train Loss: 1.434033\n",
      "Epoch: 0, Batch: 1136, Train Loss: 1.447079\n",
      "Epoch: 0, Batch: 1137, Train Loss: 1.464581\n",
      "Epoch: 0, Batch: 1138, Train Loss: 1.447021\n",
      "Epoch: 0, Batch: 1139, Train Loss: 1.439740\n",
      "Epoch: 0, Batch: 1140, Train Loss: 1.414494\n",
      "Epoch: 0, Batch: 1140, Train Loss: 1.414494, Validation Loss: 1.399096\n",
      "The also and society into the set of the precedence.\n",
      "\n",
      "===See also==\n",
      "*[[Antone Bengal State]]\n",
      "* [[Amagra\n",
      "\n",
      "Epoch: 0, Batch: 1141, Train Loss: 1.422723\n",
      "Epoch: 0, Batch: 1142, Train Loss: 1.420866\n",
      "Epoch: 0, Batch: 1143, Train Loss: 1.416002\n",
      "Epoch: 0, Batch: 1144, Train Loss: 1.438038\n",
      "Epoch: 0, Batch: 1145, Train Loss: 1.433662\n",
      "Epoch: 0, Batch: 1146, Train Loss: 1.424268\n",
      "Epoch: 0, Batch: 1147, Train Loss: 1.435016\n",
      "Epoch: 0, Batch: 1148, Train Loss: 1.432163\n",
      "Epoch: 0, Batch: 1149, Train Loss: 1.432830\n",
      "Epoch: 0, Batch: 1150, Train Loss: 1.432647\n",
      "Epoch: 0, Batch: 1150, Train Loss: 1.432647, Validation Loss: 1.398810\n",
      "The also bands of the support of the popular crutalizing and the [[United Medition]]. In addition to a \n",
      "\n",
      "Epoch: 0, Batch: 1151, Train Loss: 1.443469\n",
      "Epoch: 0, Batch: 1152, Train Loss: 1.444412\n",
      "Epoch: 0, Batch: 1153, Train Loss: 1.454661\n",
      "Epoch: 0, Batch: 1154, Train Loss: 1.459586\n",
      "Epoch: 0, Batch: 1155, Train Loss: 1.462251\n",
      "Epoch: 0, Batch: 1156, Train Loss: 1.437938\n",
      "Epoch: 0, Batch: 1157, Train Loss: 1.439496\n",
      "Epoch: 0, Batch: 1158, Train Loss: 1.444777\n",
      "Epoch: 0, Batch: 1159, Train Loss: 1.466826\n",
      "Epoch: 0, Batch: 1160, Train Loss: 1.460499\n",
      "Epoch: 0, Batch: 1160, Train Loss: 1.460499, Validation Loss: 1.398470\n",
      "The simplestor of the son [[Marriage]].\n",
      "\n",
      "=== South America's stabling to [[Samassia]]\n",
      "*[[Managari Stand\n",
      "\n",
      "Epoch: 0, Batch: 1161, Train Loss: 1.445683\n",
      "Epoch: 0, Batch: 1162, Train Loss: 1.440244\n",
      "Epoch: 0, Batch: 1163, Train Loss: 1.419704\n",
      "Epoch: 0, Batch: 1164, Train Loss: 1.439305\n",
      "Epoch: 0, Batch: 1165, Train Loss: 1.437939\n",
      "Epoch: 0, Batch: 1166, Train Loss: 1.444340\n",
      "Epoch: 0, Batch: 1167, Train Loss: 1.439235\n",
      "Epoch: 0, Batch: 1168, Train Loss: 1.446597\n",
      "Epoch: 0, Batch: 1169, Train Loss: 1.436066\n",
      "Epoch: 0, Batch: 1170, Train Loss: 1.447948\n",
      "Epoch: 0, Batch: 1170, Train Loss: 1.447948, Validation Loss: 1.395046\n",
      "The [[City of Bossian]] and a communication of the series of [[Community of China]]\n",
      "\n",
      "The [[Chicage Argu\n",
      "\n",
      "Epoch: 0, Batch: 1171, Train Loss: 1.427051\n",
      "Epoch: 0, Batch: 1172, Train Loss: 1.468202\n",
      "Epoch: 0, Batch: 1173, Train Loss: 1.440526\n",
      "Epoch: 0, Batch: 1174, Train Loss: 1.447082\n",
      "Epoch: 0, Batch: 1175, Train Loss: 1.434087\n",
      "Epoch: 0, Batch: 1176, Train Loss: 1.423704\n",
      "Epoch: 0, Batch: 1177, Train Loss: 1.440703\n",
      "Epoch: 0, Batch: 1178, Train Loss: 1.449896\n",
      "Epoch: 0, Batch: 1179, Train Loss: 1.428418\n",
      "Epoch: 0, Batch: 1180, Train Loss: 1.449008\n",
      "Epoch: 0, Batch: 1180, Train Loss: 1.449008, Validation Loss: 1.393202\n",
      "The [http://www.country.org/lefers/colleges/calculatas.com/stristic/diespondasic.case organically impro\n",
      "\n",
      "Epoch: 0, Batch: 1181, Train Loss: 1.441445\n",
      "Epoch: 0, Batch: 1182, Train Loss: 1.433892\n",
      "Epoch: 0, Batch: 1183, Train Loss: 1.436517\n",
      "Epoch: 0, Batch: 1184, Train Loss: 1.452373\n",
      "Epoch: 0, Batch: 1185, Train Loss: 1.436133\n",
      "Epoch: 0, Batch: 1186, Train Loss: 1.433427\n",
      "Epoch: 0, Batch: 1187, Train Loss: 1.429826\n",
      "Epoch: 0, Batch: 1188, Train Loss: 1.430155\n",
      "Epoch: 0, Batch: 1189, Train Loss: 1.435117\n",
      "Epoch: 0, Batch: 1190, Train Loss: 1.447696\n",
      "Epoch: 0, Batch: 1190, Train Loss: 1.447696, Validation Loss: 1.393047\n",
      "The few main acts to be played\n",
      "*[[1936]] - [[Canadian Elizabeth]] (2001) [[Midly-Monte Conceptions|Brea\n",
      "\n",
      "Epoch: 0, Batch: 1191, Train Loss: 1.431477\n",
      "Epoch: 0, Batch: 1192, Train Loss: 1.426788\n",
      "Epoch: 0, Batch: 1193, Train Loss: 1.418979\n",
      "Epoch: 0, Batch: 1194, Train Loss: 1.430095\n",
      "Epoch: 0, Batch: 1195, Train Loss: 1.424454\n",
      "Epoch: 0, Batch: 1196, Train Loss: 1.439689\n",
      "Epoch: 0, Batch: 1197, Train Loss: 1.423413\n",
      "Epoch: 0, Batch: 1198, Train Loss: 1.409570\n",
      "Epoch: 0, Batch: 1199, Train Loss: 1.400285\n",
      "Epoch: 0, Batch: 1200, Train Loss: 1.426721\n",
      "Epoch: 0, Batch: 1200, Train Loss: 1.426721, Validation Loss: 1.388961\n",
      "The accurate are standing to the [[Authoritan Reading]] (1998) and the [[Christopher Calcur]], was new \n",
      "\n",
      "Epoch: 0, Batch: 1201, Train Loss: 1.431162\n",
      "Epoch: 0, Batch: 1202, Train Loss: 1.417886\n",
      "Epoch: 0, Batch: 1203, Train Loss: 1.418267\n",
      "Epoch: 0, Batch: 1204, Train Loss: 1.408727\n",
      "Epoch: 0, Batch: 1205, Train Loss: 1.407040\n",
      "Epoch: 0, Batch: 1206, Train Loss: 1.431224\n",
      "Epoch: 0, Batch: 1207, Train Loss: 1.422688\n",
      "Epoch: 0, Batch: 1208, Train Loss: 1.432733\n",
      "Epoch: 0, Batch: 1209, Train Loss: 1.425660\n",
      "Epoch: 0, Batch: 1210, Train Loss: 1.438456\n",
      "Epoch: 0, Batch: 1210, Train Loss: 1.438456, Validation Loss: 1.388479\n",
      "There was no roller three as a [[clean]] and [[south-art]]s.  In the lines of the population that this \n",
      "\n",
      "Epoch: 0, Batch: 1211, Train Loss: 1.443380\n",
      "Epoch: 0, Batch: 1212, Train Loss: 1.435962\n",
      "Epoch: 0, Batch: 1213, Train Loss: 1.426330\n",
      "Epoch: 0, Batch: 1214, Train Loss: 1.447389\n",
      "Epoch: 0, Batch: 1215, Train Loss: 1.434637\n",
      "Epoch: 0, Batch: 1216, Train Loss: 1.437646\n",
      "Epoch: 0, Batch: 1217, Train Loss: 1.420073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 1218, Train Loss: 1.442925\n",
      "Epoch: 0, Batch: 1219, Train Loss: 1.435255\n",
      "Epoch: 0, Batch: 1220, Train Loss: 1.437492\n",
      "Epoch: 0, Batch: 1220, Train Loss: 1.437492, Validation Loss: 1.386883\n",
      "The civil rational playing in a prediction.  And there of the college of the second standing of [[Barti\n",
      "\n",
      "Epoch: 0, Batch: 1221, Train Loss: 1.433946\n",
      "Epoch: 0, Batch: 1222, Train Loss: 1.450746\n",
      "Epoch: 0, Batch: 1223, Train Loss: 1.440278\n",
      "Epoch: 0, Batch: 1224, Train Loss: 1.425331\n",
      "Epoch: 0, Batch: 1225, Train Loss: 1.414078\n",
      "Epoch: 0, Batch: 1226, Train Loss: 1.429964\n",
      "Epoch: 0, Batch: 1227, Train Loss: 1.434697\n",
      "Epoch: 0, Batch: 1228, Train Loss: 1.442446\n",
      "Epoch: 0, Batch: 1229, Train Loss: 1.452857\n",
      "Epoch: 0, Batch: 1230, Train Loss: 1.461060\n",
      "Epoch: 0, Batch: 1230, Train Loss: 1.461060, Validation Loss: 1.386154\n",
      "The film, [[Sarir Allen]], the team of [[South Autros]].\n",
      "\n",
      "==The sites===\n",
      "*''[[The Albania in the Aristo\n",
      "\n",
      "Epoch: 0, Batch: 1231, Train Loss: 1.450081\n",
      "Epoch: 0, Batch: 1232, Train Loss: 1.450142\n",
      "Epoch: 0, Batch: 1233, Train Loss: 1.438754\n",
      "Epoch: 0, Batch: 1234, Train Loss: 1.435579\n",
      "Epoch: 0, Batch: 1235, Train Loss: 1.435772\n",
      "Epoch: 0, Batch: 1236, Train Loss: 1.452221\n",
      "Epoch: 0, Batch: 1237, Train Loss: 1.436405\n",
      "Epoch: 0, Batch: 1238, Train Loss: 1.436872\n",
      "Epoch: 0, Batch: 1239, Train Loss: 1.441324\n",
      "Epoch: 0, Batch: 1240, Train Loss: 1.418450\n",
      "Epoch: 0, Batch: 1240, Train Loss: 1.418450, Validation Loss: 1.383118\n",
      "The maintain of the most solution is so that are not seen as the presenver, which the mainline on soldi\n",
      "\n",
      "Epoch: 0, Batch: 1241, Train Loss: 1.432627\n",
      "Epoch: 0, Batch: 1242, Train Loss: 1.420427\n",
      "Epoch: 0, Batch: 1243, Train Loss: 1.395155\n",
      "Epoch: 0, Batch: 1244, Train Loss: 1.426190\n",
      "Epoch: 0, Batch: 1245, Train Loss: 1.414259\n",
      "Epoch: 0, Batch: 1246, Train Loss: 1.427808\n",
      "Epoch: 0, Batch: 1247, Train Loss: 1.391099\n",
      "Epoch: 0, Batch: 1248, Train Loss: 1.410988\n",
      "Epoch: 0, Batch: 1249, Train Loss: 1.386336\n",
      "Epoch: 0, Batch: 1250, Train Loss: 1.411550\n",
      "Epoch: 0, Batch: 1250, Train Loss: 1.411550, Validation Loss: 1.384092\n",
      "The access that heir which was since the [[Christian Eastern Europe|Spanian Court]] in [[1949]] as a fi\n",
      "\n",
      "Epoch: 0, Batch: 1251, Train Loss: 1.427072\n",
      "Epoch: 0, Batch: 1252, Train Loss: 1.433449\n",
      "Epoch: 0, Batch: 1253, Train Loss: 1.431975\n",
      "Epoch: 0, Batch: 1254, Train Loss: 1.439059\n",
      "Epoch: 0, Batch: 1255, Train Loss: 1.443558\n",
      "Epoch: 0, Batch: 1256, Train Loss: 1.424479\n",
      "Epoch: 0, Batch: 1257, Train Loss: 1.423281\n",
      "Epoch: 0, Batch: 1258, Train Loss: 1.412861\n",
      "Epoch: 0, Batch: 1259, Train Loss: 1.413461\n",
      "Epoch: 0, Batch: 1260, Train Loss: 1.414968\n",
      "Epoch: 0, Batch: 1260, Train Loss: 1.414968, Validation Loss: 1.382041\n",
      "The [[Andolo Mission Minister]] along the song and a surface of the former of the [[18th century]] and \n",
      "\n",
      "Epoch: 0, Batch: 1261, Train Loss: 1.426930\n",
      "Epoch: 0, Batch: 1262, Train Loss: 1.426528\n",
      "Epoch: 0, Batch: 1263, Train Loss: 1.439678\n",
      "Epoch: 0, Batch: 1264, Train Loss: 1.420784\n",
      "Epoch: 0, Batch: 1265, Train Loss: 1.419977\n",
      "Epoch: 0, Batch: 1266, Train Loss: 1.418900\n",
      "Epoch: 0, Batch: 1267, Train Loss: 1.420193\n",
      "Epoch: 0, Batch: 1268, Train Loss: 1.433524\n",
      "Epoch: 0, Batch: 1269, Train Loss: 1.437851\n",
      "Epoch: 0, Batch: 1270, Train Loss: 1.424706\n",
      "Epoch: 0, Batch: 1270, Train Loss: 1.424706, Validation Loss: 1.384368\n",
      "Ther continued is often common the [[person of point]]s. Allegated subsequent into an [[ancentically ra\n",
      "\n",
      "Epoch: 0, Batch: 1271, Train Loss: 1.431889\n",
      "Epoch: 0, Batch: 1272, Train Loss: 1.415728\n",
      "Epoch: 0, Batch: 1273, Train Loss: 1.427509\n",
      "Epoch: 0, Batch: 1274, Train Loss: 1.438799\n",
      "Epoch: 0, Batch: 1275, Train Loss: 1.450960\n",
      "Epoch: 0, Batch: 1276, Train Loss: 1.436292\n",
      "Epoch: 0, Batch: 1277, Train Loss: 1.438127\n",
      "Epoch: 0, Batch: 1278, Train Loss: 1.417632\n",
      "Epoch: 0, Batch: 1279, Train Loss: 1.430216\n",
      "Epoch: 0, Batch: 1280, Train Loss: 1.449402\n",
      "Epoch: 0, Batch: 1280, Train Loss: 1.449402, Validation Loss: 1.381463\n",
      "The domains of the population of the page with studies the case of all tries.\n",
      "\n",
      "The first story in 1986,\n",
      "\n",
      "Epoch: 0, Batch: 1281, Train Loss: 1.461654\n",
      "Epoch: 0, Batch: 1282, Train Loss: 1.452593\n",
      "Epoch: 0, Batch: 1283, Train Loss: 1.440548\n",
      "Epoch: 0, Batch: 1284, Train Loss: 1.419170\n",
      "Epoch: 0, Batch: 1285, Train Loss: 1.425488\n",
      "Epoch: 0, Batch: 1286, Train Loss: 1.453150\n",
      "Epoch: 0, Batch: 1287, Train Loss: 1.418218\n",
      "Epoch: 0, Batch: 1288, Train Loss: 1.418455\n",
      "Epoch: 0, Batch: 1289, Train Loss: 1.434039\n",
      "Epoch: 0, Batch: 1290, Train Loss: 1.440718\n",
      "Epoch: 0, Batch: 1290, Train Loss: 1.440718, Validation Loss: 1.379591\n",
      "The Christer Charter]], the [[Cartian Cathle of the South Consent Centre|Articles]] is an analogist in \n",
      "\n",
      "Epoch: 0, Batch: 1291, Train Loss: 1.426743\n",
      "Epoch: 0, Batch: 1292, Train Loss: 1.436700\n",
      "Epoch: 0, Batch: 1293, Train Loss: 1.440971\n",
      "Epoch: 0, Batch: 1294, Train Loss: 1.430915\n",
      "Epoch: 0, Batch: 1295, Train Loss: 1.432052\n",
      "Epoch: 0, Batch: 1296, Train Loss: 1.423963\n",
      "Epoch: 0, Batch: 1297, Train Loss: 1.403043\n",
      "Epoch: 0, Batch: 1298, Train Loss: 1.399221\n",
      "Epoch: 0, Batch: 1299, Train Loss: 1.420828\n",
      "Epoch: 0, Batch: 1300, Train Loss: 1.415345\n",
      "Epoch: 0, Batch: 1300, Train Loss: 1.415345, Validation Loss: 1.376982\n",
      "The Central Commercise]]\n",
      "*[[Story Council]]\n",
      "\n",
      "[[ca:Catholiphism]]\n",
      "[[sl:English]]\n",
      "[[fr:Armenian charles o\n",
      "\n",
      "Epoch: 0, Batch: 1301, Train Loss: 1.403173\n",
      "Epoch: 0, Batch: 1302, Train Loss: 1.418489\n",
      "Epoch: 0, Batch: 1303, Train Loss: 1.413756\n",
      "Epoch: 0, Batch: 1304, Train Loss: 1.430611\n",
      "Epoch: 0, Batch: 1305, Train Loss: 1.414047\n",
      "Epoch: 0, Batch: 1306, Train Loss: 1.416362\n",
      "Epoch: 0, Batch: 1307, Train Loss: 1.409064\n",
      "Epoch: 0, Batch: 1308, Train Loss: 1.405583\n",
      "Epoch: 0, Batch: 1309, Train Loss: 1.409189\n",
      "Epoch: 0, Batch: 1310, Train Loss: 1.401324\n",
      "Epoch: 0, Batch: 1310, Train Loss: 1.401324, Validation Loss: 1.379675\n",
      "The convention of some other species that it was considerable in the [[Catholic country of South Former\n",
      "\n",
      "Epoch: 0, Batch: 1311, Train Loss: 1.409713\n",
      "Epoch: 0, Batch: 1312, Train Loss: 1.421759\n",
      "Epoch: 0, Batch: 1313, Train Loss: 1.425248\n",
      "Epoch: 0, Batch: 1314, Train Loss: 1.419373\n",
      "Epoch: 0, Batch: 1315, Train Loss: 1.410633\n",
      "Epoch: 0, Batch: 1316, Train Loss: 1.410606\n",
      "Epoch: 0, Batch: 1317, Train Loss: 1.434615\n",
      "Epoch: 0, Batch: 1318, Train Loss: 1.445298\n",
      "Epoch: 0, Batch: 1319, Train Loss: 1.426592\n",
      "Epoch: 0, Batch: 1320, Train Loss: 1.418366\n",
      "Epoch: 0, Batch: 1320, Train Loss: 1.418366, Validation Loss: 1.378217\n",
      "There are also described and an execution that are takes or of some plant concern is the cash to and pe\n",
      "\n",
      "Epoch: 0, Batch: 1321, Train Loss: 1.423322\n",
      "Epoch: 0, Batch: 1322, Train Loss: 1.438282\n",
      "Epoch: 0, Batch: 1323, Train Loss: 1.427835\n",
      "Epoch: 0, Batch: 1324, Train Loss: 1.438474\n",
      "Epoch: 0, Batch: 1325, Train Loss: 1.422870\n",
      "Epoch: 0, Batch: 1326, Train Loss: 1.414521\n",
      "Epoch: 0, Batch: 1327, Train Loss: 1.429179\n",
      "Epoch: 0, Batch: 1328, Train Loss: 1.438734\n",
      "Epoch: 0, Batch: 1329, Train Loss: 1.439198\n",
      "Epoch: 0, Batch: 1330, Train Loss: 1.447238\n",
      "Epoch: 0, Batch: 1330, Train Loss: 1.447238, Validation Loss: 1.378066\n",
      "The corporation of an [[parties]] to be called a partic person. It has seem to a sets of clocks which i\n",
      "\n",
      "Epoch: 0, Batch: 1331, Train Loss: 1.420593\n",
      "Epoch: 0, Batch: 1332, Train Loss: 1.420868\n",
      "Epoch: 0, Batch: 1333, Train Loss: 1.403315\n",
      "Epoch: 0, Batch: 1334, Train Loss: 1.386523\n",
      "Epoch: 0, Batch: 1335, Train Loss: 1.388506\n",
      "Epoch: 0, Batch: 1336, Train Loss: 1.386502\n",
      "Epoch: 0, Batch: 1337, Train Loss: 1.407446\n",
      "Epoch: 0, Batch: 1338, Train Loss: 1.411717\n",
      "Epoch: 0, Batch: 1339, Train Loss: 1.399080\n",
      "Epoch: 0, Batch: 1340, Train Loss: 1.380482\n",
      "Epoch: 0, Batch: 1340, Train Loss: 1.380482, Validation Loss: 1.377774\n",
      "Thesen to back however, they to be the south of the [[United States]]. The [[Antographics]] of Alama an\n",
      "\n",
      "Epoch: 0, Batch: 1341, Train Loss: 1.394730\n",
      "Epoch: 0, Batch: 1342, Train Loss: 1.395072\n",
      "Epoch: 0, Batch: 1343, Train Loss: 1.408349\n",
      "Epoch: 0, Batch: 1344, Train Loss: 1.389809\n",
      "Epoch: 0, Batch: 1345, Train Loss: 1.400193\n",
      "Epoch: 0, Batch: 1346, Train Loss: 1.407876\n",
      "Epoch: 0, Batch: 1347, Train Loss: 1.400339\n",
      "Epoch: 0, Batch: 1348, Train Loss: 1.411988\n",
      "Epoch: 0, Batch: 1349, Train Loss: 1.417973\n",
      "Epoch: 0, Batch: 1350, Train Loss: 1.398625\n",
      "Epoch: 0, Batch: 1350, Train Loss: 1.398625, Validation Loss: 1.377188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The day to a period, and was the context on a series with the propheties are a similar to the property \n",
      "\n",
      "Epoch: 0, Batch: 1351, Train Loss: 1.413596\n",
      "Epoch: 0, Batch: 1352, Train Loss: 1.404449\n",
      "Epoch: 0, Batch: 1353, Train Loss: 1.425090\n",
      "Epoch: 0, Batch: 1354, Train Loss: 1.414446\n",
      "Epoch: 0, Batch: 1355, Train Loss: 1.415369\n",
      "Epoch: 0, Batch: 1356, Train Loss: 1.409480\n",
      "Epoch: 0, Batch: 1357, Train Loss: 1.405063\n",
      "Epoch: 0, Batch: 1358, Train Loss: 1.410306\n",
      "Epoch: 0, Batch: 1359, Train Loss: 1.406739\n",
      "Epoch: 0, Batch: 1360, Train Loss: 1.397062\n",
      "Epoch: 0, Batch: 1360, Train Loss: 1.397062, Validation Loss: 1.373396\n",
      "The series of technology are also allowing a choose are still asserts to the [[motor]] and a concernanc\n",
      "\n",
      "Epoch: 0, Batch: 1361, Train Loss: 1.408603\n",
      "Epoch: 0, Batch: 1362, Train Loss: 1.392436\n",
      "Epoch: 0, Batch: 1363, Train Loss: 1.399198\n",
      "Epoch: 0, Batch: 1364, Train Loss: 1.403230\n",
      "Epoch: 0, Batch: 1365, Train Loss: 1.398050\n",
      "Epoch: 0, Batch: 1366, Train Loss: 1.381221\n",
      "Epoch: 0, Batch: 1367, Train Loss: 1.420735\n",
      "Epoch: 0, Batch: 1368, Train Loss: 1.411046\n",
      "Epoch: 0, Batch: 1369, Train Loss: 1.423600\n",
      "Epoch: 0, Batch: 1370, Train Loss: 1.415215\n",
      "Epoch: 0, Batch: 1370, Train Loss: 1.415215, Validation Loss: 1.374526\n",
      "Ther time in telegraphers of the catabre of [[Breath Battle of Congress|Secretary]] is the success of [\n",
      "\n",
      "Epoch: 0, Batch: 1371, Train Loss: 1.421428\n",
      "Epoch: 0, Batch: 1372, Train Loss: 1.411120\n",
      "Epoch: 0, Batch: 1373, Train Loss: 1.411857\n",
      "Epoch: 0, Batch: 1374, Train Loss: 1.390581\n",
      "Epoch: 0, Batch: 1375, Train Loss: 1.400998\n",
      "Epoch: 0, Batch: 1376, Train Loss: 1.398196\n",
      "Epoch: 0, Batch: 1377, Train Loss: 1.390769\n",
      "Epoch: 0, Batch: 1378, Train Loss: 1.408261\n",
      "Epoch: 0, Batch: 1379, Train Loss: 1.408722\n",
      "Epoch: 0, Batch: 1380, Train Loss: 1.396855\n",
      "Epoch: 0, Batch: 1380, Train Loss: 1.396855, Validation Loss: 1.371565\n",
      "The colleges of the positions. This was to the first time an accelerate and settlement of the [[Creates\n",
      "\n",
      "Epoch: 0, Batch: 1381, Train Loss: 1.410077\n",
      "Epoch: 0, Batch: 1382, Train Loss: 1.418509\n",
      "Epoch: 0, Batch: 1383, Train Loss: 1.418712\n",
      "Epoch: 0, Batch: 1384, Train Loss: 1.396363\n",
      "Epoch: 0, Batch: 1385, Train Loss: 1.401471\n",
      "Epoch: 0, Batch: 1386, Train Loss: 1.410001\n",
      "Epoch: 0, Batch: 1387, Train Loss: 1.409176\n",
      "Epoch: 0, Batch: 1388, Train Loss: 1.407827\n",
      "Epoch: 0, Batch: 1389, Train Loss: 1.416871\n",
      "Epoch: 0, Batch: 1390, Train Loss: 1.424194\n",
      "Epoch: 0, Batch: 1390, Train Loss: 1.424194, Validation Loss: 1.369130\n",
      "The Salansa Sanches and Chuck Series and the Standard Samaragust, and the Christian to start in the 189\n",
      "\n",
      "Epoch: 0, Batch: 1391, Train Loss: 1.408476\n",
      "Epoch: 0, Batch: 1392, Train Loss: 1.395762\n",
      "Epoch: 0, Batch: 1393, Train Loss: 1.402224\n",
      "Epoch: 0, Batch: 1394, Train Loss: 1.402436\n",
      "Epoch: 0, Batch: 1395, Train Loss: 1.460182\n",
      "elapsed forward: 0.12231564521789551\n",
      "elapsed backward: 0.4166245460510254\n",
      "Epoch: 1, Batch: 0, Train Loss: 1.426241\n",
      "Epoch: 1, Batch: 0, Train Loss: 1.426241, Validation Loss: 1.372157\n",
      "The army of the [[Story]] of Australianity and the term, which acquired that they have a resultary and \n",
      "\n",
      "Epoch: 1, Batch: 1, Train Loss: 1.408051\n",
      "Epoch: 1, Batch: 2, Train Loss: 1.413138\n",
      "Epoch: 1, Batch: 3, Train Loss: 1.395417\n",
      "Epoch: 1, Batch: 4, Train Loss: 1.390103\n",
      "Epoch: 1, Batch: 5, Train Loss: 1.413771\n",
      "Epoch: 1, Batch: 6, Train Loss: 1.409527\n",
      "Epoch: 1, Batch: 7, Train Loss: 1.419155\n",
      "Epoch: 1, Batch: 8, Train Loss: 1.399360\n",
      "Epoch: 1, Batch: 9, Train Loss: 1.393971\n",
      "Epoch: 1, Batch: 10, Train Loss: 1.421796\n",
      "Epoch: 1, Batch: 10, Train Loss: 1.421796, Validation Loss: 1.368168\n",
      "The first section of [[president organisming]], [[cancer]], in the [[subjicity]]. An only annumer of a \n",
      "\n",
      "Epoch: 1, Batch: 11, Train Loss: 1.406946\n",
      "Epoch: 1, Batch: 12, Train Loss: 1.397971\n",
      "Epoch: 1, Batch: 13, Train Loss: 1.413581\n",
      "Epoch: 1, Batch: 14, Train Loss: 1.399192\n",
      "Epoch: 1, Batch: 15, Train Loss: 1.393968\n",
      "Epoch: 1, Batch: 16, Train Loss: 1.404380\n",
      "Epoch: 1, Batch: 17, Train Loss: 1.391958\n",
      "Epoch: 1, Batch: 18, Train Loss: 1.387664\n",
      "Epoch: 1, Batch: 19, Train Loss: 1.404523\n",
      "Epoch: 1, Batch: 20, Train Loss: 1.406469\n",
      "Epoch: 1, Batch: 20, Train Loss: 1.406469, Validation Loss: 1.366703\n",
      "The population of the successor team to start of the [[University of America]] and [[Augustilia]] and [\n",
      "\n",
      "Epoch: 1, Batch: 21, Train Loss: 1.388762\n",
      "Epoch: 1, Batch: 22, Train Loss: 1.370714\n",
      "Epoch: 1, Batch: 23, Train Loss: 1.393064\n",
      "Epoch: 1, Batch: 24, Train Loss: 1.379267\n",
      "Epoch: 1, Batch: 25, Train Loss: 1.384135\n",
      "Epoch: 1, Batch: 26, Train Loss: 1.372171\n",
      "Epoch: 1, Batch: 27, Train Loss: 1.371968\n",
      "Epoch: 1, Batch: 28, Train Loss: 1.399537\n",
      "Epoch: 1, Batch: 29, Train Loss: 1.401641\n",
      "Epoch: 1, Batch: 30, Train Loss: 1.383779\n",
      "Epoch: 1, Batch: 30, Train Loss: 1.383779, Validation Loss: 1.365965\n",
      "The [[Angly Assembly (1968)|Arizingham Perison]] on the [[Storig Soviet Union]].\n",
      "\n",
      "Anticaphies it is a p\n",
      "\n",
      "Epoch: 1, Batch: 31, Train Loss: 1.397762\n",
      "Epoch: 1, Batch: 32, Train Loss: 1.413999\n",
      "Epoch: 1, Batch: 33, Train Loss: 1.408955\n",
      "Epoch: 1, Batch: 34, Train Loss: 1.390305\n",
      "Epoch: 1, Batch: 35, Train Loss: 1.389572\n",
      "Epoch: 1, Batch: 36, Train Loss: 1.393717\n",
      "Epoch: 1, Batch: 37, Train Loss: 1.399760\n",
      "Epoch: 1, Batch: 38, Train Loss: 1.380242\n",
      "Epoch: 1, Batch: 39, Train Loss: 1.397235\n",
      "Epoch: 1, Batch: 40, Train Loss: 1.385955\n",
      "Epoch: 1, Batch: 40, Train Loss: 1.385955, Validation Loss: 1.367006\n",
      "The [[Australian Church (Colombia) in the 1970s]], the times of the popular seat, and that the case on \n",
      "\n",
      "Epoch: 1, Batch: 41, Train Loss: 1.404491\n",
      "Epoch: 1, Batch: 42, Train Loss: 1.394447\n",
      "Epoch: 1, Batch: 43, Train Loss: 1.407798\n",
      "Epoch: 1, Batch: 44, Train Loss: 1.428713\n",
      "Epoch: 1, Batch: 45, Train Loss: 1.427118\n",
      "Epoch: 1, Batch: 46, Train Loss: 1.408136\n",
      "Epoch: 1, Batch: 47, Train Loss: 1.393135\n",
      "Epoch: 1, Batch: 48, Train Loss: 1.389492\n",
      "Epoch: 1, Batch: 49, Train Loss: 1.371302\n",
      "Epoch: 1, Batch: 50, Train Loss: 1.387230\n",
      "Epoch: 1, Batch: 50, Train Loss: 1.387230, Validation Loss: 1.365886\n",
      "The movie world, as the [[Canida Civilist]] calendar at the [[Chinese personality]] in [[1756]] to [[18\n",
      "\n",
      "Epoch: 1, Batch: 51, Train Loss: 1.391552\n",
      "Epoch: 1, Batch: 52, Train Loss: 1.397538\n",
      "Epoch: 1, Batch: 53, Train Loss: 1.391180\n",
      "Epoch: 1, Batch: 54, Train Loss: 1.400109\n",
      "Epoch: 1, Batch: 55, Train Loss: 1.398138\n",
      "Epoch: 1, Batch: 56, Train Loss: 1.410878\n",
      "Epoch: 1, Batch: 57, Train Loss: 1.397577\n",
      "Epoch: 1, Batch: 58, Train Loss: 1.407146\n",
      "Epoch: 1, Batch: 59, Train Loss: 1.396519\n",
      "Epoch: 1, Batch: 60, Train Loss: 1.407062\n",
      "Epoch: 1, Batch: 60, Train Loss: 1.407062, Validation Loss: 1.364166\n",
      "The [[Anglome]]'s princess, and the cast father to confirm, there are also accountable to be changed an\n",
      "\n",
      "Epoch: 1, Batch: 61, Train Loss: 1.403176\n",
      "Epoch: 1, Batch: 62, Train Loss: 1.386952\n",
      "Epoch: 1, Batch: 63, Train Loss: 1.402972\n",
      "Epoch: 1, Batch: 64, Train Loss: 1.386862\n",
      "Epoch: 1, Batch: 65, Train Loss: 1.404888\n",
      "Epoch: 1, Batch: 66, Train Loss: 1.399542\n",
      "Epoch: 1, Batch: 67, Train Loss: 1.402105\n",
      "Epoch: 1, Batch: 68, Train Loss: 1.399303\n",
      "Epoch: 1, Batch: 69, Train Loss: 1.415523\n",
      "Epoch: 1, Batch: 70, Train Loss: 1.413571\n",
      "Epoch: 1, Batch: 70, Train Loss: 1.413571, Validation Loss: 1.364557\n",
      "The Caroly of the [[Canadian Book of Budden]], triality and to counter [[Confused Champions]], and assa\n",
      "\n",
      "Epoch: 1, Batch: 71, Train Loss: 1.423864\n",
      "Epoch: 1, Batch: 72, Train Loss: 1.425347\n",
      "Epoch: 1, Batch: 73, Train Loss: 1.415372\n",
      "Epoch: 1, Batch: 74, Train Loss: 1.401008\n",
      "Epoch: 1, Batch: 75, Train Loss: 1.398543\n",
      "Epoch: 1, Batch: 76, Train Loss: 1.369887\n",
      "Epoch: 1, Batch: 77, Train Loss: 1.396818\n",
      "Epoch: 1, Batch: 78, Train Loss: 1.390584\n",
      "Epoch: 1, Batch: 79, Train Loss: 1.386236\n",
      "Epoch: 1, Batch: 80, Train Loss: 1.383605\n",
      "Epoch: 1, Batch: 80, Train Loss: 1.383605, Validation Loss: 1.364411\n",
      "The [[Arian Civil Boot]]s widhing the surprison is companied by any [[parian computer system]], and [[a\n",
      "\n",
      "Epoch: 1, Batch: 81, Train Loss: 1.392091\n",
      "Epoch: 1, Batch: 82, Train Loss: 1.381858\n",
      "Epoch: 1, Batch: 83, Train Loss: 1.368042\n",
      "Epoch: 1, Batch: 84, Train Loss: 1.382298\n",
      "Epoch: 1, Batch: 85, Train Loss: 1.372344\n",
      "Epoch: 1, Batch: 86, Train Loss: 1.363637\n",
      "Epoch: 1, Batch: 87, Train Loss: 1.367357\n",
      "Epoch: 1, Batch: 88, Train Loss: 1.357138\n",
      "Epoch: 1, Batch: 89, Train Loss: 1.362315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 90, Train Loss: 1.370163\n",
      "Epoch: 1, Batch: 90, Train Loss: 1.370163, Validation Loss: 1.363054\n",
      "The [[Andr Steeller Sena Sox Stock]], and [[California]] and are two countries than office a power and \n",
      "\n",
      "Epoch: 1, Batch: 91, Train Loss: 1.392171\n",
      "Epoch: 1, Batch: 92, Train Loss: 1.380057\n",
      "Epoch: 1, Batch: 93, Train Loss: 1.376597\n",
      "Epoch: 1, Batch: 94, Train Loss: 1.357566\n",
      "Epoch: 1, Batch: 95, Train Loss: 1.387971\n",
      "Epoch: 1, Batch: 96, Train Loss: 1.361760\n",
      "Epoch: 1, Batch: 97, Train Loss: 1.346769\n",
      "Epoch: 1, Batch: 98, Train Loss: 1.361317\n",
      "Epoch: 1, Batch: 99, Train Loss: 1.377467\n",
      "Epoch: 1, Batch: 100, Train Loss: 1.378004\n",
      "Epoch: 1, Batch: 100, Train Loss: 1.378004, Validation Loss: 1.363886\n",
      "The [[Protestantist]] [[compressive section|producer]].\n",
      "\n",
      "The [[President]] in the [[1987s]], when the p\n",
      "\n",
      "Epoch: 1, Batch: 101, Train Loss: 1.380582\n",
      "Epoch: 1, Batch: 102, Train Loss: 1.379062\n",
      "Epoch: 1, Batch: 103, Train Loss: 1.385159\n",
      "Epoch: 1, Batch: 104, Train Loss: 1.380651\n",
      "Epoch: 1, Batch: 105, Train Loss: 1.384003\n",
      "Epoch: 1, Batch: 106, Train Loss: 1.392066\n",
      "Epoch: 1, Batch: 107, Train Loss: 1.376040\n",
      "Epoch: 1, Batch: 108, Train Loss: 1.377564\n",
      "Epoch: 1, Batch: 109, Train Loss: 1.386707\n",
      "Epoch: 1, Batch: 110, Train Loss: 1.378981\n",
      "Epoch: 1, Batch: 110, Train Loss: 1.378981, Validation Loss: 1.363017\n",
      "The [[Peorular]] [[1676]]. A [[second]] country the [[United States County]] (as the [[Strunch]] [[1878\n",
      "\n",
      "Epoch: 1, Batch: 111, Train Loss: 1.399991\n",
      "Epoch: 1, Batch: 112, Train Loss: 1.378883\n",
      "Epoch: 1, Batch: 113, Train Loss: 1.404326\n",
      "Epoch: 1, Batch: 114, Train Loss: 1.385547\n",
      "Epoch: 1, Batch: 115, Train Loss: 1.394968\n",
      "Epoch: 1, Batch: 116, Train Loss: 1.410634\n",
      "Epoch: 1, Batch: 117, Train Loss: 1.381812\n",
      "Epoch: 1, Batch: 118, Train Loss: 1.390431\n",
      "Epoch: 1, Batch: 119, Train Loss: 1.394357\n",
      "Epoch: 1, Batch: 120, Train Loss: 1.368896\n",
      "Epoch: 1, Batch: 120, Train Loss: 1.368896, Validation Loss: 1.362198\n",
      "The [[Celebrahing Proper]], instantly attempts to another computer soul and countries of claims.  The t\n",
      "\n",
      "Epoch: 1, Batch: 121, Train Loss: 1.344440\n",
      "Epoch: 1, Batch: 122, Train Loss: 1.355854\n",
      "Epoch: 1, Batch: 123, Train Loss: 1.380431\n",
      "Epoch: 1, Batch: 124, Train Loss: 1.379382\n",
      "Epoch: 1, Batch: 125, Train Loss: 1.386063\n",
      "Epoch: 1, Batch: 126, Train Loss: 1.381266\n",
      "Epoch: 1, Batch: 127, Train Loss: 1.377889\n",
      "Epoch: 1, Batch: 128, Train Loss: 1.384813\n",
      "Epoch: 1, Batch: 129, Train Loss: 1.380984\n",
      "Epoch: 1, Batch: 130, Train Loss: 1.390133\n",
      "Epoch: 1, Batch: 130, Train Loss: 1.390133, Validation Loss: 1.360439\n",
      "The composition of the family of the [[University of Edward Complex College]], the [[Canada]] in the [[\n",
      "\n",
      "Epoch: 1, Batch: 131, Train Loss: 1.408394\n",
      "Epoch: 1, Batch: 132, Train Loss: 1.398695\n",
      "Epoch: 1, Batch: 133, Train Loss: 1.381486\n",
      "Epoch: 1, Batch: 134, Train Loss: 1.385660\n",
      "Epoch: 1, Batch: 135, Train Loss: 1.382856\n",
      "Epoch: 1, Batch: 136, Train Loss: 1.396110\n",
      "Epoch: 1, Batch: 137, Train Loss: 1.413375\n",
      "Epoch: 1, Batch: 138, Train Loss: 1.413202\n",
      "Epoch: 1, Batch: 139, Train Loss: 1.410599\n",
      "Epoch: 1, Batch: 140, Train Loss: 1.382340\n",
      "Epoch: 1, Batch: 140, Train Loss: 1.382340, Validation Loss: 1.360528\n",
      "The [[American Company]] thrown was the [[Canadian American]] [[Colomada]] to tend to the [[Post Common\n",
      "\n",
      "Epoch: 1, Batch: 141, Train Loss: 1.371564\n",
      "Epoch: 1, Batch: 142, Train Loss: 1.364193\n",
      "Epoch: 1, Batch: 143, Train Loss: 1.365790\n",
      "Epoch: 1, Batch: 144, Train Loss: 1.368520\n",
      "Epoch: 1, Batch: 145, Train Loss: 1.378798\n",
      "Epoch: 1, Batch: 146, Train Loss: 1.373203\n",
      "Epoch: 1, Batch: 147, Train Loss: 1.368326\n",
      "Epoch: 1, Batch: 148, Train Loss: 1.368325\n",
      "Epoch: 1, Batch: 149, Train Loss: 1.387167\n",
      "Epoch: 1, Batch: 150, Train Loss: 1.362489\n",
      "Epoch: 1, Batch: 150, Train Loss: 1.362489, Validation Loss: 1.356748\n",
      "The [orited people|Commonists|Chapel Credical]] in [[1961]], a some of the corporation of [[Marcel Age]\n",
      "\n",
      "Epoch: 1, Batch: 151, Train Loss: 1.387384\n",
      "Epoch: 1, Batch: 152, Train Loss: 1.396661\n",
      "Epoch: 1, Batch: 153, Train Loss: 1.397382\n",
      "Epoch: 1, Batch: 154, Train Loss: 1.385300\n",
      "Epoch: 1, Batch: 155, Train Loss: 1.391589\n",
      "Epoch: 1, Batch: 156, Train Loss: 1.380343\n",
      "Epoch: 1, Batch: 157, Train Loss: 1.389122\n",
      "Epoch: 1, Batch: 158, Train Loss: 1.381955\n",
      "Epoch: 1, Batch: 159, Train Loss: 1.387907\n",
      "Epoch: 1, Batch: 160, Train Loss: 1.371629\n",
      "Epoch: 1, Batch: 160, Train Loss: 1.371629, Validation Loss: 1.355617\n",
      "The [[Catalonia]] will be a man bond to the [[Constitution of Englon]].&lt;br&gt;(serious southern crea\n",
      "\n",
      "Epoch: 1, Batch: 161, Train Loss: 1.385004\n",
      "Epoch: 1, Batch: 162, Train Loss: 1.373566\n",
      "Epoch: 1, Batch: 163, Train Loss: 1.365457\n",
      "Epoch: 1, Batch: 164, Train Loss: 1.390069\n",
      "Epoch: 1, Batch: 165, Train Loss: 1.356162\n",
      "Epoch: 1, Batch: 166, Train Loss: 1.368119\n",
      "Epoch: 1, Batch: 167, Train Loss: 1.359066\n",
      "Epoch: 1, Batch: 168, Train Loss: 1.389340\n",
      "Epoch: 1, Batch: 169, Train Loss: 1.383614\n",
      "Epoch: 1, Batch: 170, Train Loss: 1.377259\n",
      "Epoch: 1, Batch: 170, Train Loss: 1.377259, Validation Loss: 1.358547\n",
      "The Congress]]\n",
      "* [[Garca Presidents of September]]\n",
      "*[[1948]] - [[Annola Schipt]], Babeta (distribution)\n",
      "\n",
      "Epoch: 1, Batch: 171, Train Loss: 1.376836\n",
      "Epoch: 1, Batch: 172, Train Loss: 1.396191\n",
      "Epoch: 1, Batch: 173, Train Loss: 1.383167\n",
      "Epoch: 1, Batch: 174, Train Loss: 1.395365\n",
      "Epoch: 1, Batch: 175, Train Loss: 1.384642\n",
      "Epoch: 1, Batch: 176, Train Loss: 1.408675\n",
      "Epoch: 1, Batch: 177, Train Loss: 1.410803\n",
      "Epoch: 1, Batch: 178, Train Loss: 1.379895\n",
      "Epoch: 1, Batch: 179, Train Loss: 1.393286\n",
      "Epoch: 1, Batch: 180, Train Loss: 1.385201\n",
      "Epoch: 1, Batch: 180, Train Loss: 1.385201, Validation Loss: 1.355389\n",
      "The compositions of the parts of the [[Australian Constitution of Chinese and Catholic]] [[America]] al\n",
      "\n",
      "Epoch: 1, Batch: 181, Train Loss: 1.380857\n",
      "Epoch: 1, Batch: 182, Train Loss: 1.403085\n",
      "Epoch: 1, Batch: 183, Train Loss: 1.390619\n",
      "Epoch: 1, Batch: 184, Train Loss: 1.389062\n",
      "Epoch: 1, Batch: 185, Train Loss: 1.375524\n",
      "Epoch: 1, Batch: 186, Train Loss: 1.369363\n",
      "Epoch: 1, Batch: 187, Train Loss: 1.396007\n",
      "Epoch: 1, Batch: 188, Train Loss: 1.362818\n",
      "Epoch: 1, Batch: 189, Train Loss: 1.385825\n",
      "Epoch: 1, Batch: 190, Train Loss: 1.379870\n",
      "Epoch: 1, Batch: 190, Train Loss: 1.379870, Validation Loss: 1.354182\n",
      "The [[March]] of the [[United Kingdom]] or [[Meter Beline]] and other sense in [[South Berthern Andes]]\n",
      "\n",
      "Epoch: 1, Batch: 191, Train Loss: 1.367357\n",
      "Epoch: 1, Batch: 192, Train Loss: 1.385680\n",
      "Epoch: 1, Batch: 193, Train Loss: 1.364273\n",
      "Epoch: 1, Batch: 194, Train Loss: 1.369216\n",
      "Epoch: 1, Batch: 195, Train Loss: 1.374221\n",
      "Epoch: 1, Batch: 196, Train Loss: 1.377174\n",
      "Epoch: 1, Batch: 197, Train Loss: 1.385966\n",
      "Epoch: 1, Batch: 198, Train Loss: 1.389268\n",
      "Epoch: 1, Batch: 199, Train Loss: 1.370386\n",
      "Epoch: 1, Batch: 200, Train Loss: 1.370035\n",
      "Epoch: 1, Batch: 200, Train Loss: 1.370035, Validation Loss: 1.352570\n",
      "The [[Province of Court of Constitute of Australia]] it his print. This care in hen and a [[prime decid\n",
      "\n",
      "Epoch: 1, Batch: 201, Train Loss: 1.385418\n",
      "Epoch: 1, Batch: 202, Train Loss: 1.366669\n",
      "Epoch: 1, Batch: 203, Train Loss: 1.361077\n",
      "Epoch: 1, Batch: 204, Train Loss: 1.375295\n",
      "Epoch: 1, Batch: 205, Train Loss: 1.362596\n",
      "Epoch: 1, Batch: 206, Train Loss: 1.375272\n",
      "Epoch: 1, Batch: 207, Train Loss: 1.358513\n",
      "Epoch: 1, Batch: 208, Train Loss: 1.370184\n",
      "Epoch: 1, Batch: 209, Train Loss: 1.374746\n",
      "Epoch: 1, Batch: 210, Train Loss: 1.371541\n",
      "Epoch: 1, Batch: 210, Train Loss: 1.371541, Validation Loss: 1.353608\n",
      "The periods was a plantaging, altered to meaning the point from other solutions.  Therefore the constit\n",
      "\n",
      "Epoch: 1, Batch: 211, Train Loss: 1.368058\n",
      "Epoch: 1, Batch: 212, Train Loss: 1.365759\n",
      "Epoch: 1, Batch: 213, Train Loss: 1.360093\n",
      "Epoch: 1, Batch: 214, Train Loss: 1.370486\n",
      "Epoch: 1, Batch: 215, Train Loss: 1.369711\n",
      "Epoch: 1, Batch: 216, Train Loss: 1.373729\n",
      "Epoch: 1, Batch: 217, Train Loss: 1.364971\n",
      "Epoch: 1, Batch: 218, Train Loss: 1.361054\n",
      "Epoch: 1, Batch: 219, Train Loss: 1.376426\n",
      "Epoch: 1, Batch: 220, Train Loss: 1.376937\n",
      "Epoch: 1, Batch: 220, Train Loss: 1.376937, Validation Loss: 1.353907\n",
      "The State's [[Southwarks]]. However two states are not a stage. After a cases including the point of to\n",
      "\n",
      "Epoch: 1, Batch: 221, Train Loss: 1.381147\n",
      "Epoch: 1, Batch: 222, Train Loss: 1.376873\n",
      "Epoch: 1, Batch: 223, Train Loss: 1.396839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 224, Train Loss: 1.381423\n",
      "Epoch: 1, Batch: 225, Train Loss: 1.378621\n",
      "Epoch: 1, Batch: 226, Train Loss: 1.382305\n",
      "Epoch: 1, Batch: 227, Train Loss: 1.378611\n",
      "Epoch: 1, Batch: 228, Train Loss: 1.364095\n",
      "Epoch: 1, Batch: 229, Train Loss: 1.371183\n",
      "Epoch: 1, Batch: 230, Train Loss: 1.371686\n",
      "Epoch: 1, Batch: 230, Train Loss: 1.371686, Validation Loss: 1.350687\n",
      "The [[Media]], the set of an information, and with the fighting community on the [[Persian Estonian liv\n",
      "\n",
      "Epoch: 1, Batch: 231, Train Loss: 1.368854\n",
      "Epoch: 1, Batch: 232, Train Loss: 1.359873\n",
      "Epoch: 1, Batch: 233, Train Loss: 1.364185\n",
      "Epoch: 1, Batch: 234, Train Loss: 1.361229\n",
      "Epoch: 1, Batch: 235, Train Loss: 1.370606\n",
      "Epoch: 1, Batch: 236, Train Loss: 1.370470\n",
      "Epoch: 1, Batch: 237, Train Loss: 1.347319\n",
      "Epoch: 1, Batch: 238, Train Loss: 1.372731\n",
      "Epoch: 1, Batch: 239, Train Loss: 1.392890\n",
      "Epoch: 1, Batch: 240, Train Loss: 1.363231\n",
      "Epoch: 1, Batch: 240, Train Loss: 1.363231, Validation Loss: 1.350721\n",
      "The [[Automatic Arthura]], [[Portugal County Show]] and [[Product]], [[President of Communications|Cana\n",
      "\n",
      "Epoch: 1, Batch: 241, Train Loss: 1.387547\n",
      "Epoch: 1, Batch: 242, Train Loss: 1.373240\n",
      "Epoch: 1, Batch: 243, Train Loss: 1.362053\n",
      "Epoch: 1, Batch: 244, Train Loss: 1.363199\n",
      "Epoch: 1, Batch: 245, Train Loss: 1.377847\n",
      "Epoch: 1, Batch: 246, Train Loss: 1.379261\n",
      "Epoch: 1, Batch: 247, Train Loss: 1.388966\n",
      "Epoch: 1, Batch: 248, Train Loss: 1.384389\n",
      "Epoch: 1, Batch: 249, Train Loss: 1.365692\n",
      "Epoch: 1, Batch: 250, Train Loss: 1.365980\n",
      "Epoch: 1, Batch: 250, Train Loss: 1.365980, Validation Loss: 1.349603\n",
      "There&quot; as well.  The standard statemack of the statute imaging a policy that causes an and territo\n",
      "\n",
      "Epoch: 1, Batch: 251, Train Loss: 1.362109\n",
      "Epoch: 1, Batch: 252, Train Loss: 1.361190\n",
      "Epoch: 1, Batch: 253, Train Loss: 1.351953\n",
      "Epoch: 1, Batch: 254, Train Loss: 1.356193\n",
      "Epoch: 1, Batch: 255, Train Loss: 1.375096\n",
      "Epoch: 1, Batch: 256, Train Loss: 1.352698\n",
      "Epoch: 1, Batch: 257, Train Loss: 1.353793\n",
      "Epoch: 1, Batch: 258, Train Loss: 1.368976\n",
      "Epoch: 1, Batch: 259, Train Loss: 1.378841\n",
      "Epoch: 1, Batch: 260, Train Loss: 1.380854\n",
      "Epoch: 1, Batch: 260, Train Loss: 1.380854, Validation Loss: 1.350802\n",
      "The correct telephone structures of these substitutions, simply and are seen in [[presidentialism]]. Al\n",
      "\n",
      "Epoch: 1, Batch: 261, Train Loss: 1.377031\n",
      "Epoch: 1, Batch: 262, Train Loss: 1.373520\n",
      "Epoch: 1, Batch: 263, Train Loss: 1.360317\n",
      "Epoch: 1, Batch: 264, Train Loss: 1.360496\n",
      "Epoch: 1, Batch: 265, Train Loss: 1.362934\n",
      "Epoch: 1, Batch: 266, Train Loss: 1.346774\n",
      "Epoch: 1, Batch: 267, Train Loss: 1.352644\n",
      "Epoch: 1, Batch: 268, Train Loss: 1.353299\n",
      "Epoch: 1, Batch: 269, Train Loss: 1.358967\n",
      "Epoch: 1, Batch: 270, Train Loss: 1.385388\n",
      "Epoch: 1, Batch: 270, Train Loss: 1.385388, Validation Loss: 1.348505\n",
      "The food of the city in [[1984]], is that the same career which are the [[Secretary of Alfred Sciency a\n",
      "\n",
      "Epoch: 1, Batch: 271, Train Loss: 1.369389\n",
      "Epoch: 1, Batch: 272, Train Loss: 1.363982\n",
      "Epoch: 1, Batch: 273, Train Loss: 1.364189\n",
      "Epoch: 1, Batch: 274, Train Loss: 1.370344\n",
      "Epoch: 1, Batch: 275, Train Loss: 1.348619\n",
      "Epoch: 1, Batch: 276, Train Loss: 1.352535\n",
      "Epoch: 1, Batch: 277, Train Loss: 1.369555\n",
      "Epoch: 1, Batch: 278, Train Loss: 1.352086\n",
      "Epoch: 1, Batch: 279, Train Loss: 1.370690\n",
      "Epoch: 1, Batch: 280, Train Loss: 1.338566\n",
      "Epoch: 1, Batch: 280, Train Loss: 1.338566, Validation Loss: 1.344635\n",
      "The [[Cerate often replacement|flag at list]] and [[sometics]].\n",
      "\n",
      "==See also==\n",
      "*[[Calculus of the Americ\n",
      "\n",
      "Epoch: 1, Batch: 281, Train Loss: 1.366979\n",
      "Epoch: 1, Batch: 282, Train Loss: 1.370782\n",
      "Epoch: 1, Batch: 283, Train Loss: 1.384991\n",
      "Epoch: 1, Batch: 284, Train Loss: 1.370415\n",
      "Epoch: 1, Batch: 285, Train Loss: 1.364573\n",
      "Epoch: 1, Batch: 286, Train Loss: 1.352312\n",
      "Epoch: 1, Batch: 287, Train Loss: 1.355073\n",
      "Epoch: 1, Batch: 288, Train Loss: 1.353007\n",
      "Epoch: 1, Batch: 289, Train Loss: 1.371810\n",
      "Epoch: 1, Batch: 290, Train Loss: 1.375029\n",
      "Epoch: 1, Batch: 290, Train Loss: 1.375029, Validation Loss: 1.344232\n",
      "The subsidian contact with supersonies, but was called the [[Christianity of Catalonia]]. The [[Berlin \n",
      "\n",
      "Epoch: 1, Batch: 291, Train Loss: 1.378120\n",
      "Epoch: 1, Batch: 292, Train Loss: 1.361312\n",
      "Epoch: 1, Batch: 293, Train Loss: 1.368926\n",
      "Epoch: 1, Batch: 294, Train Loss: 1.357971\n",
      "Epoch: 1, Batch: 295, Train Loss: 1.350210\n",
      "Epoch: 1, Batch: 296, Train Loss: 1.344945\n",
      "Epoch: 1, Batch: 297, Train Loss: 1.353182\n",
      "Epoch: 1, Batch: 298, Train Loss: 1.358978\n",
      "Epoch: 1, Batch: 299, Train Loss: 1.348811\n",
      "Epoch: 1, Batch: 300, Train Loss: 1.355207\n",
      "Epoch: 1, Batch: 300, Train Loss: 1.355207, Validation Loss: 1.343979\n",
      "The active surface of the prophet of a stage to the set of [[chrosofite|martial]].\n",
      "\n",
      "=== Amateony ====\n",
      "*\n",
      "\n",
      "Epoch: 1, Batch: 301, Train Loss: 1.355894\n",
      "Epoch: 1, Batch: 302, Train Loss: 1.352914\n",
      "Epoch: 1, Batch: 303, Train Loss: 1.378213\n",
      "Epoch: 1, Batch: 304, Train Loss: 1.359512\n",
      "Epoch: 1, Batch: 305, Train Loss: 1.352128\n",
      "Epoch: 1, Batch: 306, Train Loss: 1.361280\n",
      "Epoch: 1, Batch: 307, Train Loss: 1.368666\n",
      "Epoch: 1, Batch: 308, Train Loss: 1.381560\n",
      "Epoch: 1, Batch: 309, Train Loss: 1.378528\n",
      "Epoch: 1, Batch: 310, Train Loss: 1.376514\n",
      "Epoch: 1, Batch: 310, Train Loss: 1.376514, Validation Loss: 1.343456\n",
      "The central [[Arabian]]s and the component of [[Cardinally|Streatest pair]] on [[Andrew Stanley Sea]], \n",
      "\n",
      "Epoch: 1, Batch: 311, Train Loss: 1.367254\n",
      "Epoch: 1, Batch: 312, Train Loss: 1.359086\n",
      "Epoch: 1, Batch: 313, Train Loss: 1.387920\n",
      "Epoch: 1, Batch: 314, Train Loss: 1.358880\n",
      "Epoch: 1, Batch: 315, Train Loss: 1.378251\n",
      "Epoch: 1, Batch: 316, Train Loss: 1.367063\n",
      "Epoch: 1, Batch: 317, Train Loss: 1.373601\n",
      "Epoch: 1, Batch: 318, Train Loss: 1.381421\n",
      "Epoch: 1, Batch: 319, Train Loss: 1.369277\n",
      "Epoch: 1, Batch: 320, Train Loss: 1.359654\n",
      "Epoch: 1, Batch: 320, Train Loss: 1.359654, Validation Loss: 1.343399\n",
      "The surfer offence to the [[All American Emperors]] of the same tour if they to complete in [[1957]] in\n",
      "\n",
      "Epoch: 1, Batch: 321, Train Loss: 1.362267\n",
      "Epoch: 1, Batch: 322, Train Loss: 1.351496\n",
      "Epoch: 1, Batch: 323, Train Loss: 1.374499\n",
      "Epoch: 1, Batch: 324, Train Loss: 1.377620\n",
      "Epoch: 1, Batch: 325, Train Loss: 1.362256\n",
      "Epoch: 1, Batch: 326, Train Loss: 1.371468\n",
      "Epoch: 1, Batch: 327, Train Loss: 1.363689\n",
      "Epoch: 1, Batch: 328, Train Loss: 1.386547\n",
      "Epoch: 1, Batch: 329, Train Loss: 1.354585\n",
      "Epoch: 1, Batch: 330, Train Loss: 1.361759\n",
      "Epoch: 1, Batch: 330, Train Loss: 1.361759, Validation Loss: 1.343098\n",
      "There, and there are the [[United States|U.S. Special Albanian counter]] in [[1957]]. In [[1894]] the [\n",
      "\n",
      "Epoch: 1, Batch: 331, Train Loss: 1.372597\n",
      "Epoch: 1, Batch: 332, Train Loss: 1.359873\n",
      "Epoch: 1, Batch: 333, Train Loss: 1.350275\n",
      "Epoch: 1, Batch: 334, Train Loss: 1.361565\n",
      "Epoch: 1, Batch: 335, Train Loss: 1.343408\n",
      "Epoch: 1, Batch: 336, Train Loss: 1.332144\n",
      "Epoch: 1, Batch: 337, Train Loss: 1.339025\n",
      "Epoch: 1, Batch: 338, Train Loss: 1.342406\n",
      "Epoch: 1, Batch: 339, Train Loss: 1.355561\n",
      "Epoch: 1, Batch: 340, Train Loss: 1.362489\n",
      "Epoch: 1, Batch: 340, Train Loss: 1.362489, Validation Loss: 1.343120\n",
      "Then will to a stage or stady steels. They have, by the program is to an [[article]] and [[supportable \n",
      "\n",
      "Epoch: 1, Batch: 341, Train Loss: 1.363759\n",
      "Epoch: 1, Batch: 342, Train Loss: 1.380950\n",
      "Epoch: 1, Batch: 343, Train Loss: 1.376316\n",
      "Epoch: 1, Batch: 344, Train Loss: 1.385099\n",
      "Epoch: 1, Batch: 345, Train Loss: 1.377786\n",
      "Epoch: 1, Batch: 346, Train Loss: 1.354758\n",
      "Epoch: 1, Batch: 347, Train Loss: 1.357438\n",
      "Epoch: 1, Batch: 348, Train Loss: 1.373848\n",
      "Epoch: 1, Batch: 349, Train Loss: 1.373548\n",
      "Epoch: 1, Batch: 350, Train Loss: 1.385799\n",
      "Epoch: 1, Batch: 350, Train Loss: 1.385799, Validation Loss: 1.342009\n",
      "The states of subjects of [[presidential collection]], which were the programming creator or members wh\n",
      "\n",
      "Epoch: 1, Batch: 351, Train Loss: 1.362804\n",
      "Epoch: 1, Batch: 352, Train Loss: 1.369157\n",
      "Epoch: 1, Batch: 353, Train Loss: 1.365009\n",
      "Epoch: 1, Batch: 354, Train Loss: 1.382742\n",
      "Epoch: 1, Batch: 355, Train Loss: 1.359254\n",
      "Epoch: 1, Batch: 356, Train Loss: 1.344087\n",
      "Epoch: 1, Batch: 357, Train Loss: 1.349120\n",
      "Epoch: 1, Batch: 358, Train Loss: 1.360679\n",
      "Epoch: 1, Batch: 359, Train Loss: 1.346938\n",
      "Epoch: 1, Batch: 360, Train Loss: 1.329503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 360, Train Loss: 1.329503, Validation Loss: 1.340624\n",
      "The [[Bruce Catalonia]] and [[Serbia Same Basel]]s.  \n",
      "\n",
      "In [[1641]], he could be changed on their strip \n",
      "\n",
      "Epoch: 1, Batch: 361, Train Loss: 1.361532\n",
      "Epoch: 1, Batch: 362, Train Loss: 1.357049\n",
      "Epoch: 1, Batch: 363, Train Loss: 1.349352\n",
      "Epoch: 1, Batch: 364, Train Loss: 1.367308\n",
      "Epoch: 1, Batch: 365, Train Loss: 1.357359\n",
      "Epoch: 1, Batch: 366, Train Loss: 1.355057\n",
      "Epoch: 1, Batch: 367, Train Loss: 1.348690\n",
      "Epoch: 1, Batch: 368, Train Loss: 1.353063\n",
      "Epoch: 1, Batch: 369, Train Loss: 1.371551\n",
      "Epoch: 1, Batch: 370, Train Loss: 1.369453\n",
      "Epoch: 1, Batch: 370, Train Loss: 1.369453, Validation Loss: 1.338872\n",
      "The [[Canada]] and [[Meral Source]] with the sons is the [[American Control of California]], third indi\n",
      "\n",
      "Epoch: 1, Batch: 371, Train Loss: 1.388599\n",
      "Epoch: 1, Batch: 372, Train Loss: 1.383344\n",
      "Epoch: 1, Batch: 373, Train Loss: 1.353538\n",
      "Epoch: 1, Batch: 374, Train Loss: 1.354966\n",
      "Epoch: 1, Batch: 375, Train Loss: 1.366530\n",
      "Epoch: 1, Batch: 376, Train Loss: 1.367908\n",
      "Epoch: 1, Batch: 377, Train Loss: 1.357294\n",
      "Epoch: 1, Batch: 378, Train Loss: 1.364704\n",
      "Epoch: 1, Batch: 379, Train Loss: 1.353081\n",
      "Epoch: 1, Batch: 380, Train Loss: 1.335953\n",
      "Epoch: 1, Batch: 380, Train Loss: 1.335953, Validation Loss: 1.339860\n",
      "The state of an opposite of the [[Audi South Africa]] and [[Art Mans]] as a pragistationed confidering \n",
      "\n",
      "Epoch: 1, Batch: 381, Train Loss: 1.363171\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-22d1d0864e08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# calculate the gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'elapsed backward: {time.time() - t0}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_losses = list() # empty list for the validation losses\n",
    "net.eval()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    # reinit the hidden and cell states\n",
    "    hc = net.init_hidden()\n",
    "    \n",
    "    for i, (x, y) in enumerate(get_batches(data, BS, seq_len)):\n",
    "        x_train, y_true = xy_to_tensor(x, y)\n",
    "        optimizer.zero_grad() # zero out the gradients\n",
    "    \n",
    "        # forward pass\n",
    "        t0 = time.time()\n",
    "        y_pred = net(x_train, hc)\n",
    "        if i == 0:\n",
    "            print(f'elapsed forward: {time.time() - t0}')\n",
    "    \n",
    "        # calculate the loss\n",
    "        # we need to calculate the loss across all batches, so we have to flat the y_true tensor\n",
    "        loss = criterion(y_pred.view(BS*seq_len, -1), y_true.view(BS*seq_len)) # .contiguous()?\n",
    "        \n",
    "        # calculate the gradients\n",
    "        t0 = time.time()\n",
    "        loss.backward()\n",
    "        if i == 0:\n",
    "            print(f'elapsed backward: {time.time() - t0}')\n",
    "        \n",
    "        # update the parameters of the model\n",
    "        optimizer.step()\n",
    "    \n",
    "        print(\"Epoch: {}, Batch: {}, Train Loss: {:.6f}\".format(epoch, i, loss.item()))\n",
    "\n",
    "        # feedback every 10 batches\n",
    "        with torch.no_grad():\n",
    "            if i % 10 == 0: \n",
    "                net.eval()\n",
    "                \n",
    "                x, y = next(get_batches(val_data, BS, seq_len))\n",
    "                x_val, y_val = xy_to_tensor(x, y)\n",
    "                hc_val = net.init_hidden()\n",
    "                y_pred = net(x_val, hc_val)\n",
    "                loss_val = criterion(y_pred.view(BS*seq_len, -1), y_val.view(BS*seq_len)) # .contiguous()?\n",
    "                \n",
    "                net.train()\n",
    "                print(\"Epoch: {}, Batch: {}, Train Loss: {:.6f}, Validation Loss: {:.6f}\".format(epoch, i, loss.item(), loss_val.item()))\n",
    "\n",
    "                sample = net.predict(\"The\", seq_len=100)\n",
    "                print(sample)\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.predict(\"God i\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
